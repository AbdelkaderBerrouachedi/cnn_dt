{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# cnn model\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "try: from sklearn.model_selection import train_test_split\n",
    "except: from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the train & test and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data, split into training/testing groups\n",
    "d=datasets.load_digits()\n",
    "X=d.data\n",
    "Y=d.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.asarray(X), np.asarray(Y), test_size=0.3, shuffle= True)\n",
    "x_train_, x_test_, y_train_, y_test_ = x_train, x_test, y_train, y_test\n",
    "# The known number of output classes.\n",
    "num_classes = len(np.unique(y_train))\n",
    "# Input image dimensions\n",
    "input_shape = (X.shape[1],)\n",
    "\n",
    "# Convert class vectors to binary class matrices. This uses 1 hot encoding.\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], X.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0], X.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def CNN_net():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1],1)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(X.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1257 samples, validate on 540 samples\n",
      "Epoch 1/210\n",
      "1257/1257 [==============================] - 1s 804us/step - loss: 3.3079 - accuracy: 0.0883 - val_loss: 2.3877 - val_accuracy: 0.0833\n",
      "Epoch 2/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 2.8735 - accuracy: 0.1034 - val_loss: 2.3349 - val_accuracy: 0.0889\n",
      "Epoch 3/210\n",
      "1257/1257 [==============================] - 1s 411us/step - loss: 2.5973 - accuracy: 0.1154 - val_loss: 2.3065 - val_accuracy: 0.1296\n",
      "Epoch 4/210\n",
      "1257/1257 [==============================] - 0s 391us/step - loss: 2.4597 - accuracy: 0.1225 - val_loss: 2.2908 - val_accuracy: 0.1593\n",
      "Epoch 5/210\n",
      "1257/1257 [==============================] - 1s 407us/step - loss: 2.3822 - accuracy: 0.1177 - val_loss: 2.2843 - val_accuracy: 0.2148\n",
      "Epoch 6/210\n",
      "1257/1257 [==============================] - 1s 421us/step - loss: 2.3562 - accuracy: 0.1169 - val_loss: 2.2825 - val_accuracy: 0.2222\n",
      "Epoch 7/210\n",
      "1257/1257 [==============================] - 0s 390us/step - loss: 2.3147 - accuracy: 0.1106 - val_loss: 2.2816 - val_accuracy: 0.2352\n",
      "Epoch 8/210\n",
      "1257/1257 [==============================] - 0s 361us/step - loss: 2.3058 - accuracy: 0.1257 - val_loss: 2.2811 - val_accuracy: 0.2278\n",
      "Epoch 9/210\n",
      "1257/1257 [==============================] - 0s 391us/step - loss: 2.2923 - accuracy: 0.1400 - val_loss: 2.2807 - val_accuracy: 0.2130\n",
      "Epoch 10/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 2.2923 - accuracy: 0.1249 - val_loss: 2.2801 - val_accuracy: 0.2093\n",
      "Epoch 11/210\n",
      "1257/1257 [==============================] - 1s 419us/step - loss: 2.2836 - accuracy: 0.1384 - val_loss: 2.2793 - val_accuracy: 0.1907\n",
      "Epoch 12/210\n",
      "1257/1257 [==============================] - 0s 389us/step - loss: 2.2824 - accuracy: 0.1313 - val_loss: 2.2781 - val_accuracy: 0.1870\n",
      "Epoch 13/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 2.2762 - accuracy: 0.1368 - val_loss: 2.2765 - val_accuracy: 0.1944\n",
      "Epoch 14/210\n",
      "1257/1257 [==============================] - 0s 370us/step - loss: 2.2787 - accuracy: 0.1376 - val_loss: 2.2744 - val_accuracy: 0.1889\n",
      "Epoch 15/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 2.2844 - accuracy: 0.1329 - val_loss: 2.2719 - val_accuracy: 0.1981\n",
      "Epoch 16/210\n",
      "1257/1257 [==============================] - 0s 336us/step - loss: 2.2776 - accuracy: 0.1480 - val_loss: 2.2688 - val_accuracy: 0.2056\n",
      "Epoch 17/210\n",
      "1257/1257 [==============================] - 0s 364us/step - loss: 2.2801 - accuracy: 0.1297 - val_loss: 2.2649 - val_accuracy: 0.2111\n",
      "Epoch 18/210\n",
      "1257/1257 [==============================] - 0s 347us/step - loss: 2.2801 - accuracy: 0.1337 - val_loss: 2.2605 - val_accuracy: 0.2241\n",
      "Epoch 19/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 2.2689 - accuracy: 0.1488 - val_loss: 2.2549 - val_accuracy: 0.2333\n",
      "Epoch 20/210\n",
      "1257/1257 [==============================] - 0s 354us/step - loss: 2.2544 - accuracy: 0.1527 - val_loss: 2.2481 - val_accuracy: 0.2593\n",
      "Epoch 21/210\n",
      "1257/1257 [==============================] - 0s 375us/step - loss: 2.2593 - accuracy: 0.1480 - val_loss: 2.2402 - val_accuracy: 0.2759\n",
      "Epoch 22/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 2.2544 - accuracy: 0.1599 - val_loss: 2.2312 - val_accuracy: 0.2889\n",
      "Epoch 23/210\n",
      "1257/1257 [==============================] - 0s 368us/step - loss: 2.2451 - accuracy: 0.1543 - val_loss: 2.2213 - val_accuracy: 0.3000\n",
      "Epoch 24/210\n",
      "1257/1257 [==============================] - 0s 355us/step - loss: 2.2506 - accuracy: 0.1631 - val_loss: 2.2103 - val_accuracy: 0.3204\n",
      "Epoch 25/210\n",
      "1257/1257 [==============================] - 0s 351us/step - loss: 2.2318 - accuracy: 0.1575 - val_loss: 2.1973 - val_accuracy: 0.3481\n",
      "Epoch 26/210\n",
      "1257/1257 [==============================] - 0s 361us/step - loss: 2.2293 - accuracy: 0.1806 - val_loss: 2.1830 - val_accuracy: 0.3519\n",
      "Epoch 27/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 2.2216 - accuracy: 0.1647 - val_loss: 2.1663 - val_accuracy: 0.3648\n",
      "Epoch 28/210\n",
      "1257/1257 [==============================] - 0s 364us/step - loss: 2.2151 - accuracy: 0.1766 - val_loss: 2.1472 - val_accuracy: 0.3889\n",
      "Epoch 29/210\n",
      "1257/1257 [==============================] - 0s 361us/step - loss: 2.2068 - accuracy: 0.1671 - val_loss: 2.1257 - val_accuracy: 0.4037\n",
      "Epoch 30/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 2.1669 - accuracy: 0.2100 - val_loss: 2.0995 - val_accuracy: 0.4278\n",
      "Epoch 31/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 2.1802 - accuracy: 0.1941 - val_loss: 2.0695 - val_accuracy: 0.4500\n",
      "Epoch 32/210\n",
      "1257/1257 [==============================] - 0s 351us/step - loss: 2.1585 - accuracy: 0.1909 - val_loss: 2.0350 - val_accuracy: 0.4722\n",
      "Epoch 33/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 2.1384 - accuracy: 0.2060 - val_loss: 1.9959 - val_accuracy: 0.5074\n",
      "Epoch 34/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 2.1021 - accuracy: 0.2403 - val_loss: 1.9520 - val_accuracy: 0.5259\n",
      "Epoch 35/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 2.0959 - accuracy: 0.2251 - val_loss: 1.9029 - val_accuracy: 0.5519\n",
      "Epoch 36/210\n",
      "1257/1257 [==============================] - 0s 375us/step - loss: 2.0777 - accuracy: 0.2291 - val_loss: 1.8497 - val_accuracy: 0.5833\n",
      "Epoch 37/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 2.0401 - accuracy: 0.2482 - val_loss: 1.7938 - val_accuracy: 0.6130\n",
      "Epoch 38/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 2.0232 - accuracy: 0.2562 - val_loss: 1.7379 - val_accuracy: 0.6574\n",
      "Epoch 39/210\n",
      "1257/1257 [==============================] - 1s 400us/step - loss: 1.9794 - accuracy: 0.2673 - val_loss: 1.6812 - val_accuracy: 0.6870\n",
      "Epoch 40/210\n",
      "1257/1257 [==============================] - 1s 403us/step - loss: 1.9493 - accuracy: 0.2792 - val_loss: 1.6249 - val_accuracy: 0.6944\n",
      "Epoch 41/210\n",
      "1257/1257 [==============================] - 1s 416us/step - loss: 1.9143 - accuracy: 0.3079 - val_loss: 1.5649 - val_accuracy: 0.7167\n",
      "Epoch 42/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 1.8770 - accuracy: 0.3134 - val_loss: 1.5029 - val_accuracy: 0.7389\n",
      "Epoch 43/210\n",
      "1257/1257 [==============================] - 0s 395us/step - loss: 1.8113 - accuracy: 0.3381 - val_loss: 1.4367 - val_accuracy: 0.7537\n",
      "Epoch 44/210\n",
      "1257/1257 [==============================] - 1s 417us/step - loss: 1.8086 - accuracy: 0.3540 - val_loss: 1.3687 - val_accuracy: 0.7741\n",
      "Epoch 45/210\n",
      "1257/1257 [==============================] - 1s 421us/step - loss: 1.7405 - accuracy: 0.3556 - val_loss: 1.3005 - val_accuracy: 0.7870\n",
      "Epoch 46/210\n",
      "1257/1257 [==============================] - 1s 402us/step - loss: 1.7102 - accuracy: 0.3922 - val_loss: 1.2341 - val_accuracy: 0.7963\n",
      "Epoch 47/210\n",
      "1257/1257 [==============================] - 0s 380us/step - loss: 1.6585 - accuracy: 0.4304 - val_loss: 1.1683 - val_accuracy: 0.8037\n",
      "Epoch 48/210\n",
      "1257/1257 [==============================] - 0s 375us/step - loss: 1.5740 - accuracy: 0.4368 - val_loss: 1.1011 - val_accuracy: 0.8056\n",
      "Epoch 49/210\n",
      "1257/1257 [==============================] - 0s 347us/step - loss: 1.5680 - accuracy: 0.4352 - val_loss: 1.0358 - val_accuracy: 0.8259\n",
      "Epoch 50/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 1.4837 - accuracy: 0.4662 - val_loss: 0.9713 - val_accuracy: 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/210\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 1.4414 - accuracy: 0.4845 - val_loss: 0.9059 - val_accuracy: 0.8444\n",
      "Epoch 52/210\n",
      "1257/1257 [==============================] - 0s 357us/step - loss: 1.4469 - accuracy: 0.4694 - val_loss: 0.8451 - val_accuracy: 0.8444\n",
      "Epoch 53/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 1.4065 - accuracy: 0.5012 - val_loss: 0.7907 - val_accuracy: 0.8481\n",
      "Epoch 54/210\n",
      "1257/1257 [==============================] - 0s 349us/step - loss: 1.3654 - accuracy: 0.5370 - val_loss: 0.7428 - val_accuracy: 0.8519\n",
      "Epoch 55/210\n",
      "1257/1257 [==============================] - 0s 380us/step - loss: 1.2940 - accuracy: 0.5632 - val_loss: 0.6988 - val_accuracy: 0.8593\n",
      "Epoch 56/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 1.2615 - accuracy: 0.5632 - val_loss: 0.6572 - val_accuracy: 0.8667\n",
      "Epoch 57/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 1.2295 - accuracy: 0.5792 - val_loss: 0.6209 - val_accuracy: 0.8704\n",
      "Epoch 58/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 1.2103 - accuracy: 0.5855 - val_loss: 0.5903 - val_accuracy: 0.8722\n",
      "Epoch 59/210\n",
      "1257/1257 [==============================] - 0s 354us/step - loss: 1.1930 - accuracy: 0.5919 - val_loss: 0.5642 - val_accuracy: 0.8759\n",
      "Epoch 60/210\n",
      "1257/1257 [==============================] - 0s 368us/step - loss: 1.1340 - accuracy: 0.6046 - val_loss: 0.5391 - val_accuracy: 0.8759\n",
      "Epoch 61/210\n",
      "1257/1257 [==============================] - 0s 358us/step - loss: 1.0972 - accuracy: 0.6269 - val_loss: 0.5130 - val_accuracy: 0.8815\n",
      "Epoch 62/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 1.0482 - accuracy: 0.6420 - val_loss: 0.4870 - val_accuracy: 0.8889\n",
      "Epoch 63/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 1.0290 - accuracy: 0.6500 - val_loss: 0.4610 - val_accuracy: 0.8926\n",
      "Epoch 64/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 0.9853 - accuracy: 0.6635 - val_loss: 0.4348 - val_accuracy: 0.8889\n",
      "Epoch 65/210\n",
      "1257/1257 [==============================] - 0s 367us/step - loss: 0.9857 - accuracy: 0.6754 - val_loss: 0.4106 - val_accuracy: 0.8907\n",
      "Epoch 66/210\n",
      "1257/1257 [==============================] - 0s 364us/step - loss: 0.9375 - accuracy: 0.6921 - val_loss: 0.3880 - val_accuracy: 0.8963\n",
      "Epoch 67/210\n",
      "1257/1257 [==============================] - 0s 358us/step - loss: 0.9307 - accuracy: 0.6874 - val_loss: 0.3658 - val_accuracy: 0.8907\n",
      "Epoch 68/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 0.8521 - accuracy: 0.7080 - val_loss: 0.3468 - val_accuracy: 0.8981\n",
      "Epoch 69/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 0.8561 - accuracy: 0.7112 - val_loss: 0.3321 - val_accuracy: 0.9019\n",
      "Epoch 70/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.8594 - accuracy: 0.7072 - val_loss: 0.3211 - val_accuracy: 0.9000\n",
      "Epoch 71/210\n",
      "1257/1257 [==============================] - 1s 415us/step - loss: 0.9176 - accuracy: 0.7049 - val_loss: 0.3120 - val_accuracy: 0.9074\n",
      "Epoch 72/210\n",
      "1257/1257 [==============================] - 1s 426us/step - loss: 0.8307 - accuracy: 0.7224 - val_loss: 0.3036 - val_accuracy: 0.9111\n",
      "Epoch 73/210\n",
      "1257/1257 [==============================] - 1s 406us/step - loss: 0.7840 - accuracy: 0.7319 - val_loss: 0.2948 - val_accuracy: 0.9093\n",
      "Epoch 74/210\n",
      "1257/1257 [==============================] - 1s 440us/step - loss: 0.7835 - accuracy: 0.7343 - val_loss: 0.2876 - val_accuracy: 0.9111\n",
      "Epoch 75/210\n",
      "1257/1257 [==============================] - 1s 421us/step - loss: 0.7673 - accuracy: 0.7438 - val_loss: 0.2797 - val_accuracy: 0.9130\n",
      "Epoch 76/210\n",
      "1257/1257 [==============================] - 1s 444us/step - loss: 0.7362 - accuracy: 0.7526 - val_loss: 0.2717 - val_accuracy: 0.9167\n",
      "Epoch 77/210\n",
      "1257/1257 [==============================] - 1s 451us/step - loss: 0.7287 - accuracy: 0.7637 - val_loss: 0.2639 - val_accuracy: 0.9204\n",
      "Epoch 78/210\n",
      "1257/1257 [==============================] - 1s 419us/step - loss: 0.7574 - accuracy: 0.7494 - val_loss: 0.2552 - val_accuracy: 0.9259\n",
      "Epoch 79/210\n",
      "1257/1257 [==============================] - 1s 421us/step - loss: 0.7405 - accuracy: 0.7693 - val_loss: 0.2442 - val_accuracy: 0.9315\n",
      "Epoch 80/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.6630 - accuracy: 0.7812 - val_loss: 0.2331 - val_accuracy: 0.9333\n",
      "Epoch 81/210\n",
      "1257/1257 [==============================] - 0s 368us/step - loss: 0.7090 - accuracy: 0.7613 - val_loss: 0.2237 - val_accuracy: 0.9407\n",
      "Epoch 82/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 0.6378 - accuracy: 0.7868 - val_loss: 0.2148 - val_accuracy: 0.9426\n",
      "Epoch 83/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.6849 - accuracy: 0.7757 - val_loss: 0.2075 - val_accuracy: 0.9481\n",
      "Epoch 84/210\n",
      "1257/1257 [==============================] - 0s 357us/step - loss: 0.6554 - accuracy: 0.7868 - val_loss: 0.2008 - val_accuracy: 0.9500\n",
      "Epoch 85/210\n",
      "1257/1257 [==============================] - 0s 385us/step - loss: 0.6395 - accuracy: 0.7844 - val_loss: 0.1954 - val_accuracy: 0.9481\n",
      "Epoch 86/210\n",
      "1257/1257 [==============================] - 1s 419us/step - loss: 0.6261 - accuracy: 0.7892 - val_loss: 0.1922 - val_accuracy: 0.9444\n",
      "Epoch 87/210\n",
      "1257/1257 [==============================] - 0s 341us/step - loss: 0.5938 - accuracy: 0.8051 - val_loss: 0.1885 - val_accuracy: 0.9463\n",
      "Epoch 88/210\n",
      "1257/1257 [==============================] - 0s 376us/step - loss: 0.6448 - accuracy: 0.7987 - val_loss: 0.1820 - val_accuracy: 0.9519\n",
      "Epoch 89/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.5635 - accuracy: 0.7979 - val_loss: 0.1771 - val_accuracy: 0.9519\n",
      "Epoch 90/210\n",
      "1257/1257 [==============================] - 0s 366us/step - loss: 0.5852 - accuracy: 0.8162 - val_loss: 0.1733 - val_accuracy: 0.9519\n",
      "Epoch 91/210\n",
      "1257/1257 [==============================] - 0s 367us/step - loss: 0.5702 - accuracy: 0.8345 - val_loss: 0.1700 - val_accuracy: 0.9556\n",
      "Epoch 92/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.5504 - accuracy: 0.8154 - val_loss: 0.1659 - val_accuracy: 0.9574\n",
      "Epoch 93/210\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 0.5476 - accuracy: 0.8218 - val_loss: 0.1623 - val_accuracy: 0.9593\n",
      "Epoch 94/210\n",
      "1257/1257 [==============================] - 0s 381us/step - loss: 0.5481 - accuracy: 0.8305 - val_loss: 0.1590 - val_accuracy: 0.9593\n",
      "Epoch 95/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 0.5217 - accuracy: 0.8321 - val_loss: 0.1555 - val_accuracy: 0.9593\n",
      "Epoch 96/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 0.4899 - accuracy: 0.8488 - val_loss: 0.1516 - val_accuracy: 0.9593\n",
      "Epoch 97/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 0.4920 - accuracy: 0.8369 - val_loss: 0.1474 - val_accuracy: 0.9593\n",
      "Epoch 98/210\n",
      "1257/1257 [==============================] - 0s 364us/step - loss: 0.5205 - accuracy: 0.8313 - val_loss: 0.1432 - val_accuracy: 0.9593\n",
      "Epoch 99/210\n",
      "1257/1257 [==============================] - 0s 386us/step - loss: 0.4792 - accuracy: 0.8393 - val_loss: 0.1388 - val_accuracy: 0.9630\n",
      "Epoch 100/210\n",
      "1257/1257 [==============================] - 0s 346us/step - loss: 0.4746 - accuracy: 0.8584 - val_loss: 0.1356 - val_accuracy: 0.9648\n",
      "Epoch 101/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.4935 - accuracy: 0.8496 - val_loss: 0.1326 - val_accuracy: 0.9667\n",
      "Epoch 102/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 0.4984 - accuracy: 0.8337 - val_loss: 0.1302 - val_accuracy: 0.9685\n",
      "Epoch 103/210\n",
      "1257/1257 [==============================] - 0s 389us/step - loss: 0.4699 - accuracy: 0.8528 - val_loss: 0.1283 - val_accuracy: 0.9685\n",
      "Epoch 104/210\n",
      "1257/1257 [==============================] - 1s 417us/step - loss: 0.4514 - accuracy: 0.8457 - val_loss: 0.1266 - val_accuracy: 0.9648\n",
      "Epoch 105/210\n",
      "1257/1257 [==============================] - 1s 416us/step - loss: 0.4445 - accuracy: 0.8560 - val_loss: 0.1259 - val_accuracy: 0.9648\n",
      "Epoch 106/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.4789 - accuracy: 0.8632 - val_loss: 0.1262 - val_accuracy: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/210\n",
      "1257/1257 [==============================] - 1s 455us/step - loss: 0.4818 - accuracy: 0.8560 - val_loss: 0.1266 - val_accuracy: 0.9630\n",
      "Epoch 108/210\n",
      "1257/1257 [==============================] - 1s 457us/step - loss: 0.4383 - accuracy: 0.8679 - val_loss: 0.1269 - val_accuracy: 0.9630\n",
      "Epoch 109/210\n",
      "1257/1257 [==============================] - 1s 449us/step - loss: 0.4925 - accuracy: 0.8377 - val_loss: 0.1257 - val_accuracy: 0.9630\n",
      "Epoch 110/210\n",
      "1257/1257 [==============================] - 1s 434us/step - loss: 0.4405 - accuracy: 0.8536 - val_loss: 0.1237 - val_accuracy: 0.9648\n",
      "Epoch 111/210\n",
      "1257/1257 [==============================] - 1s 419us/step - loss: 0.4043 - accuracy: 0.8727 - val_loss: 0.1213 - val_accuracy: 0.9667\n",
      "Epoch 112/210\n",
      "1257/1257 [==============================] - 0s 395us/step - loss: 0.4173 - accuracy: 0.8616 - val_loss: 0.1186 - val_accuracy: 0.9704\n",
      "Epoch 113/210\n",
      "1257/1257 [==============================] - 0s 391us/step - loss: 0.4343 - accuracy: 0.8687 - val_loss: 0.1163 - val_accuracy: 0.9704\n",
      "Epoch 114/210\n",
      "1257/1257 [==============================] - 0s 378us/step - loss: 0.4426 - accuracy: 0.8663 - val_loss: 0.1140 - val_accuracy: 0.9685\n",
      "Epoch 115/210\n",
      "1257/1257 [==============================] - 0s 357us/step - loss: 0.4391 - accuracy: 0.8624 - val_loss: 0.1121 - val_accuracy: 0.9685\n",
      "Epoch 116/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 0.4283 - accuracy: 0.8656 - val_loss: 0.1101 - val_accuracy: 0.9722\n",
      "Epoch 117/210\n",
      "1257/1257 [==============================] - 0s 368us/step - loss: 0.4564 - accuracy: 0.8632 - val_loss: 0.1080 - val_accuracy: 0.9722\n",
      "Epoch 118/210\n",
      "1257/1257 [==============================] - 0s 378us/step - loss: 0.3977 - accuracy: 0.8703 - val_loss: 0.1056 - val_accuracy: 0.9741\n",
      "Epoch 119/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.3827 - accuracy: 0.8775 - val_loss: 0.1032 - val_accuracy: 0.9741\n",
      "Epoch 120/210\n",
      "1257/1257 [==============================] - 0s 380us/step - loss: 0.3771 - accuracy: 0.8854 - val_loss: 0.1009 - val_accuracy: 0.9759\n",
      "Epoch 121/210\n",
      "1257/1257 [==============================] - 0s 376us/step - loss: 0.3843 - accuracy: 0.8767 - val_loss: 0.0995 - val_accuracy: 0.9741\n",
      "Epoch 122/210\n",
      "1257/1257 [==============================] - 0s 372us/step - loss: 0.3727 - accuracy: 0.8831 - val_loss: 0.0981 - val_accuracy: 0.9704\n",
      "Epoch 123/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.3717 - accuracy: 0.8886 - val_loss: 0.0973 - val_accuracy: 0.9704\n",
      "Epoch 124/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 0.4170 - accuracy: 0.8687 - val_loss: 0.0961 - val_accuracy: 0.9759\n",
      "Epoch 125/210\n",
      "1257/1257 [==============================] - 0s 364us/step - loss: 0.3790 - accuracy: 0.8775 - val_loss: 0.0947 - val_accuracy: 0.9759\n",
      "Epoch 126/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 0.3581 - accuracy: 0.8942 - val_loss: 0.0938 - val_accuracy: 0.9778\n",
      "Epoch 127/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.3700 - accuracy: 0.8846 - val_loss: 0.0928 - val_accuracy: 0.9796\n",
      "Epoch 128/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.4017 - accuracy: 0.8727 - val_loss: 0.0914 - val_accuracy: 0.9796\n",
      "Epoch 129/210\n",
      "1257/1257 [==============================] - 0s 393us/step - loss: 0.3603 - accuracy: 0.8839 - val_loss: 0.0902 - val_accuracy: 0.9796\n",
      "Epoch 130/210\n",
      "1257/1257 [==============================] - 0s 370us/step - loss: 0.4043 - accuracy: 0.8735 - val_loss: 0.0892 - val_accuracy: 0.9796\n",
      "Epoch 131/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 0.3615 - accuracy: 0.8910 - val_loss: 0.0880 - val_accuracy: 0.9796\n",
      "Epoch 132/210\n",
      "1257/1257 [==============================] - 0s 347us/step - loss: 0.3447 - accuracy: 0.8703 - val_loss: 0.0870 - val_accuracy: 0.9796\n",
      "Epoch 133/210\n",
      "1257/1257 [==============================] - 0s 351us/step - loss: 0.3597 - accuracy: 0.8958 - val_loss: 0.0861 - val_accuracy: 0.9796\n",
      "Epoch 134/210\n",
      "1257/1257 [==============================] - 0s 369us/step - loss: 0.3443 - accuracy: 0.8942 - val_loss: 0.0850 - val_accuracy: 0.9778\n",
      "Epoch 135/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 0.3416 - accuracy: 0.8918 - val_loss: 0.0836 - val_accuracy: 0.9759\n",
      "Epoch 136/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.3442 - accuracy: 0.8870 - val_loss: 0.0827 - val_accuracy: 0.9778\n",
      "Epoch 137/210\n",
      "1257/1257 [==============================] - 0s 384us/step - loss: 0.3191 - accuracy: 0.8950 - val_loss: 0.0816 - val_accuracy: 0.9778\n",
      "Epoch 138/210\n",
      "1257/1257 [==============================] - 1s 412us/step - loss: 0.3177 - accuracy: 0.9037 - val_loss: 0.0807 - val_accuracy: 0.9778\n",
      "Epoch 139/210\n",
      "1257/1257 [==============================] - 1s 410us/step - loss: 0.3044 - accuracy: 0.8966 - val_loss: 0.0794 - val_accuracy: 0.9778\n",
      "Epoch 140/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 0.3419 - accuracy: 0.8894 - val_loss: 0.0785 - val_accuracy: 0.9778\n",
      "Epoch 141/210\n",
      "1257/1257 [==============================] - 0s 389us/step - loss: 0.3157 - accuracy: 0.8998 - val_loss: 0.0778 - val_accuracy: 0.9759\n",
      "Epoch 142/210\n",
      "1257/1257 [==============================] - 0s 385us/step - loss: 0.3612 - accuracy: 0.8934 - val_loss: 0.0774 - val_accuracy: 0.9778\n",
      "Epoch 143/210\n",
      "1257/1257 [==============================] - 1s 436us/step - loss: 0.3090 - accuracy: 0.9037 - val_loss: 0.0770 - val_accuracy: 0.9778\n",
      "Epoch 144/210\n",
      "1257/1257 [==============================] - 0s 396us/step - loss: 0.3262 - accuracy: 0.8831 - val_loss: 0.0770 - val_accuracy: 0.9759\n",
      "Epoch 145/210\n",
      "1257/1257 [==============================] - 0s 396us/step - loss: 0.2701 - accuracy: 0.9109 - val_loss: 0.0762 - val_accuracy: 0.9796\n",
      "Epoch 146/210\n",
      "1257/1257 [==============================] - 1s 399us/step - loss: 0.3568 - accuracy: 0.8878 - val_loss: 0.0761 - val_accuracy: 0.9796\n",
      "Epoch 147/210\n",
      "1257/1257 [==============================] - 0s 368us/step - loss: 0.3340 - accuracy: 0.8839 - val_loss: 0.0764 - val_accuracy: 0.9796\n",
      "Epoch 148/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 0.3438 - accuracy: 0.8926 - val_loss: 0.0766 - val_accuracy: 0.9796\n",
      "Epoch 149/210\n",
      "1257/1257 [==============================] - 0s 358us/step - loss: 0.3131 - accuracy: 0.9101 - val_loss: 0.0767 - val_accuracy: 0.9815\n",
      "Epoch 150/210\n",
      "1257/1257 [==============================] - 0s 367us/step - loss: 0.3237 - accuracy: 0.8998 - val_loss: 0.0770 - val_accuracy: 0.9796\n",
      "Epoch 151/210\n",
      "1257/1257 [==============================] - 0s 341us/step - loss: 0.3516 - accuracy: 0.8886 - val_loss: 0.0775 - val_accuracy: 0.9778\n",
      "Epoch 152/210\n",
      "1257/1257 [==============================] - 0s 367us/step - loss: 0.3111 - accuracy: 0.8966 - val_loss: 0.0773 - val_accuracy: 0.9778\n",
      "Epoch 153/210\n",
      "1257/1257 [==============================] - 0s 369us/step - loss: 0.3096 - accuracy: 0.8926 - val_loss: 0.0763 - val_accuracy: 0.9796\n",
      "Epoch 154/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 0.2938 - accuracy: 0.9045 - val_loss: 0.0753 - val_accuracy: 0.9796\n",
      "Epoch 155/210\n",
      "1257/1257 [==============================] - 0s 333us/step - loss: 0.2875 - accuracy: 0.9021 - val_loss: 0.0741 - val_accuracy: 0.9778\n",
      "Epoch 156/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 0.2652 - accuracy: 0.9053 - val_loss: 0.0725 - val_accuracy: 0.9815\n",
      "Epoch 157/210\n",
      "1257/1257 [==============================] - 0s 354us/step - loss: 0.2904 - accuracy: 0.9029 - val_loss: 0.0703 - val_accuracy: 0.9815\n",
      "Epoch 158/210\n",
      "1257/1257 [==============================] - 0s 391us/step - loss: 0.2611 - accuracy: 0.9220 - val_loss: 0.0678 - val_accuracy: 0.9815\n",
      "Epoch 159/210\n",
      "1257/1257 [==============================] - 0s 351us/step - loss: 0.3302 - accuracy: 0.8998 - val_loss: 0.0656 - val_accuracy: 0.9833\n",
      "Epoch 160/210\n",
      "1257/1257 [==============================] - 0s 361us/step - loss: 0.3016 - accuracy: 0.9101 - val_loss: 0.0640 - val_accuracy: 0.9833\n",
      "Epoch 161/210\n",
      "1257/1257 [==============================] - 0s 354us/step - loss: 0.2646 - accuracy: 0.9204 - val_loss: 0.0631 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 0.2951 - accuracy: 0.9109 - val_loss: 0.0632 - val_accuracy: 0.9852\n",
      "Epoch 163/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 0.2783 - accuracy: 0.9085 - val_loss: 0.0627 - val_accuracy: 0.9852\n",
      "Epoch 164/210\n",
      "1257/1257 [==============================] - 0s 349us/step - loss: 0.2800 - accuracy: 0.9014 - val_loss: 0.0622 - val_accuracy: 0.9833\n",
      "Epoch 165/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 0.2928 - accuracy: 0.9077 - val_loss: 0.0620 - val_accuracy: 0.9833\n",
      "Epoch 166/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 0.2481 - accuracy: 0.9212 - val_loss: 0.0618 - val_accuracy: 0.9833\n",
      "Epoch 167/210\n",
      "1257/1257 [==============================] - 0s 369us/step - loss: 0.2829 - accuracy: 0.9101 - val_loss: 0.0617 - val_accuracy: 0.9815\n",
      "Epoch 168/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.2657 - accuracy: 0.9157 - val_loss: 0.0613 - val_accuracy: 0.9815\n",
      "Epoch 169/210\n",
      "1257/1257 [==============================] - 0s 369us/step - loss: 0.2360 - accuracy: 0.9236 - val_loss: 0.0611 - val_accuracy: 0.9833\n",
      "Epoch 170/210\n",
      "1257/1257 [==============================] - 1s 408us/step - loss: 0.2487 - accuracy: 0.9220 - val_loss: 0.0611 - val_accuracy: 0.9815\n",
      "Epoch 171/210\n",
      "1257/1257 [==============================] - 1s 424us/step - loss: 0.2721 - accuracy: 0.9085 - val_loss: 0.0613 - val_accuracy: 0.9833\n",
      "Epoch 172/210\n",
      "1257/1257 [==============================] - 1s 423us/step - loss: 0.2631 - accuracy: 0.9260 - val_loss: 0.0609 - val_accuracy: 0.9833\n",
      "Epoch 173/210\n",
      "1257/1257 [==============================] - 1s 435us/step - loss: 0.2843 - accuracy: 0.8998 - val_loss: 0.0598 - val_accuracy: 0.9852\n",
      "Epoch 174/210\n",
      "1257/1257 [==============================] - 1s 454us/step - loss: 0.2772 - accuracy: 0.9085 - val_loss: 0.0591 - val_accuracy: 0.9870\n",
      "Epoch 175/210\n",
      "1257/1257 [==============================] - 1s 409us/step - loss: 0.2625 - accuracy: 0.9165 - val_loss: 0.0585 - val_accuracy: 0.9870\n",
      "Epoch 176/210\n",
      "1257/1257 [==============================] - 1s 419us/step - loss: 0.2607 - accuracy: 0.9189 - val_loss: 0.0582 - val_accuracy: 0.9833\n",
      "Epoch 177/210\n",
      "1257/1257 [==============================] - 1s 454us/step - loss: 0.2545 - accuracy: 0.9212 - val_loss: 0.0582 - val_accuracy: 0.9870\n",
      "Epoch 178/210\n",
      "1257/1257 [==============================] - 1s 442us/step - loss: 0.2637 - accuracy: 0.9157 - val_loss: 0.0579 - val_accuracy: 0.9870\n",
      "Epoch 179/210\n",
      "1257/1257 [==============================] - 1s 430us/step - loss: 0.2713 - accuracy: 0.9141 - val_loss: 0.0577 - val_accuracy: 0.9870\n",
      "Epoch 180/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.2562 - accuracy: 0.9125 - val_loss: 0.0577 - val_accuracy: 0.9852\n",
      "Epoch 181/210\n",
      "1257/1257 [==============================] - 0s 364us/step - loss: 0.2504 - accuracy: 0.9204 - val_loss: 0.0579 - val_accuracy: 0.9852\n",
      "Epoch 182/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.2497 - accuracy: 0.9196 - val_loss: 0.0575 - val_accuracy: 0.9852\n",
      "Epoch 183/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 0.2639 - accuracy: 0.9189 - val_loss: 0.0571 - val_accuracy: 0.9852\n",
      "Epoch 184/210\n",
      "1257/1257 [==============================] - 0s 355us/step - loss: 0.2191 - accuracy: 0.9292 - val_loss: 0.0568 - val_accuracy: 0.9833\n",
      "Epoch 185/210\n",
      "1257/1257 [==============================] - 0s 380us/step - loss: 0.2404 - accuracy: 0.9220 - val_loss: 0.0564 - val_accuracy: 0.9852\n",
      "Epoch 186/210\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 0.2503 - accuracy: 0.9165 - val_loss: 0.0561 - val_accuracy: 0.9852\n",
      "Epoch 187/210\n",
      "1257/1257 [==============================] - 0s 382us/step - loss: 0.2836 - accuracy: 0.9141 - val_loss: 0.0559 - val_accuracy: 0.9852\n",
      "Epoch 188/210\n",
      "1257/1257 [==============================] - 0s 385us/step - loss: 0.2555 - accuracy: 0.9069 - val_loss: 0.0558 - val_accuracy: 0.9852\n",
      "Epoch 189/210\n",
      "1257/1257 [==============================] - 0s 374us/step - loss: 0.2319 - accuracy: 0.9260 - val_loss: 0.0556 - val_accuracy: 0.9852\n",
      "Epoch 190/210\n",
      "1257/1257 [==============================] - 0s 374us/step - loss: 0.2350 - accuracy: 0.9276 - val_loss: 0.0554 - val_accuracy: 0.9852\n",
      "Epoch 191/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.2181 - accuracy: 0.9316 - val_loss: 0.0547 - val_accuracy: 0.9852\n",
      "Epoch 192/210\n",
      "1257/1257 [==============================] - 0s 372us/step - loss: 0.2279 - accuracy: 0.9252 - val_loss: 0.0538 - val_accuracy: 0.9852\n",
      "Epoch 193/210\n",
      "1257/1257 [==============================] - 0s 367us/step - loss: 0.2261 - accuracy: 0.9284 - val_loss: 0.0531 - val_accuracy: 0.9852\n",
      "Epoch 194/210\n",
      "1257/1257 [==============================] - 0s 388us/step - loss: 0.1953 - accuracy: 0.9356 - val_loss: 0.0521 - val_accuracy: 0.9852\n",
      "Epoch 195/210\n",
      "1257/1257 [==============================] - 0s 366us/step - loss: 0.2422 - accuracy: 0.9149 - val_loss: 0.0511 - val_accuracy: 0.9870\n",
      "Epoch 196/210\n",
      "1257/1257 [==============================] - 1s 432us/step - loss: 0.2421 - accuracy: 0.9173 - val_loss: 0.0500 - val_accuracy: 0.9852\n",
      "Epoch 197/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.2252 - accuracy: 0.9260 - val_loss: 0.0495 - val_accuracy: 0.9852\n",
      "Epoch 198/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 0.2223 - accuracy: 0.9284 - val_loss: 0.0492 - val_accuracy: 0.9870\n",
      "Epoch 199/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 0.2152 - accuracy: 0.9268 - val_loss: 0.0491 - val_accuracy: 0.9870\n",
      "Epoch 200/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 0.2199 - accuracy: 0.9220 - val_loss: 0.0491 - val_accuracy: 0.9852\n",
      "Epoch 201/210\n",
      "1257/1257 [==============================] - 0s 375us/step - loss: 0.2200 - accuracy: 0.9316 - val_loss: 0.0486 - val_accuracy: 0.9870\n",
      "Epoch 202/210\n",
      "1257/1257 [==============================] - 1s 414us/step - loss: 0.2236 - accuracy: 0.9292 - val_loss: 0.0488 - val_accuracy: 0.9852\n",
      "Epoch 203/210\n",
      "1257/1257 [==============================] - 0s 389us/step - loss: 0.1970 - accuracy: 0.9356 - val_loss: 0.0491 - val_accuracy: 0.9889\n",
      "Epoch 204/210\n",
      "1257/1257 [==============================] - 1s 425us/step - loss: 0.1862 - accuracy: 0.9403 - val_loss: 0.0495 - val_accuracy: 0.9889\n",
      "Epoch 205/210\n",
      "1257/1257 [==============================] - 1s 434us/step - loss: 0.2190 - accuracy: 0.9244 - val_loss: 0.0497 - val_accuracy: 0.9889\n",
      "Epoch 206/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.2068 - accuracy: 0.9340 - val_loss: 0.0492 - val_accuracy: 0.9889\n",
      "Epoch 207/210\n",
      "1257/1257 [==============================] - 1s 410us/step - loss: 0.2499 - accuracy: 0.9189 - val_loss: 0.0486 - val_accuracy: 0.9870\n",
      "Epoch 208/210\n",
      "1257/1257 [==============================] - 1s 474us/step - loss: 0.2139 - accuracy: 0.9284 - val_loss: 0.0483 - val_accuracy: 0.9870\n",
      "Epoch 209/210\n",
      "1257/1257 [==============================] - 1s 439us/step - loss: 0.2120 - accuracy: 0.9268 - val_loss: 0.0479 - val_accuracy: 0.9870\n",
      "Epoch 210/210\n",
      "1257/1257 [==============================] - 1s 468us/step - loss: 0.2156 - accuracy: 0.9252 - val_loss: 0.0477 - val_accuracy: 0.9870\n"
     ]
    }
   ],
   "source": [
    "# Parametres\n",
    "verbose, epochs, batch_size = 1, 210, 1750\n",
    "# initialize the model object\n",
    "clf_cnn = CNN_net()\n",
    "# fit network\n",
    "history = clf_cnn.fit(x_train, y_train_binary, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=verbose, validation_data=(x_test, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 [==============================] - 0s 115us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04769636881020334, 0.9870370626449585]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call predict to get predictions Report the accuracy\n",
    "clf_cnn.evaluate(x_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN : Accuracy: 0.9851851851851852\n"
     ]
    }
   ],
   "source": [
    "# call predict to get predictions\n",
    "y_pred = clf_cnn.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Report the accuracy\n",
    "accuracy_CNN = accuracy_score(y_test_binary, y_pred)\n",
    "print(\"CNN : Accuracy: \" + str(accuracy_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 62, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 60, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 28, 64)            6208      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 26, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 832)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                53312     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 75,754\n",
      "Trainable params: 75,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss','Validation loss'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training acc','Validation acc'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# from the first Fully-Connected layer \n",
    "layer_name = 'dense_1'\n",
    "intermediate_layer_model = Model(inputs=clf_cnn.input,\n",
    "                                 outputs=clf_cnn.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features of the train dataset to use it in future.\n",
    "out_cnn_train = intermediate_layer_model.predict(x_train)\n",
    "# Save the features of the test dataset to use it in future.\n",
    "out_cnn_test = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (from CNN) Shape: (1257, 64)\n",
      "Training Labels (from CNN) Shape: (1257,)\n",
      "Test Features (from CNN) Shape: (540, 64)\n",
      "Test Labels (from CNN) Shape: (540,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features (from CNN) Shape:', out_cnn_train.shape)\n",
    "print('Training Labels (from CNN) Shape:', y_train.shape)\n",
    "\n",
    "print('Test Features (from CNN) Shape:', out_cnn_test.shape)\n",
    "print('Test Labels (from CNN) Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + Random Forest + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djinn iris\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:262: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:263: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:266: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:302: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:286: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:320: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:328: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:333: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:334: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn_fns.py:335: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch: 0001 cost= 2.235153381 accuracy= 0.211\n",
      "Epoch: 0002 cost= 1.991475919 accuracy= 0.241\n",
      "Epoch: 0003 cost= 1.925601882 accuracy= 0.320\n",
      "Epoch: 0004 cost= 1.765781010 accuracy= 0.332\n",
      "Epoch: 0005 cost= 1.546210703 accuracy= 0.370\n",
      "Epoch: 0006 cost= 1.424424753 accuracy= 0.478\n",
      "Epoch: 0007 cost= 1.333667699 accuracy= 0.601\n",
      "Epoch: 0008 cost= 1.195766021 accuracy= 0.681\n",
      "Epoch: 0009 cost= 0.990500068 accuracy= 0.795\n",
      "Epoch: 0010 cost= 0.679933131 accuracy= 0.823\n",
      "Epoch: 0011 cost= 0.632141876 accuracy= 0.817\n",
      "Epoch: 0012 cost= 0.503373646 accuracy= 0.879\n",
      "Epoch: 0013 cost= 0.389896103 accuracy= 0.875\n",
      "Epoch: 0014 cost= 0.333883261 accuracy= 0.925\n",
      "Epoch: 0015 cost= 0.293846459 accuracy= 0.900\n",
      "Epoch: 0016 cost= 0.245375172 accuracy= 0.919\n",
      "Epoch: 0017 cost= 0.253632069 accuracy= 0.934\n",
      "Epoch: 0018 cost= 0.224106852 accuracy= 0.934\n",
      "Epoch: 0019 cost= 0.233673183 accuracy= 0.944\n",
      "Epoch: 0020 cost= 0.170974000 accuracy= 0.956\n",
      "Epoch: 0021 cost= 0.179035341 accuracy= 0.956\n",
      "Epoch: 0022 cost= 0.184534638 accuracy= 0.955\n",
      "Epoch: 0023 cost= 0.149923267 accuracy= 0.959\n",
      "Epoch: 0024 cost= 0.202362444 accuracy= 0.956\n",
      "Epoch: 0025 cost= 0.190228336 accuracy= 0.952\n",
      "Epoch: 0026 cost= 0.162484718 accuracy= 0.953\n",
      "Epoch: 0027 cost= 0.161914895 accuracy= 0.955\n",
      "Epoch: 0028 cost= 0.177094035 accuracy= 0.946\n",
      "Epoch: 0029 cost= 0.166892440 accuracy= 0.946\n",
      "Epoch: 0030 cost= 0.172098027 accuracy= 0.963\n",
      "Epoch: 0031 cost= 0.103848978 accuracy= 0.962\n",
      "Epoch: 0032 cost= 0.113418786 accuracy= 0.966\n",
      "Epoch: 0033 cost= 0.130773529 accuracy= 0.972\n",
      "Epoch: 0034 cost= 0.143345713 accuracy= 0.973\n",
      "Epoch: 0035 cost= 0.102652255 accuracy= 0.967\n",
      "Epoch: 0036 cost= 0.161066366 accuracy= 0.959\n",
      "Epoch: 0037 cost= 0.097360250 accuracy= 0.966\n",
      "Epoch: 0038 cost= 0.098893042 accuracy= 0.974\n",
      "Epoch: 0039 cost= 0.089111268 accuracy= 0.975\n",
      "Epoch: 0040 cost= 0.079749855 accuracy= 0.979\n",
      "Epoch: 0041 cost= 0.070878283 accuracy= 0.978\n",
      "Epoch: 0042 cost= 0.070301143 accuracy= 0.977\n",
      "Epoch: 0043 cost= 0.074408332 accuracy= 0.978\n",
      "Epoch: 0044 cost= 0.107700510 accuracy= 0.973\n",
      "Epoch: 0045 cost= 0.089158476 accuracy= 0.976\n",
      "Epoch: 0046 cost= 0.075088933 accuracy= 0.980\n",
      "Epoch: 0047 cost= 0.075035840 accuracy= 0.978\n",
      "Epoch: 0048 cost= 0.085644759 accuracy= 0.975\n",
      "Epoch: 0049 cost= 0.066381148 accuracy= 0.981\n",
      "Epoch: 0050 cost= 0.076204303 accuracy= 0.975\n",
      "Epoch: 0051 cost= 0.068824742 accuracy= 0.958\n",
      "Epoch: 0052 cost= 0.070642209 accuracy= 0.985\n",
      "Epoch: 0053 cost= 0.071887501 accuracy= 0.963\n",
      "Epoch: 0054 cost= 0.124732835 accuracy= 0.977\n",
      "Epoch: 0055 cost= 0.079093985 accuracy= 0.977\n",
      "Epoch: 0056 cost= 0.068824001 accuracy= 0.977\n",
      "Epoch: 0057 cost= 0.048161645 accuracy= 0.981\n",
      "Epoch: 0058 cost= 0.054354865 accuracy= 0.962\n",
      "Epoch: 0059 cost= 0.074192908 accuracy= 0.982\n",
      "Epoch: 0060 cost= 0.072386798 accuracy= 0.989\n",
      "Epoch: 0061 cost= 0.064311609 accuracy= 0.981\n",
      "Epoch: 0062 cost= 0.051812618 accuracy= 0.984\n",
      "Epoch: 0063 cost= 0.031273718 accuracy= 0.985\n",
      "Epoch: 0064 cost= 0.046704135 accuracy= 0.976\n",
      "Epoch: 0065 cost= 0.046154678 accuracy= 0.987\n",
      "Epoch: 0066 cost= 0.040925084 accuracy= 0.987\n",
      "Epoch: 0067 cost= 0.031589828 accuracy= 0.988\n",
      "Epoch: 0068 cost= 0.037253294 accuracy= 0.991\n",
      "Epoch: 0069 cost= 0.043629459 accuracy= 0.987\n",
      "Epoch: 0070 cost= 0.052918323 accuracy= 0.983\n",
      "Epoch: 0071 cost= 0.029333569 accuracy= 0.990\n",
      "Epoch: 0072 cost= 0.024232364 accuracy= 0.994\n",
      "Epoch: 0073 cost= 0.025966439 accuracy= 0.992\n",
      "Epoch: 0074 cost= 0.057298956 accuracy= 0.993\n",
      "Epoch: 0075 cost= 0.049366204 accuracy= 0.990\n",
      "Epoch: 0076 cost= 0.031731312 accuracy= 0.979\n",
      "Epoch: 0077 cost= 0.029113123 accuracy= 0.988\n",
      "Epoch: 0078 cost= 0.035232186 accuracy= 0.990\n",
      "Epoch: 0079 cost= 0.054976759 accuracy= 0.974\n",
      "Epoch: 0080 cost= 0.025748786 accuracy= 0.995\n",
      "Epoch: 0081 cost= 0.036267904 accuracy= 0.989\n",
      "Epoch: 0082 cost= 0.034803464 accuracy= 0.993\n",
      "Epoch: 0083 cost= 0.037627335 accuracy= 0.983\n",
      "Epoch: 0084 cost= 0.044453233 accuracy= 0.996\n",
      "Epoch: 0085 cost= 0.017398741 accuracy= 0.996\n",
      "Epoch: 0086 cost= 0.019871030 accuracy= 0.993\n",
      "Epoch: 0087 cost= 0.018468316 accuracy= 0.992\n",
      "Epoch: 0088 cost= 0.029702835 accuracy= 0.993\n",
      "Epoch: 0089 cost= 0.035226808 accuracy= 0.995\n",
      "Epoch: 0090 cost= 0.018065032 accuracy= 0.989\n",
      "Epoch: 0091 cost= 0.020100950 accuracy= 0.994\n",
      "Epoch: 0092 cost= 0.012551940 accuracy= 0.993\n",
      "Epoch: 0093 cost= 0.031981550 accuracy= 0.992\n",
      "Epoch: 0094 cost= 0.029031380 accuracy= 0.994\n",
      "Epoch: 0095 cost= 0.024311096 accuracy= 0.995\n",
      "Epoch: 0096 cost= 0.021202559 accuracy= 0.994\n",
      "Epoch: 0097 cost= 0.018125693 accuracy= 0.997\n",
      "Epoch: 0098 cost= 0.017784482 accuracy= 0.996\n",
      "Epoch: 0099 cost= 0.013726377 accuracy= 0.990\n",
      "Epoch: 0100 cost= 0.025902486 accuracy= 0.996\n",
      "Epoch: 0101 cost= 0.010845080 accuracy= 0.996\n",
      "Epoch: 0102 cost= 0.020834585 accuracy= 0.998\n",
      "Epoch: 0103 cost= 0.018466440 accuracy= 0.994\n",
      "Epoch: 0104 cost= 0.018436976 accuracy= 0.989\n",
      "Epoch: 0105 cost= 0.033294533 accuracy= 0.996\n",
      "Epoch: 0106 cost= 0.038656382 accuracy= 0.989\n",
      "Epoch: 0107 cost= 0.046948449 accuracy= 0.981\n",
      "Epoch: 0108 cost= 0.038530833 accuracy= 0.996\n",
      "Epoch: 0109 cost= 0.022785467 accuracy= 0.989\n",
      "Epoch: 0110 cost= 0.020306287 accuracy= 0.997\n",
      "Epoch: 0111 cost= 0.017976900 accuracy= 0.998\n",
      "Epoch: 0112 cost= 0.007580593 accuracy= 0.995\n",
      "Epoch: 0113 cost= 0.007883479 accuracy= 0.998\n",
      "Epoch: 0114 cost= 0.006863713 accuracy= 0.997\n",
      "Epoch: 0115 cost= 0.010409028 accuracy= 0.996\n",
      "Epoch: 0116 cost= 0.003196680 accuracy= 0.999\n",
      "Epoch: 0117 cost= 0.006209033 accuracy= 0.998\n",
      "Epoch: 0118 cost= 0.006162373 accuracy= 0.999\n",
      "Epoch: 0119 cost= 0.004465115 accuracy= 0.999\n",
      "Epoch: 0120 cost= 0.002907858 accuracy= 0.999\n",
      "Epoch: 0121 cost= 0.007416717 accuracy= 0.996\n",
      "Epoch: 0122 cost= 0.008132036 accuracy= 0.996\n",
      "Epoch: 0123 cost= 0.004665489 accuracy= 0.998\n",
      "Epoch: 0124 cost= 0.027191800 accuracy= 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0125 cost= 0.068710428 accuracy= 0.958\n",
      "Epoch: 0126 cost= 0.088246424 accuracy= 0.984\n",
      "Epoch: 0127 cost= 0.034608884 accuracy= 0.988\n",
      "Epoch: 0128 cost= 0.051037873 accuracy= 0.996\n",
      "Epoch: 0129 cost= 0.013150534 accuracy= 0.992\n",
      "Epoch: 0130 cost= 0.020054810 accuracy= 0.997\n",
      "Epoch: 0131 cost= 0.008173767 accuracy= 0.996\n",
      "Epoch: 0132 cost= 0.002684931 accuracy= 0.999\n",
      "Epoch: 0133 cost= 0.001808657 accuracy= 0.998\n",
      "Epoch: 0134 cost= 0.008957625 accuracy= 1.000\n",
      "Epoch: 0135 cost= 0.006157279 accuracy= 1.000\n",
      "Epoch: 0136 cost= 0.002336000 accuracy= 0.997\n",
      "Epoch: 0137 cost= 0.006455984 accuracy= 0.998\n",
      "Epoch: 0138 cost= 0.001929199 accuracy= 0.999\n",
      "Epoch: 0139 cost= 0.004485132 accuracy= 1.000\n",
      "Epoch: 0140 cost= 0.001596343 accuracy= 0.999\n",
      "Epoch: 0141 cost= 0.002334930 accuracy= 0.997\n",
      "Epoch: 0142 cost= 0.005478027 accuracy= 0.999\n",
      "Epoch: 0143 cost= 0.006126069 accuracy= 0.998\n",
      "Epoch: 0144 cost= 0.005601927 accuracy= 0.999\n",
      "Epoch: 0145 cost= 0.002007980 accuracy= 0.999\n",
      "Epoch: 0146 cost= 0.001605325 accuracy= 0.999\n",
      "Epoch: 0147 cost= 0.002275057 accuracy= 1.000\n",
      "Epoch: 0148 cost= 0.003077006 accuracy= 1.000\n",
      "Epoch: 0149 cost= 0.002191357 accuracy= 0.999\n",
      "Epoch: 0150 cost= 0.001822579 accuracy= 1.000\n",
      "Epoch: 0151 cost= 0.002203616 accuracy= 0.997\n",
      "Epoch: 0152 cost= 0.008728816 accuracy= 0.998\n",
      "Epoch: 0153 cost= 0.003064985 accuracy= 0.993\n",
      "Epoch: 0154 cost= 0.015418251 accuracy= 0.999\n",
      "Epoch: 0155 cost= 0.003942124 accuracy= 1.000\n",
      "Epoch: 0156 cost= 0.001521697 accuracy= 1.000\n",
      "Epoch: 0157 cost= 0.001975463 accuracy= 1.000\n",
      "Epoch: 0158 cost= 0.001763340 accuracy= 1.000\n",
      "Epoch: 0159 cost= 0.001492310 accuracy= 1.000\n",
      "Epoch: 0160 cost= 0.000867585 accuracy= 1.000\n",
      "Epoch: 0161 cost= 0.002151083 accuracy= 0.999\n",
      "Epoch: 0162 cost= 0.003895434 accuracy= 0.998\n",
      "Epoch: 0163 cost= 0.002048255 accuracy= 0.999\n",
      "Epoch: 0164 cost= 0.001435200 accuracy= 1.000\n",
      "Epoch: 0165 cost= 0.002834580 accuracy= 0.998\n",
      "Epoch: 0166 cost= 0.001195662 accuracy= 0.999\n",
      "Epoch: 0167 cost= 0.002947064 accuracy= 0.996\n",
      "Epoch: 0168 cost= 0.007710350 accuracy= 0.994\n",
      "Epoch: 0169 cost= 0.009267046 accuracy= 1.000\n",
      "Epoch: 0170 cost= 0.008784349 accuracy= 0.996\n",
      "Epoch: 0171 cost= 0.003640831 accuracy= 0.996\n",
      "Epoch: 0172 cost= 0.041560250 accuracy= 0.989\n",
      "Epoch: 0173 cost= 0.041160667 accuracy= 0.977\n",
      "Epoch: 0174 cost= 0.068828987 accuracy= 0.977\n",
      "Epoch: 0175 cost= 0.016425535 accuracy= 0.996\n",
      "Epoch: 0176 cost= 0.010208994 accuracy= 0.993\n",
      "Epoch: 0177 cost= 0.012940182 accuracy= 0.998\n",
      "Epoch: 0178 cost= 0.005007927 accuracy= 0.996\n",
      "Epoch: 0179 cost= 0.025124489 accuracy= 0.998\n",
      "Epoch: 0180 cost= 0.013493481 accuracy= 0.998\n",
      "Epoch: 0181 cost= 0.006249333 accuracy= 0.996\n",
      "Epoch: 0182 cost= 0.004282908 accuracy= 0.998\n",
      "Epoch: 0183 cost= 0.005175465 accuracy= 1.000\n",
      "Epoch: 0184 cost= 0.009079547 accuracy= 0.996\n",
      "Epoch: 0185 cost= 0.015722306 accuracy= 0.997\n",
      "Epoch: 0186 cost= 0.013886213 accuracy= 0.997\n",
      "Epoch: 0187 cost= 0.016783935 accuracy= 0.997\n",
      "Epoch: 0188 cost= 0.002628494 accuracy= 0.999\n",
      "Epoch: 0189 cost= 0.007870503 accuracy= 1.000\n",
      "Epoch: 0190 cost= 0.013514369 accuracy= 0.996\n",
      "Epoch: 0191 cost= 0.002924948 accuracy= 0.999\n",
      "Epoch: 0192 cost= 0.001075773 accuracy= 1.000\n",
      "Epoch: 0193 cost= 0.001131570 accuracy= 0.999\n",
      "Epoch: 0194 cost= 0.001003845 accuracy= 1.000\n",
      "Epoch: 0195 cost= 0.000335763 accuracy= 0.999\n",
      "Epoch: 0196 cost= 0.001651136 accuracy= 0.998\n",
      "Epoch: 0197 cost= 0.001580117 accuracy= 0.999\n",
      "Epoch: 0198 cost= 0.000962339 accuracy= 0.999\n",
      "Epoch: 0199 cost= 0.000374997 accuracy= 0.999\n",
      "Epoch: 0200 cost= 0.000497856 accuracy= 0.999\n",
      "Epoch: 0201 cost= 0.000326930 accuracy= 0.999\n",
      "Epoch: 0202 cost= 0.001722627 accuracy= 1.000\n",
      "Epoch: 0203 cost= 0.005441117 accuracy= 0.996\n",
      "Epoch: 0204 cost= 0.020230424 accuracy= 0.997\n",
      "Epoch: 0205 cost= 0.006107539 accuracy= 0.998\n",
      "Epoch: 0206 cost= 0.004242213 accuracy= 1.000\n",
      "Epoch: 0207 cost= 0.001185878 accuracy= 1.000\n",
      "Epoch: 0208 cost= 0.000527011 accuracy= 0.999\n",
      "Epoch: 0209 cost= 0.000756016 accuracy= 1.000\n",
      "Epoch: 0210 cost= 0.000734342 accuracy= 1.000\n",
      "Epoch: 0211 cost= 0.003112784 accuracy= 1.000\n",
      "Epoch: 0212 cost= 0.017152041 accuracy= 0.995\n",
      "Epoch: 0213 cost= 0.006138469 accuracy= 1.000\n",
      "Epoch: 0214 cost= 0.009760053 accuracy= 0.999\n",
      "Epoch: 0215 cost= 0.025344994 accuracy= 0.996\n",
      "Epoch: 0216 cost= 0.053758453 accuracy= 0.963\n",
      "Epoch: 0217 cost= 0.247950270 accuracy= 0.945\n",
      "Epoch: 0218 cost= 0.116780787 accuracy= 0.985\n",
      "Epoch: 0219 cost= 0.039273733 accuracy= 0.985\n",
      "Epoch: 0220 cost= 0.018381374 accuracy= 0.995\n",
      "Epoch: 0221 cost= 0.009214877 accuracy= 0.993\n",
      "Epoch: 0222 cost= 0.004280607 accuracy= 0.997\n",
      "Epoch: 0223 cost= 0.007287099 accuracy= 0.996\n",
      "Epoch: 0224 cost= 0.005800769 accuracy= 1.000\n",
      "Epoch: 0225 cost= 0.006339367 accuracy= 0.994\n",
      "Epoch: 0226 cost= 0.013488085 accuracy= 0.993\n",
      "Epoch: 0227 cost= 0.006218817 accuracy= 0.998\n",
      "Epoch: 0228 cost= 0.001191349 accuracy= 0.999\n",
      "Epoch: 0229 cost= 0.002408853 accuracy= 1.000\n",
      "Epoch: 0230 cost= 0.001014877 accuracy= 1.000\n",
      "Epoch: 0231 cost= 0.001146094 accuracy= 0.999\n",
      "Epoch: 0232 cost= 0.000411302 accuracy= 1.000\n",
      "Epoch: 0233 cost= 0.000837500 accuracy= 1.000\n",
      "Epoch: 0234 cost= 0.000399674 accuracy= 1.000\n",
      "Epoch: 0235 cost= 0.000441036 accuracy= 1.000\n",
      "Epoch: 0236 cost= 0.000323953 accuracy= 1.000\n",
      "Epoch: 0237 cost= 0.000537710 accuracy= 1.000\n",
      "Epoch: 0238 cost= 0.000400581 accuracy= 1.000\n",
      "Epoch: 0239 cost= 0.000326857 accuracy= 1.000\n",
      "Epoch: 0240 cost= 0.000354942 accuracy= 1.000\n",
      "Epoch: 0241 cost= 0.000317634 accuracy= 1.000\n",
      "Epoch: 0242 cost= 0.000372198 accuracy= 1.000\n",
      "Epoch: 0243 cost= 0.000207526 accuracy= 1.000\n",
      "Epoch: 0244 cost= 0.000317754 accuracy= 1.000\n",
      "Epoch: 0245 cost= 0.000248771 accuracy= 1.000\n",
      "Epoch: 0246 cost= 0.000366672 accuracy= 1.000\n",
      "Epoch: 0247 cost= 0.000253926 accuracy= 1.000\n",
      "Epoch: 0248 cost= 0.000274652 accuracy= 1.000\n",
      "Epoch: 0249 cost= 0.000260583 accuracy= 1.000\n",
      "Epoch: 0250 cost= 0.000256123 accuracy= 1.000\n",
      "Epoch: 0251 cost= 0.000117624 accuracy= 1.000\n",
      "Epoch: 0252 cost= 0.000286613 accuracy= 1.000\n",
      "Epoch: 0253 cost= 0.000284490 accuracy= 1.000\n",
      "Epoch: 0254 cost= 0.000174656 accuracy= 1.000\n",
      "Epoch: 0255 cost= 0.000247252 accuracy= 1.000\n",
      "Epoch: 0256 cost= 0.000181074 accuracy= 1.000\n",
      "Epoch: 0257 cost= 0.000187809 accuracy= 1.000\n",
      "Epoch: 0258 cost= 0.000214039 accuracy= 1.000\n",
      "Epoch: 0259 cost= 0.000139780 accuracy= 1.000\n",
      "Epoch: 0260 cost= 0.000226616 accuracy= 1.000\n",
      "Epoch: 0261 cost= 0.000168267 accuracy= 1.000\n",
      "Epoch: 0262 cost= 0.000172408 accuracy= 1.000\n",
      "Epoch: 0263 cost= 0.000172054 accuracy= 1.000\n",
      "Epoch: 0264 cost= 0.000188383 accuracy= 1.000\n",
      "Epoch: 0265 cost= 0.000141609 accuracy= 1.000\n",
      "Epoch: 0266 cost= 0.000111303 accuracy= 1.000\n",
      "Epoch: 0267 cost= 0.000117312 accuracy= 1.000\n",
      "Epoch: 0268 cost= 0.000118851 accuracy= 1.000\n",
      "Epoch: 0269 cost= 0.000186941 accuracy= 1.000\n",
      "Epoch: 0270 cost= 0.000157900 accuracy= 1.000\n",
      "Epoch: 0271 cost= 0.000127298 accuracy= 1.000\n",
      "Epoch: 0272 cost= 0.000132506 accuracy= 1.000\n",
      "Epoch: 0273 cost= 0.000105614 accuracy= 1.000\n",
      "Epoch: 0274 cost= 0.000219055 accuracy= 1.000\n",
      "Epoch: 0275 cost= 0.000167398 accuracy= 1.000\n",
      "Epoch: 0276 cost= 0.000146852 accuracy= 1.000\n",
      "Epoch: 0277 cost= 0.000170712 accuracy= 1.000\n",
      "Epoch: 0278 cost= 0.000127198 accuracy= 1.000\n",
      "Epoch: 0279 cost= 0.000139114 accuracy= 1.000\n",
      "Epoch: 0280 cost= 0.000135527 accuracy= 1.000\n",
      "Epoch: 0281 cost= 0.000179432 accuracy= 1.000\n",
      "Epoch: 0282 cost= 0.000137676 accuracy= 1.000\n",
      "Epoch: 0283 cost= 0.000070258 accuracy= 1.000\n",
      "Epoch: 0284 cost= 0.000107729 accuracy= 1.000\n",
      "Epoch: 0285 cost= 0.000126848 accuracy= 1.000\n",
      "Epoch: 0286 cost= 0.000106410 accuracy= 1.000\n",
      "Epoch: 0287 cost= 0.000135182 accuracy= 1.000\n",
      "Epoch: 0288 cost= 0.000162936 accuracy= 1.000\n",
      "Epoch: 0289 cost= 0.000062391 accuracy= 1.000\n",
      "Epoch: 0290 cost= 0.000117162 accuracy= 1.000\n",
      "Epoch: 0291 cost= 0.000128580 accuracy= 1.000\n",
      "Epoch: 0292 cost= 0.000125690 accuracy= 1.000\n",
      "Epoch: 0293 cost= 0.000093171 accuracy= 1.000\n",
      "Epoch: 0294 cost= 0.000124938 accuracy= 1.000\n",
      "Epoch: 0295 cost= 0.000109245 accuracy= 1.000\n",
      "Epoch: 0296 cost= 0.000124284 accuracy= 1.000\n",
      "Epoch: 0297 cost= 0.000079604 accuracy= 1.000\n",
      "Epoch: 0298 cost= 0.000094603 accuracy= 1.000\n",
      "Epoch: 0299 cost= 0.000085303 accuracy= 1.000\n",
      "Epoch: 0300 cost= 0.000103310 accuracy= 1.000\n",
      "Epoch: 0301 cost= 0.000098316 accuracy= 1.000\n",
      "Epoch: 0302 cost= 0.000094455 accuracy= 1.000\n",
      "Epoch: 0303 cost= 0.000108089 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0304 cost= 0.000095530 accuracy= 1.000\n",
      "Epoch: 0305 cost= 0.000088224 accuracy= 1.000\n",
      "Epoch: 0306 cost= 0.000080109 accuracy= 1.000\n",
      "Epoch: 0307 cost= 0.000078637 accuracy= 1.000\n",
      "Epoch: 0308 cost= 0.000091691 accuracy= 1.000\n",
      "Epoch: 0309 cost= 0.000073209 accuracy= 1.000\n",
      "Epoch: 0310 cost= 0.000082071 accuracy= 1.000\n",
      "Epoch: 0311 cost= 0.000120115 accuracy= 1.000\n",
      "Epoch: 0312 cost= 0.000073625 accuracy= 1.000\n",
      "Epoch: 0313 cost= 0.000085401 accuracy= 1.000\n",
      "Epoch: 0314 cost= 0.000063744 accuracy= 1.000\n",
      "Epoch: 0315 cost= 0.000075384 accuracy= 1.000\n",
      "Epoch: 0316 cost= 0.000094087 accuracy= 1.000\n",
      "Epoch: 0317 cost= 0.000100812 accuracy= 1.000\n",
      "Epoch: 0318 cost= 0.000072175 accuracy= 1.000\n",
      "Epoch: 0319 cost= 0.000092579 accuracy= 1.000\n",
      "Epoch: 0320 cost= 0.000047397 accuracy= 1.000\n",
      "Epoch: 0321 cost= 0.000082754 accuracy= 1.000\n",
      "Epoch: 0322 cost= 0.000066783 accuracy= 1.000\n",
      "Epoch: 0323 cost= 0.000058200 accuracy= 1.000\n",
      "Epoch: 0324 cost= 0.000058969 accuracy= 1.000\n",
      "Epoch: 0325 cost= 0.000079495 accuracy= 1.000\n",
      "Epoch: 0326 cost= 0.000064502 accuracy= 1.000\n",
      "Epoch: 0327 cost= 0.000068413 accuracy= 1.000\n",
      "Epoch: 0328 cost= 0.000061965 accuracy= 1.000\n",
      "Epoch: 0329 cost= 0.000043506 accuracy= 1.000\n",
      "Epoch: 0330 cost= 0.000062805 accuracy= 1.000\n",
      "Epoch: 0331 cost= 0.000071792 accuracy= 1.000\n",
      "Epoch: 0332 cost= 0.000080369 accuracy= 1.000\n",
      "Epoch: 0333 cost= 0.000070603 accuracy= 1.000\n",
      "Epoch: 0334 cost= 0.000075289 accuracy= 1.000\n",
      "Epoch: 0335 cost= 0.000085865 accuracy= 1.000\n",
      "Epoch: 0336 cost= 0.000059754 accuracy= 1.000\n",
      "Epoch: 0337 cost= 0.000069962 accuracy= 1.000\n",
      "Epoch: 0338 cost= 0.000054577 accuracy= 1.000\n",
      "Epoch: 0339 cost= 0.000047734 accuracy= 1.000\n",
      "Epoch: 0340 cost= 0.000065130 accuracy= 1.000\n",
      "Epoch: 0341 cost= 0.000044550 accuracy= 1.000\n",
      "Epoch: 0342 cost= 0.000069137 accuracy= 1.000\n",
      "Epoch: 0343 cost= 0.000044740 accuracy= 1.000\n",
      "Epoch: 0344 cost= 0.000059296 accuracy= 1.000\n",
      "Epoch: 0345 cost= 0.000029732 accuracy= 1.000\n",
      "Epoch: 0346 cost= 0.000062342 accuracy= 1.000\n",
      "Epoch: 0347 cost= 0.000054106 accuracy= 1.000\n",
      "Epoch: 0348 cost= 0.000054349 accuracy= 1.000\n",
      "Epoch: 0349 cost= 0.000059376 accuracy= 1.000\n",
      "Epoch: 0350 cost= 0.000040543 accuracy= 1.000\n",
      "Epoch: 0351 cost= 0.000055868 accuracy= 1.000\n",
      "Epoch: 0352 cost= 0.000039273 accuracy= 1.000\n",
      "Epoch: 0353 cost= 0.000071018 accuracy= 1.000\n",
      "Epoch: 0354 cost= 0.000056670 accuracy= 1.000\n",
      "Epoch: 0355 cost= 0.000058389 accuracy= 1.000\n",
      "Epoch: 0356 cost= 0.000037885 accuracy= 1.000\n",
      "Epoch: 0357 cost= 0.000059326 accuracy= 1.000\n",
      "Epoch: 0358 cost= 0.000040870 accuracy= 1.000\n",
      "Epoch: 0359 cost= 0.000051098 accuracy= 1.000\n",
      "Epoch: 0360 cost= 0.000050739 accuracy= 1.000\n",
      "Epoch: 0361 cost= 0.000051134 accuracy= 1.000\n",
      "Epoch: 0362 cost= 0.000032720 accuracy= 1.000\n",
      "Epoch: 0363 cost= 0.000030914 accuracy= 1.000\n",
      "Epoch: 0364 cost= 0.000030213 accuracy= 1.000\n",
      "Epoch: 0365 cost= 0.000049411 accuracy= 1.000\n",
      "Epoch: 0366 cost= 0.000040046 accuracy= 1.000\n",
      "Epoch: 0367 cost= 0.000044843 accuracy= 1.000\n",
      "Epoch: 0368 cost= 0.000042770 accuracy= 1.000\n",
      "Epoch: 0369 cost= 0.000034498 accuracy= 1.000\n",
      "Epoch: 0370 cost= 0.000041701 accuracy= 1.000\n",
      "Epoch: 0371 cost= 0.000034563 accuracy= 1.000\n",
      "Epoch: 0372 cost= 0.000065917 accuracy= 1.000\n",
      "Epoch: 0373 cost= 0.000041422 accuracy= 1.000\n",
      "Epoch: 0374 cost= 0.000031347 accuracy= 1.000\n",
      "Epoch: 0375 cost= 0.000039313 accuracy= 1.000\n",
      "Epoch: 0376 cost= 0.000053225 accuracy= 1.000\n",
      "Epoch: 0377 cost= 0.000051530 accuracy= 1.000\n",
      "Epoch: 0378 cost= 0.000031933 accuracy= 1.000\n",
      "Epoch: 0379 cost= 0.000040378 accuracy= 1.000\n",
      "Epoch: 0380 cost= 0.000035526 accuracy= 1.000\n",
      "Epoch: 0381 cost= 0.000041143 accuracy= 1.000\n",
      "Epoch: 0382 cost= 0.000035216 accuracy= 1.000\n",
      "Epoch: 0383 cost= 0.000028577 accuracy= 1.000\n",
      "Epoch: 0384 cost= 0.000055131 accuracy= 1.000\n",
      "Epoch: 0385 cost= 0.000025895 accuracy= 1.000\n",
      "Epoch: 0386 cost= 0.000029729 accuracy= 1.000\n",
      "Epoch: 0387 cost= 0.000038102 accuracy= 1.000\n",
      "Epoch: 0388 cost= 0.000035433 accuracy= 1.000\n",
      "Epoch: 0389 cost= 0.000030911 accuracy= 1.000\n",
      "Epoch: 0390 cost= 0.000020847 accuracy= 1.000\n",
      "Epoch: 0391 cost= 0.000042286 accuracy= 1.000\n",
      "Epoch: 0392 cost= 0.000033786 accuracy= 1.000\n",
      "Epoch: 0393 cost= 0.000036979 accuracy= 1.000\n",
      "Epoch: 0394 cost= 0.000031132 accuracy= 1.000\n",
      "Epoch: 0395 cost= 0.000025889 accuracy= 1.000\n",
      "Epoch: 0396 cost= 0.000030427 accuracy= 1.000\n",
      "Epoch: 0397 cost= 0.000034628 accuracy= 1.000\n",
      "Epoch: 0398 cost= 0.000041740 accuracy= 1.000\n",
      "Epoch: 0399 cost= 0.000033972 accuracy= 1.000\n",
      "Epoch: 0400 cost= 0.000031274 accuracy= 1.000\n",
      "Epoch: 0401 cost= 0.000024983 accuracy= 1.000\n",
      "Epoch: 0402 cost= 0.000033591 accuracy= 1.000\n",
      "Epoch: 0403 cost= 0.000030675 accuracy= 1.000\n",
      "Epoch: 0404 cost= 0.000028897 accuracy= 1.000\n",
      "Epoch: 0405 cost= 0.000032327 accuracy= 1.000\n",
      "Epoch: 0406 cost= 0.000031869 accuracy= 1.000\n",
      "Epoch: 0407 cost= 0.000024263 accuracy= 1.000\n",
      "Epoch: 0408 cost= 0.000026425 accuracy= 1.000\n",
      "Epoch: 0409 cost= 0.000021847 accuracy= 1.000\n",
      "Epoch: 0410 cost= 0.000018074 accuracy= 1.000\n",
      "Epoch: 0411 cost= 0.000023202 accuracy= 1.000\n",
      "Epoch: 0412 cost= 0.000037257 accuracy= 1.000\n",
      "Epoch: 0413 cost= 0.000034620 accuracy= 1.000\n",
      "Epoch: 0414 cost= 0.000023798 accuracy= 1.000\n",
      "Epoch: 0415 cost= 0.000030990 accuracy= 1.000\n",
      "Epoch: 0416 cost= 0.000023099 accuracy= 1.000\n",
      "Epoch: 0417 cost= 0.000023624 accuracy= 1.000\n",
      "Epoch: 0418 cost= 0.000027153 accuracy= 1.000\n",
      "Epoch: 0419 cost= 0.000024043 accuracy= 1.000\n",
      "Epoch: 0420 cost= 0.000023916 accuracy= 1.000\n",
      "Epoch: 0421 cost= 0.000022233 accuracy= 1.000\n",
      "Epoch: 0422 cost= 0.000025196 accuracy= 1.000\n",
      "Epoch: 0423 cost= 0.000025947 accuracy= 1.000\n",
      "Epoch: 0424 cost= 0.000029089 accuracy= 1.000\n",
      "Epoch: 0425 cost= 0.000021116 accuracy= 1.000\n",
      "Epoch: 0426 cost= 0.000021766 accuracy= 1.000\n",
      "Epoch: 0427 cost= 0.000022506 accuracy= 1.000\n",
      "Epoch: 0428 cost= 0.000023680 accuracy= 1.000\n",
      "Epoch: 0429 cost= 0.000021686 accuracy= 1.000\n",
      "Epoch: 0430 cost= 0.000024101 accuracy= 1.000\n",
      "Epoch: 0431 cost= 0.000018630 accuracy= 1.000\n",
      "Epoch: 0432 cost= 0.000027512 accuracy= 1.000\n",
      "Epoch: 0433 cost= 0.000027559 accuracy= 1.000\n",
      "Epoch: 0434 cost= 0.000017963 accuracy= 1.000\n",
      "Epoch: 0435 cost= 0.000020260 accuracy= 1.000\n",
      "Epoch: 0436 cost= 0.000021574 accuracy= 1.000\n",
      "Epoch: 0437 cost= 0.000022616 accuracy= 1.000\n",
      "Epoch: 0438 cost= 0.000023785 accuracy= 1.000\n",
      "Epoch: 0439 cost= 0.000024294 accuracy= 1.000\n",
      "Epoch: 0440 cost= 0.000018481 accuracy= 1.000\n",
      "Epoch: 0441 cost= 0.000017785 accuracy= 1.000\n",
      "Epoch: 0442 cost= 0.000017621 accuracy= 1.000\n",
      "Epoch: 0443 cost= 0.000016440 accuracy= 1.000\n",
      "Epoch: 0444 cost= 0.000022430 accuracy= 1.000\n",
      "Epoch: 0445 cost= 0.000018363 accuracy= 1.000\n",
      "Epoch: 0446 cost= 0.000014726 accuracy= 1.000\n",
      "Epoch: 0447 cost= 0.000020644 accuracy= 1.000\n",
      "Epoch: 0448 cost= 0.000017422 accuracy= 1.000\n",
      "Epoch: 0449 cost= 0.000019337 accuracy= 1.000\n",
      "Epoch: 0450 cost= 0.000017654 accuracy= 1.000\n",
      "Epoch: 0451 cost= 0.000020101 accuracy= 1.000\n",
      "Epoch: 0452 cost= 0.000015881 accuracy= 1.000\n",
      "Epoch: 0453 cost= 0.000020914 accuracy= 1.000\n",
      "Epoch: 0454 cost= 0.000020816 accuracy= 1.000\n",
      "Epoch: 0455 cost= 0.000013001 accuracy= 1.000\n",
      "Epoch: 0456 cost= 0.000023328 accuracy= 1.000\n",
      "Epoch: 0457 cost= 0.000015633 accuracy= 1.000\n",
      "Epoch: 0458 cost= 0.000019163 accuracy= 1.000\n",
      "Epoch: 0459 cost= 0.000017937 accuracy= 1.000\n",
      "Epoch: 0460 cost= 0.000021750 accuracy= 1.000\n",
      "Epoch: 0461 cost= 0.000014990 accuracy= 1.000\n",
      "Epoch: 0462 cost= 0.000020515 accuracy= 1.000\n",
      "Epoch: 0463 cost= 0.000017130 accuracy= 1.000\n",
      "Epoch: 0464 cost= 0.000020544 accuracy= 1.000\n",
      "Epoch: 0465 cost= 0.000017612 accuracy= 1.000\n",
      "Epoch: 0466 cost= 0.000019478 accuracy= 1.000\n",
      "Epoch: 0467 cost= 0.000019804 accuracy= 1.000\n",
      "Epoch: 0468 cost= 0.000016616 accuracy= 1.000\n",
      "Epoch: 0469 cost= 0.000015886 accuracy= 1.000\n",
      "Epoch: 0470 cost= 0.000015774 accuracy= 1.000\n",
      "Epoch: 0471 cost= 0.000019322 accuracy= 1.000\n",
      "Epoch: 0472 cost= 0.000015011 accuracy= 1.000\n",
      "Epoch: 0473 cost= 0.000015866 accuracy= 1.000\n",
      "Epoch: 0474 cost= 0.000016886 accuracy= 1.000\n",
      "Epoch: 0475 cost= 0.000012637 accuracy= 1.000\n",
      "Epoch: 0476 cost= 0.000017591 accuracy= 1.000\n",
      "Epoch: 0477 cost= 0.000015050 accuracy= 1.000\n",
      "Epoch: 0478 cost= 0.000016699 accuracy= 1.000\n",
      "Epoch: 0479 cost= 0.000014652 accuracy= 1.000\n",
      "Epoch: 0480 cost= 0.000010470 accuracy= 1.000\n",
      "Epoch: 0481 cost= 0.000013561 accuracy= 1.000\n",
      "Epoch: 0482 cost= 0.000014840 accuracy= 1.000\n",
      "Epoch: 0483 cost= 0.000011290 accuracy= 1.000\n",
      "Epoch: 0484 cost= 0.000015532 accuracy= 1.000\n",
      "Epoch: 0485 cost= 0.000016871 accuracy= 1.000\n",
      "Epoch: 0486 cost= 0.000014983 accuracy= 1.000\n",
      "Epoch: 0487 cost= 0.000014856 accuracy= 1.000\n",
      "Epoch: 0488 cost= 0.000015638 accuracy= 1.000\n",
      "Epoch: 0489 cost= 0.000013468 accuracy= 1.000\n",
      "Epoch: 0490 cost= 0.000013263 accuracy= 1.000\n",
      "Epoch: 0491 cost= 0.000014787 accuracy= 1.000\n",
      "Epoch: 0492 cost= 0.000012160 accuracy= 1.000\n",
      "Epoch: 0493 cost= 0.000013033 accuracy= 1.000\n",
      "Epoch: 0494 cost= 0.000016201 accuracy= 1.000\n",
      "Epoch: 0495 cost= 0.000011464 accuracy= 1.000\n",
      "Epoch: 0496 cost= 0.000015438 accuracy= 1.000\n",
      "Epoch: 0497 cost= 0.000010810 accuracy= 1.000\n",
      "Epoch: 0498 cost= 0.000013048 accuracy= 1.000\n",
      "Epoch: 0499 cost= 0.000016533 accuracy= 1.000\n",
      "Epoch: 0500 cost= 0.000015065 accuracy= 1.000\n",
      "Epoch: 0501 cost= 0.000016150 accuracy= 1.000\n",
      "Epoch: 0502 cost= 0.000011456 accuracy= 1.000\n",
      "Epoch: 0503 cost= 0.000011998 accuracy= 1.000\n",
      "Epoch: 0504 cost= 0.000007175 accuracy= 1.000\n",
      "Epoch: 0505 cost= 0.000015783 accuracy= 1.000\n",
      "Epoch: 0506 cost= 0.000014434 accuracy= 1.000\n",
      "Epoch: 0507 cost= 0.000012696 accuracy= 1.000\n",
      "Epoch: 0508 cost= 0.000010796 accuracy= 1.000\n",
      "Epoch: 0509 cost= 0.000012038 accuracy= 1.000\n",
      "Epoch: 0510 cost= 0.000014021 accuracy= 1.000\n",
      "Epoch: 0511 cost= 0.000009614 accuracy= 1.000\n",
      "Epoch: 0512 cost= 0.000013710 accuracy= 1.000\n",
      "Epoch: 0513 cost= 0.000010788 accuracy= 1.000\n",
      "Epoch: 0514 cost= 0.000012013 accuracy= 1.000\n",
      "Epoch: 0515 cost= 0.000011811 accuracy= 1.000\n",
      "Epoch: 0516 cost= 0.000007682 accuracy= 1.000\n",
      "Epoch: 0517 cost= 0.000013373 accuracy= 1.000\n",
      "Epoch: 0518 cost= 0.000010814 accuracy= 1.000\n",
      "Epoch: 0519 cost= 0.000009673 accuracy= 1.000\n",
      "Epoch: 0520 cost= 0.000010862 accuracy= 1.000\n",
      "Epoch: 0521 cost= 0.000009162 accuracy= 1.000\n",
      "Epoch: 0522 cost= 0.000011983 accuracy= 1.000\n",
      "Epoch: 0523 cost= 0.000008868 accuracy= 1.000\n",
      "Epoch: 0524 cost= 0.000011095 accuracy= 1.000\n",
      "Epoch: 0525 cost= 0.000009328 accuracy= 1.000\n",
      "Epoch: 0526 cost= 0.000014020 accuracy= 1.000\n",
      "Epoch: 0527 cost= 0.000009387 accuracy= 1.000\n",
      "Epoch: 0528 cost= 0.000009748 accuracy= 1.000\n",
      "Epoch: 0529 cost= 0.000007603 accuracy= 1.000\n",
      "Epoch: 0530 cost= 0.000010927 accuracy= 1.000\n",
      "Epoch: 0531 cost= 0.000010738 accuracy= 1.000\n",
      "Epoch: 0532 cost= 0.000011904 accuracy= 1.000\n",
      "Epoch: 0533 cost= 0.000008973 accuracy= 1.000\n",
      "Epoch: 0534 cost= 0.000007037 accuracy= 1.000\n",
      "Epoch: 0535 cost= 0.000010409 accuracy= 1.000\n",
      "Epoch: 0536 cost= 0.000012628 accuracy= 1.000\n",
      "Epoch: 0537 cost= 0.000009800 accuracy= 1.000\n",
      "Epoch: 0538 cost= 0.000011048 accuracy= 1.000\n",
      "Epoch: 0539 cost= 0.000010173 accuracy= 1.000\n",
      "Epoch: 0540 cost= 0.000008936 accuracy= 1.000\n",
      "Epoch: 0541 cost= 0.000009725 accuracy= 1.000\n",
      "Epoch: 0542 cost= 0.000008735 accuracy= 1.000\n",
      "Epoch: 0543 cost= 0.000010512 accuracy= 1.000\n",
      "Epoch: 0544 cost= 0.000010710 accuracy= 1.000\n",
      "Epoch: 0545 cost= 0.000010636 accuracy= 1.000\n",
      "Epoch: 0546 cost= 0.000010116 accuracy= 1.000\n",
      "Epoch: 0547 cost= 0.000006982 accuracy= 1.000\n",
      "Epoch: 0548 cost= 0.000008137 accuracy= 1.000\n",
      "Epoch: 0549 cost= 0.000006616 accuracy= 1.000\n",
      "Epoch: 0550 cost= 0.000009904 accuracy= 1.000\n",
      "Epoch: 0551 cost= 0.000006990 accuracy= 1.000\n",
      "Epoch: 0552 cost= 0.000011703 accuracy= 1.000\n",
      "Epoch: 0553 cost= 0.000009272 accuracy= 1.000\n",
      "Epoch: 0554 cost= 0.000006632 accuracy= 1.000\n",
      "Epoch: 0555 cost= 0.000008829 accuracy= 1.000\n",
      "Epoch: 0556 cost= 0.000006779 accuracy= 1.000\n",
      "Epoch: 0557 cost= 0.000006978 accuracy= 1.000\n",
      "Epoch: 0558 cost= 0.000007094 accuracy= 1.000\n",
      "Epoch: 0559 cost= 0.000008164 accuracy= 1.000\n",
      "Epoch: 0560 cost= 0.000007273 accuracy= 1.000\n",
      "Epoch: 0561 cost= 0.000009501 accuracy= 1.000\n",
      "Epoch: 0562 cost= 0.000008023 accuracy= 1.000\n",
      "Epoch: 0563 cost= 0.000008998 accuracy= 1.000\n",
      "Epoch: 0564 cost= 0.000008640 accuracy= 1.000\n",
      "Epoch: 0565 cost= 0.000008242 accuracy= 1.000\n",
      "Epoch: 0566 cost= 0.000007121 accuracy= 1.000\n",
      "Epoch: 0567 cost= 0.000008597 accuracy= 1.000\n",
      "Epoch: 0568 cost= 0.000008476 accuracy= 1.000\n",
      "Epoch: 0569 cost= 0.000009935 accuracy= 1.000\n",
      "Epoch: 0570 cost= 0.000007695 accuracy= 1.000\n",
      "Epoch: 0571 cost= 0.000009616 accuracy= 1.000\n",
      "Epoch: 0572 cost= 0.000006285 accuracy= 1.000\n",
      "Epoch: 0573 cost= 0.000007606 accuracy= 1.000\n",
      "Epoch: 0574 cost= 0.000007120 accuracy= 1.000\n",
      "Epoch: 0575 cost= 0.000006742 accuracy= 1.000\n",
      "Epoch: 0576 cost= 0.000008275 accuracy= 1.000\n",
      "Epoch: 0577 cost= 0.000006912 accuracy= 1.000\n",
      "Epoch: 0578 cost= 0.000006364 accuracy= 1.000\n",
      "Epoch: 0579 cost= 0.000005911 accuracy= 1.000\n",
      "Epoch: 0580 cost= 0.000007517 accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_iris_tree0.ckpt\n",
      "Epoch: 0001 cost= 2.236860135 accuracy= 0.208\n",
      "Epoch: 0002 cost= 1.878769419 accuracy= 0.265\n",
      "Epoch: 0003 cost= 1.681331494 accuracy= 0.293\n",
      "Epoch: 0004 cost= 1.596724314 accuracy= 0.322\n",
      "Epoch: 0005 cost= 1.471835305 accuracy= 0.415\n",
      "Epoch: 0006 cost= 1.291195372 accuracy= 0.570\n",
      "Epoch: 0007 cost= 1.056865990 accuracy= 0.661\n",
      "Epoch: 0008 cost= 0.860612498 accuracy= 0.656\n",
      "Epoch: 0009 cost= 0.849562571 accuracy= 0.658\n",
      "Epoch: 0010 cost= 0.789519296 accuracy= 0.733\n",
      "Epoch: 0011 cost= 0.744105514 accuracy= 0.701\n",
      "Epoch: 0012 cost= 0.653226118 accuracy= 0.785\n",
      "Epoch: 0013 cost= 0.681258251 accuracy= 0.804\n",
      "Epoch: 0014 cost= 0.609182684 accuracy= 0.827\n",
      "Epoch: 0015 cost= 0.493807070 accuracy= 0.825\n",
      "Epoch: 0016 cost= 0.594730801 accuracy= 0.841\n",
      "Epoch: 0017 cost= 0.482465485 accuracy= 0.871\n",
      "Epoch: 0018 cost= 0.360999851 accuracy= 0.886\n",
      "Epoch: 0019 cost= 0.335975479 accuracy= 0.896\n",
      "Epoch: 0020 cost= 0.270058799 accuracy= 0.934\n",
      "Epoch: 0021 cost= 0.245301349 accuracy= 0.940\n",
      "Epoch: 0022 cost= 0.235323943 accuracy= 0.951\n",
      "Epoch: 0023 cost= 0.164515856 accuracy= 0.953\n",
      "Epoch: 0024 cost= 0.162815489 accuracy= 0.949\n",
      "Epoch: 0025 cost= 0.235436295 accuracy= 0.947\n",
      "Epoch: 0026 cost= 0.163674777 accuracy= 0.958\n",
      "Epoch: 0027 cost= 0.151231709 accuracy= 0.958\n",
      "Epoch: 0028 cost= 0.123723825 accuracy= 0.964\n",
      "Epoch: 0029 cost= 0.129222352 accuracy= 0.969\n",
      "Epoch: 0030 cost= 0.150266629 accuracy= 0.959\n",
      "Epoch: 0031 cost= 0.110871443 accuracy= 0.961\n",
      "Epoch: 0032 cost= 0.144583003 accuracy= 0.948\n",
      "Epoch: 0033 cost= 0.136070567 accuracy= 0.968\n",
      "Epoch: 0034 cost= 0.127955712 accuracy= 0.945\n",
      "Epoch: 0035 cost= 0.097701755 accuracy= 0.973\n",
      "Epoch: 0036 cost= 0.102570655 accuracy= 0.973\n",
      "Epoch: 0037 cost= 0.118626191 accuracy= 0.975\n",
      "Epoch: 0038 cost= 0.091330195 accuracy= 0.975\n",
      "Epoch: 0039 cost= 0.116399184 accuracy= 0.968\n",
      "Epoch: 0040 cost= 0.080000722 accuracy= 0.977\n",
      "Epoch: 0041 cost= 0.085884012 accuracy= 0.971\n",
      "Epoch: 0042 cost= 0.061528634 accuracy= 0.982\n",
      "Epoch: 0043 cost= 0.081107241 accuracy= 0.977\n",
      "Epoch: 0044 cost= 0.069558498 accuracy= 0.980\n",
      "Epoch: 0045 cost= 0.084809170 accuracy= 0.979\n",
      "Epoch: 0046 cost= 0.058120585 accuracy= 0.982\n",
      "Epoch: 0047 cost= 0.067849251 accuracy= 0.982\n",
      "Epoch: 0048 cost= 0.056892136 accuracy= 0.973\n",
      "Epoch: 0049 cost= 0.073198984 accuracy= 0.969\n",
      "Epoch: 0050 cost= 0.063059727 accuracy= 0.986\n",
      "Epoch: 0051 cost= 0.061922428 accuracy= 0.971\n",
      "Epoch: 0052 cost= 0.066463592 accuracy= 0.969\n",
      "Epoch: 0053 cost= 0.059871980 accuracy= 0.982\n",
      "Epoch: 0054 cost= 0.055519803 accuracy= 0.984\n",
      "Epoch: 0055 cost= 0.060390716 accuracy= 0.986\n",
      "Epoch: 0056 cost= 0.051551745 accuracy= 0.975\n",
      "Epoch: 0057 cost= 0.045314645 accuracy= 0.987\n",
      "Epoch: 0058 cost= 0.031973099 accuracy= 0.989\n",
      "Epoch: 0059 cost= 0.045716247 accuracy= 0.981\n",
      "Epoch: 0060 cost= 0.064168075 accuracy= 0.989\n",
      "Epoch: 0061 cost= 0.041332180 accuracy= 0.986\n",
      "Epoch: 0062 cost= 0.065836235 accuracy= 0.973\n",
      "Epoch: 0063 cost= 0.059912976 accuracy= 0.977\n",
      "Epoch: 0064 cost= 0.063441623 accuracy= 0.983\n",
      "Epoch: 0065 cost= 0.037335981 accuracy= 0.981\n",
      "Epoch: 0066 cost= 0.043105597 accuracy= 0.988\n",
      "Epoch: 0067 cost= 0.047471171 accuracy= 0.982\n",
      "Epoch: 0068 cost= 0.060203501 accuracy= 0.975\n",
      "Epoch: 0069 cost= 0.080579946 accuracy= 0.956\n",
      "Epoch: 0070 cost= 0.083919453 accuracy= 0.981\n",
      "Epoch: 0071 cost= 0.058924219 accuracy= 0.989\n",
      "Epoch: 0072 cost= 0.059843109 accuracy= 0.976\n",
      "Epoch: 0073 cost= 0.037053177 accuracy= 0.992\n",
      "Epoch: 0074 cost= 0.036347125 accuracy= 0.993\n",
      "Epoch: 0075 cost= 0.023897900 accuracy= 0.985\n",
      "Epoch: 0076 cost= 0.047514176 accuracy= 0.985\n",
      "Epoch: 0077 cost= 0.040948962 accuracy= 0.963\n",
      "Epoch: 0078 cost= 0.062193977 accuracy= 0.971\n",
      "Epoch: 0079 cost= 0.039219826 accuracy= 0.987\n",
      "Epoch: 0080 cost= 0.048934588 accuracy= 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 cost= 0.030334534 accuracy= 0.985\n",
      "Epoch: 0082 cost= 0.031173310 accuracy= 0.995\n",
      "Epoch: 0083 cost= 0.019708162 accuracy= 0.993\n",
      "Epoch: 0084 cost= 0.020842355 accuracy= 0.996\n",
      "Epoch: 0085 cost= 0.024087327 accuracy= 0.996\n",
      "Epoch: 0086 cost= 0.020577075 accuracy= 0.989\n",
      "Epoch: 0087 cost= 0.029267613 accuracy= 0.980\n",
      "Epoch: 0088 cost= 0.038020339 accuracy= 0.988\n",
      "Epoch: 0089 cost= 0.032178161 accuracy= 0.993\n",
      "Epoch: 0090 cost= 0.025620140 accuracy= 0.993\n",
      "Epoch: 0091 cost= 0.019457055 accuracy= 0.995\n",
      "Epoch: 0092 cost= 0.019503403 accuracy= 0.996\n",
      "Epoch: 0093 cost= 0.026218680 accuracy= 0.993\n",
      "Epoch: 0094 cost= 0.018543471 accuracy= 0.992\n",
      "Epoch: 0095 cost= 0.017001266 accuracy= 0.996\n",
      "Epoch: 0096 cost= 0.017768013 accuracy= 0.997\n",
      "Epoch: 0097 cost= 0.009681985 accuracy= 0.998\n",
      "Epoch: 0098 cost= 0.009509851 accuracy= 0.997\n",
      "Epoch: 0099 cost= 0.015690863 accuracy= 0.997\n",
      "Epoch: 0100 cost= 0.020188091 accuracy= 0.989\n",
      "Epoch: 0101 cost= 0.036824376 accuracy= 0.977\n",
      "Epoch: 0102 cost= 0.022249876 accuracy= 0.996\n",
      "Epoch: 0103 cost= 0.014237989 accuracy= 0.996\n",
      "Epoch: 0104 cost= 0.016779024 accuracy= 0.995\n",
      "Epoch: 0105 cost= 0.009844721 accuracy= 0.996\n",
      "Epoch: 0106 cost= 0.018905637 accuracy= 0.996\n",
      "Epoch: 0107 cost= 0.007922864 accuracy= 0.997\n",
      "Epoch: 0108 cost= 0.012198591 accuracy= 0.998\n",
      "Epoch: 0109 cost= 0.004368936 accuracy= 0.996\n",
      "Epoch: 0110 cost= 0.006786592 accuracy= 0.995\n",
      "Epoch: 0111 cost= 0.012463538 accuracy= 0.999\n",
      "Epoch: 0112 cost= 0.008234768 accuracy= 0.999\n",
      "Epoch: 0113 cost= 0.012283139 accuracy= 0.997\n",
      "Epoch: 0114 cost= 0.005180677 accuracy= 0.998\n",
      "Epoch: 0115 cost= 0.008458652 accuracy= 0.999\n",
      "Epoch: 0116 cost= 0.010271311 accuracy= 0.997\n",
      "Epoch: 0117 cost= 0.005144393 accuracy= 0.997\n",
      "Epoch: 0118 cost= 0.014696256 accuracy= 0.995\n",
      "Epoch: 0119 cost= 0.023661325 accuracy= 0.995\n",
      "Epoch: 0120 cost= 0.017468564 accuracy= 0.996\n",
      "Epoch: 0121 cost= 0.008475759 accuracy= 0.997\n",
      "Epoch: 0122 cost= 0.010414562 accuracy= 0.996\n",
      "Epoch: 0123 cost= 0.006219962 accuracy= 0.996\n",
      "Epoch: 0124 cost= 0.023150408 accuracy= 0.999\n",
      "Epoch: 0125 cost= 0.027073492 accuracy= 0.995\n",
      "Epoch: 0126 cost= 0.042026290 accuracy= 0.966\n",
      "Epoch: 0127 cost= 0.089312048 accuracy= 0.962\n",
      "Epoch: 0128 cost= 0.074026792 accuracy= 0.966\n",
      "Epoch: 0129 cost= 0.040832935 accuracy= 0.989\n",
      "Epoch: 0130 cost= 0.020325086 accuracy= 0.996\n",
      "Epoch: 0131 cost= 0.022852120 accuracy= 0.989\n",
      "Epoch: 0132 cost= 0.022414975 accuracy= 0.996\n",
      "Epoch: 0133 cost= 0.006033329 accuracy= 0.996\n",
      "Epoch: 0134 cost= 0.040922720 accuracy= 0.987\n",
      "Epoch: 0135 cost= 0.029829030 accuracy= 0.993\n",
      "Epoch: 0136 cost= 0.019706609 accuracy= 0.996\n",
      "Epoch: 0137 cost= 0.009064666 accuracy= 0.998\n",
      "Epoch: 0138 cost= 0.015032848 accuracy= 0.996\n",
      "Epoch: 0139 cost= 0.011835894 accuracy= 0.999\n",
      "Epoch: 0140 cost= 0.013294718 accuracy= 1.000\n",
      "Epoch: 0141 cost= 0.013280093 accuracy= 0.989\n",
      "Epoch: 0142 cost= 0.012465741 accuracy= 0.999\n",
      "Epoch: 0143 cost= 0.005944261 accuracy= 0.999\n",
      "Epoch: 0144 cost= 0.007802836 accuracy= 0.999\n",
      "Epoch: 0145 cost= 0.002130361 accuracy= 0.998\n",
      "Epoch: 0146 cost= 0.003143190 accuracy= 1.000\n",
      "Epoch: 0147 cost= 0.005923938 accuracy= 0.999\n",
      "Epoch: 0148 cost= 0.016545317 accuracy= 0.996\n",
      "Epoch: 0149 cost= 0.027463502 accuracy= 0.993\n",
      "Epoch: 0150 cost= 0.017129873 accuracy= 0.996\n",
      "Epoch: 0151 cost= 0.009343665 accuracy= 0.996\n",
      "Epoch: 0152 cost= 0.014100381 accuracy= 0.992\n",
      "Epoch: 0153 cost= 0.026973743 accuracy= 0.998\n",
      "Epoch: 0154 cost= 0.019780984 accuracy= 0.994\n",
      "Epoch: 0155 cost= 0.008281907 accuracy= 0.995\n",
      "Epoch: 0156 cost= 0.009358164 accuracy= 0.998\n",
      "Epoch: 0157 cost= 0.013038673 accuracy= 0.996\n",
      "Epoch: 0158 cost= 0.006381108 accuracy= 0.998\n",
      "Epoch: 0159 cost= 0.003590080 accuracy= 0.999\n",
      "Epoch: 0160 cost= 0.001056833 accuracy= 0.999\n",
      "Epoch: 0161 cost= 0.003077956 accuracy= 1.000\n",
      "Epoch: 0162 cost= 0.001103391 accuracy= 0.999\n",
      "Epoch: 0163 cost= 0.001493351 accuracy= 1.000\n",
      "Epoch: 0164 cost= 0.001860224 accuracy= 1.000\n",
      "Epoch: 0165 cost= 0.001551965 accuracy= 1.000\n",
      "Epoch: 0166 cost= 0.000537011 accuracy= 0.999\n",
      "Epoch: 0167 cost= 0.001160020 accuracy= 1.000\n",
      "Epoch: 0168 cost= 0.001025613 accuracy= 1.000\n",
      "Epoch: 0169 cost= 0.001320453 accuracy= 1.000\n",
      "Epoch: 0170 cost= 0.000981163 accuracy= 1.000\n",
      "Epoch: 0171 cost= 0.000900767 accuracy= 1.000\n",
      "Epoch: 0172 cost= 0.001043361 accuracy= 0.999\n",
      "Epoch: 0173 cost= 0.000707440 accuracy= 0.999\n",
      "Epoch: 0174 cost= 0.002245877 accuracy= 1.000\n",
      "Epoch: 0175 cost= 0.000881474 accuracy= 0.999\n",
      "Epoch: 0176 cost= 0.003721809 accuracy= 0.999\n",
      "Epoch: 0177 cost= 0.007063264 accuracy= 1.000\n",
      "Epoch: 0178 cost= 0.019885659 accuracy= 0.991\n",
      "Epoch: 0179 cost= 0.076323262 accuracy= 0.988\n",
      "Epoch: 0180 cost= 0.051300451 accuracy= 0.990\n",
      "Epoch: 0181 cost= 0.035259607 accuracy= 0.998\n",
      "Epoch: 0182 cost= 0.027206931 accuracy= 0.994\n",
      "Epoch: 0183 cost= 0.020464498 accuracy= 0.993\n",
      "Epoch: 0184 cost= 0.024215816 accuracy= 0.989\n",
      "Epoch: 0185 cost= 0.014049767 accuracy= 0.996\n",
      "Epoch: 0186 cost= 0.005065948 accuracy= 0.993\n",
      "Epoch: 0187 cost= 0.007156712 accuracy= 0.998\n",
      "Epoch: 0188 cost= 0.008087116 accuracy= 0.999\n",
      "Epoch: 0189 cost= 0.016278004 accuracy= 0.997\n",
      "Epoch: 0190 cost= 0.007756519 accuracy= 0.997\n",
      "Epoch: 0191 cost= 0.004164513 accuracy= 0.999\n",
      "Epoch: 0192 cost= 0.001481248 accuracy= 0.999\n",
      "Epoch: 0193 cost= 0.001014754 accuracy= 0.999\n",
      "Epoch: 0194 cost= 0.008425135 accuracy= 0.998\n",
      "Epoch: 0195 cost= 0.006030476 accuracy= 0.999\n",
      "Epoch: 0196 cost= 0.001533978 accuracy= 1.000\n",
      "Epoch: 0197 cost= 0.001295956 accuracy= 1.000\n",
      "Epoch: 0198 cost= 0.001205142 accuracy= 1.000\n",
      "Epoch: 0199 cost= 0.001333988 accuracy= 0.999\n",
      "Epoch: 0200 cost= 0.002558965 accuracy= 0.999\n",
      "Epoch: 0201 cost= 0.001579342 accuracy= 0.999\n",
      "Epoch: 0202 cost= 0.002793637 accuracy= 0.999\n",
      "Epoch: 0203 cost= 0.002687959 accuracy= 1.000\n",
      "Epoch: 0204 cost= 0.004136465 accuracy= 1.000\n",
      "Epoch: 0205 cost= 0.002470830 accuracy= 1.000\n",
      "Epoch: 0206 cost= 0.006866639 accuracy= 0.998\n",
      "Epoch: 0207 cost= 0.002394001 accuracy= 0.998\n",
      "Epoch: 0208 cost= 0.001633357 accuracy= 1.000\n",
      "Epoch: 0209 cost= 0.000710974 accuracy= 1.000\n",
      "Epoch: 0210 cost= 0.000677081 accuracy= 1.000\n",
      "Epoch: 0211 cost= 0.000377522 accuracy= 1.000\n",
      "Epoch: 0212 cost= 0.001149383 accuracy= 1.000\n",
      "Epoch: 0213 cost= 0.000483038 accuracy= 1.000\n",
      "Epoch: 0214 cost= 0.000267237 accuracy= 1.000\n",
      "Epoch: 0215 cost= 0.000505175 accuracy= 1.000\n",
      "Epoch: 0216 cost= 0.000485986 accuracy= 1.000\n",
      "Epoch: 0217 cost= 0.000308547 accuracy= 1.000\n",
      "Epoch: 0218 cost= 0.000199578 accuracy= 1.000\n",
      "Epoch: 0219 cost= 0.000256017 accuracy= 1.000\n",
      "Epoch: 0220 cost= 0.000473463 accuracy= 1.000\n",
      "Epoch: 0221 cost= 0.000472778 accuracy= 1.000\n",
      "Epoch: 0222 cost= 0.000407934 accuracy= 1.000\n",
      "Epoch: 0223 cost= 0.000277945 accuracy= 1.000\n",
      "Epoch: 0224 cost= 0.000292930 accuracy= 1.000\n",
      "Epoch: 0225 cost= 0.000184234 accuracy= 1.000\n",
      "Epoch: 0226 cost= 0.000425418 accuracy= 1.000\n",
      "Epoch: 0227 cost= 0.000380742 accuracy= 1.000\n",
      "Epoch: 0228 cost= 0.000358412 accuracy= 1.000\n",
      "Epoch: 0229 cost= 0.000313402 accuracy= 1.000\n",
      "Epoch: 0230 cost= 0.000291442 accuracy= 1.000\n",
      "Epoch: 0231 cost= 0.000186102 accuracy= 1.000\n",
      "Epoch: 0232 cost= 0.000184138 accuracy= 1.000\n",
      "Epoch: 0233 cost= 0.000264661 accuracy= 1.000\n",
      "Epoch: 0234 cost= 0.000192842 accuracy= 1.000\n",
      "Epoch: 0235 cost= 0.000181189 accuracy= 1.000\n",
      "Epoch: 0236 cost= 0.000148051 accuracy= 1.000\n",
      "Epoch: 0237 cost= 0.000208051 accuracy= 1.000\n",
      "Epoch: 0238 cost= 0.000159333 accuracy= 1.000\n",
      "Epoch: 0239 cost= 0.000267228 accuracy= 1.000\n",
      "Epoch: 0240 cost= 0.000113611 accuracy= 1.000\n",
      "Epoch: 0241 cost= 0.000214498 accuracy= 1.000\n",
      "Epoch: 0242 cost= 0.000431351 accuracy= 1.000\n",
      "Epoch: 0243 cost= 0.000223947 accuracy= 1.000\n",
      "Epoch: 0244 cost= 0.000287474 accuracy= 1.000\n",
      "Epoch: 0245 cost= 0.000214357 accuracy= 1.000\n",
      "Epoch: 0246 cost= 0.000116156 accuracy= 1.000\n",
      "Epoch: 0247 cost= 0.000204659 accuracy= 1.000\n",
      "Epoch: 0248 cost= 0.000191571 accuracy= 1.000\n",
      "Epoch: 0249 cost= 0.000161929 accuracy= 1.000\n",
      "Epoch: 0250 cost= 0.000149920 accuracy= 1.000\n",
      "Epoch: 0251 cost= 0.000116377 accuracy= 1.000\n",
      "Epoch: 0252 cost= 0.000174213 accuracy= 1.000\n",
      "Epoch: 0253 cost= 0.000167971 accuracy= 1.000\n",
      "Epoch: 0254 cost= 0.000194831 accuracy= 1.000\n",
      "Epoch: 0255 cost= 0.000156567 accuracy= 1.000\n",
      "Epoch: 0256 cost= 0.000106977 accuracy= 1.000\n",
      "Epoch: 0257 cost= 0.000166759 accuracy= 1.000\n",
      "Epoch: 0258 cost= 0.000155205 accuracy= 1.000\n",
      "Epoch: 0259 cost= 0.000170567 accuracy= 1.000\n",
      "Epoch: 0260 cost= 0.000133365 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0261 cost= 0.000159404 accuracy= 1.000\n",
      "Epoch: 0262 cost= 0.000159401 accuracy= 1.000\n",
      "Epoch: 0263 cost= 0.000157788 accuracy= 1.000\n",
      "Epoch: 0264 cost= 0.000090980 accuracy= 1.000\n",
      "Epoch: 0265 cost= 0.000093446 accuracy= 1.000\n",
      "Epoch: 0266 cost= 0.000109696 accuracy= 1.000\n",
      "Epoch: 0267 cost= 0.000216971 accuracy= 1.000\n",
      "Epoch: 0268 cost= 0.000370797 accuracy= 1.000\n",
      "Epoch: 0269 cost= 0.000104849 accuracy= 1.000\n",
      "Epoch: 0270 cost= 0.000115924 accuracy= 1.000\n",
      "Epoch: 0271 cost= 0.000152691 accuracy= 1.000\n",
      "Epoch: 0272 cost= 0.000145250 accuracy= 1.000\n",
      "Epoch: 0273 cost= 0.000100315 accuracy= 1.000\n",
      "Epoch: 0274 cost= 0.000179001 accuracy= 1.000\n",
      "Epoch: 0275 cost= 0.000121071 accuracy= 1.000\n",
      "Epoch: 0276 cost= 0.000119583 accuracy= 1.000\n",
      "Epoch: 0277 cost= 0.000131936 accuracy= 1.000\n",
      "Epoch: 0278 cost= 0.000099089 accuracy= 1.000\n",
      "Epoch: 0279 cost= 0.000134469 accuracy= 1.000\n",
      "Epoch: 0280 cost= 0.000107119 accuracy= 1.000\n",
      "Epoch: 0281 cost= 0.000112624 accuracy= 1.000\n",
      "Epoch: 0282 cost= 0.000075856 accuracy= 1.000\n",
      "Epoch: 0283 cost= 0.000124069 accuracy= 1.000\n",
      "Epoch: 0284 cost= 0.000070356 accuracy= 1.000\n",
      "Epoch: 0285 cost= 0.000098373 accuracy= 1.000\n",
      "Epoch: 0286 cost= 0.000083380 accuracy= 1.000\n",
      "Epoch: 0287 cost= 0.000126107 accuracy= 1.000\n",
      "Epoch: 0288 cost= 0.000117291 accuracy= 1.000\n",
      "Epoch: 0289 cost= 0.000130344 accuracy= 1.000\n",
      "Epoch: 0290 cost= 0.000108683 accuracy= 1.000\n",
      "Epoch: 0291 cost= 0.000094717 accuracy= 1.000\n",
      "Epoch: 0292 cost= 0.000112200 accuracy= 1.000\n",
      "Epoch: 0293 cost= 0.000063460 accuracy= 1.000\n",
      "Epoch: 0294 cost= 0.000091312 accuracy= 1.000\n",
      "Epoch: 0295 cost= 0.000093969 accuracy= 1.000\n",
      "Epoch: 0296 cost= 0.000079142 accuracy= 1.000\n",
      "Epoch: 0297 cost= 0.000086865 accuracy= 1.000\n",
      "Epoch: 0298 cost= 0.000085483 accuracy= 1.000\n",
      "Epoch: 0299 cost= 0.000094206 accuracy= 1.000\n",
      "Epoch: 0300 cost= 0.000106921 accuracy= 1.000\n",
      "Epoch: 0301 cost= 0.000076058 accuracy= 1.000\n",
      "Epoch: 0302 cost= 0.000071154 accuracy= 1.000\n",
      "Epoch: 0303 cost= 0.000092904 accuracy= 1.000\n",
      "Epoch: 0304 cost= 0.000068895 accuracy= 1.000\n",
      "Epoch: 0305 cost= 0.000062894 accuracy= 1.000\n",
      "Epoch: 0306 cost= 0.000071571 accuracy= 1.000\n",
      "Epoch: 0307 cost= 0.000071576 accuracy= 1.000\n",
      "Epoch: 0308 cost= 0.000069517 accuracy= 1.000\n",
      "Epoch: 0309 cost= 0.000062127 accuracy= 1.000\n",
      "Epoch: 0310 cost= 0.000066680 accuracy= 1.000\n",
      "Epoch: 0311 cost= 0.000053816 accuracy= 1.000\n",
      "Epoch: 0312 cost= 0.000095985 accuracy= 1.000\n",
      "Epoch: 0313 cost= 0.000068172 accuracy= 1.000\n",
      "Epoch: 0314 cost= 0.000067639 accuracy= 1.000\n",
      "Epoch: 0315 cost= 0.000047686 accuracy= 1.000\n",
      "Epoch: 0316 cost= 0.000088328 accuracy= 1.000\n",
      "Epoch: 0317 cost= 0.000067675 accuracy= 1.000\n",
      "Epoch: 0318 cost= 0.000062483 accuracy= 1.000\n",
      "Epoch: 0319 cost= 0.000089461 accuracy= 1.000\n",
      "Epoch: 0320 cost= 0.000077563 accuracy= 1.000\n",
      "Epoch: 0321 cost= 0.000088961 accuracy= 1.000\n",
      "Epoch: 0322 cost= 0.000073187 accuracy= 1.000\n",
      "Epoch: 0323 cost= 0.000047357 accuracy= 1.000\n",
      "Epoch: 0324 cost= 0.000059608 accuracy= 1.000\n",
      "Epoch: 0325 cost= 0.000059215 accuracy= 1.000\n",
      "Epoch: 0326 cost= 0.000065549 accuracy= 1.000\n",
      "Epoch: 0327 cost= 0.000046593 accuracy= 1.000\n",
      "Epoch: 0328 cost= 0.000074539 accuracy= 1.000\n",
      "Epoch: 0329 cost= 0.000041017 accuracy= 1.000\n",
      "Epoch: 0330 cost= 0.000053932 accuracy= 1.000\n",
      "Epoch: 0331 cost= 0.000050059 accuracy= 1.000\n",
      "Epoch: 0332 cost= 0.000062140 accuracy= 1.000\n",
      "Epoch: 0333 cost= 0.000046239 accuracy= 1.000\n",
      "Epoch: 0334 cost= 0.000054819 accuracy= 1.000\n",
      "Epoch: 0335 cost= 0.000045840 accuracy= 1.000\n",
      "Epoch: 0336 cost= 0.000054008 accuracy= 1.000\n",
      "Epoch: 0337 cost= 0.000049713 accuracy= 1.000\n",
      "Epoch: 0338 cost= 0.000040029 accuracy= 1.000\n",
      "Epoch: 0339 cost= 0.000049493 accuracy= 1.000\n",
      "Epoch: 0340 cost= 0.000047532 accuracy= 1.000\n",
      "Epoch: 0341 cost= 0.000038724 accuracy= 1.000\n",
      "Epoch: 0342 cost= 0.000044545 accuracy= 1.000\n",
      "Epoch: 0343 cost= 0.000048821 accuracy= 1.000\n",
      "Epoch: 0344 cost= 0.000063910 accuracy= 1.000\n",
      "Epoch: 0345 cost= 0.000061459 accuracy= 1.000\n",
      "Epoch: 0346 cost= 0.000040579 accuracy= 1.000\n",
      "Epoch: 0347 cost= 0.000051413 accuracy= 1.000\n",
      "Epoch: 0348 cost= 0.000037980 accuracy= 1.000\n",
      "Epoch: 0349 cost= 0.000044325 accuracy= 1.000\n",
      "Epoch: 0350 cost= 0.000063017 accuracy= 1.000\n",
      "Epoch: 0351 cost= 0.000041939 accuracy= 1.000\n",
      "Epoch: 0352 cost= 0.000046674 accuracy= 1.000\n",
      "Epoch: 0353 cost= 0.000041310 accuracy= 1.000\n",
      "Epoch: 0354 cost= 0.000041187 accuracy= 1.000\n",
      "Epoch: 0355 cost= 0.000053385 accuracy= 1.000\n",
      "Epoch: 0356 cost= 0.000051840 accuracy= 1.000\n",
      "Epoch: 0357 cost= 0.000036911 accuracy= 1.000\n",
      "Epoch: 0358 cost= 0.000046097 accuracy= 1.000\n",
      "Epoch: 0359 cost= 0.000038074 accuracy= 1.000\n",
      "Epoch: 0360 cost= 0.000040653 accuracy= 1.000\n",
      "Epoch: 0361 cost= 0.000024866 accuracy= 1.000\n",
      "Epoch: 0362 cost= 0.000049235 accuracy= 1.000\n",
      "Epoch: 0363 cost= 0.000036421 accuracy= 1.000\n",
      "Epoch: 0364 cost= 0.000026237 accuracy= 1.000\n",
      "Epoch: 0365 cost= 0.000043158 accuracy= 1.000\n",
      "Epoch: 0366 cost= 0.000039215 accuracy= 1.000\n",
      "Epoch: 0367 cost= 0.000041915 accuracy= 1.000\n",
      "Epoch: 0368 cost= 0.000044457 accuracy= 1.000\n",
      "Epoch: 0369 cost= 0.000042949 accuracy= 1.000\n",
      "Epoch: 0370 cost= 0.000039612 accuracy= 1.000\n",
      "Epoch: 0371 cost= 0.000036587 accuracy= 1.000\n",
      "Epoch: 0372 cost= 0.000033932 accuracy= 1.000\n",
      "Epoch: 0373 cost= 0.000033229 accuracy= 1.000\n",
      "Epoch: 0374 cost= 0.000037237 accuracy= 1.000\n",
      "Epoch: 0375 cost= 0.000027659 accuracy= 1.000\n",
      "Epoch: 0376 cost= 0.000027735 accuracy= 1.000\n",
      "Epoch: 0377 cost= 0.000025057 accuracy= 1.000\n",
      "Epoch: 0378 cost= 0.000033526 accuracy= 1.000\n",
      "Epoch: 0379 cost= 0.000029588 accuracy= 1.000\n",
      "Epoch: 0380 cost= 0.000038792 accuracy= 1.000\n",
      "Epoch: 0381 cost= 0.000027378 accuracy= 1.000\n",
      "Epoch: 0382 cost= 0.000029075 accuracy= 1.000\n",
      "Epoch: 0383 cost= 0.000039521 accuracy= 1.000\n",
      "Epoch: 0384 cost= 0.000033269 accuracy= 1.000\n",
      "Epoch: 0385 cost= 0.000030252 accuracy= 1.000\n",
      "Epoch: 0386 cost= 0.000037653 accuracy= 1.000\n",
      "Epoch: 0387 cost= 0.000035048 accuracy= 1.000\n",
      "Epoch: 0388 cost= 0.000029980 accuracy= 1.000\n",
      "Epoch: 0389 cost= 0.000035043 accuracy= 1.000\n",
      "Epoch: 0390 cost= 0.000038996 accuracy= 1.000\n",
      "Epoch: 0391 cost= 0.000021595 accuracy= 1.000\n",
      "Epoch: 0392 cost= 0.000040777 accuracy= 1.000\n",
      "Epoch: 0393 cost= 0.000030341 accuracy= 1.000\n",
      "Epoch: 0394 cost= 0.000031290 accuracy= 1.000\n",
      "Epoch: 0395 cost= 0.000027408 accuracy= 1.000\n",
      "Epoch: 0396 cost= 0.000037536 accuracy= 1.000\n",
      "Epoch: 0397 cost= 0.000031317 accuracy= 1.000\n",
      "Epoch: 0398 cost= 0.000022814 accuracy= 1.000\n",
      "Epoch: 0399 cost= 0.000028910 accuracy= 1.000\n",
      "Epoch: 0400 cost= 0.000029288 accuracy= 1.000\n",
      "Epoch: 0401 cost= 0.000019798 accuracy= 1.000\n",
      "Epoch: 0402 cost= 0.000026844 accuracy= 1.000\n",
      "Epoch: 0403 cost= 0.000022353 accuracy= 1.000\n",
      "Epoch: 0404 cost= 0.000031730 accuracy= 1.000\n",
      "Epoch: 0405 cost= 0.000023969 accuracy= 1.000\n",
      "Epoch: 0406 cost= 0.000033939 accuracy= 1.000\n",
      "Epoch: 0407 cost= 0.000032124 accuracy= 1.000\n",
      "Epoch: 0408 cost= 0.000033936 accuracy= 1.000\n",
      "Epoch: 0409 cost= 0.000027832 accuracy= 1.000\n",
      "Epoch: 0410 cost= 0.000022489 accuracy= 1.000\n",
      "Epoch: 0411 cost= 0.000022272 accuracy= 1.000\n",
      "Epoch: 0412 cost= 0.000023190 accuracy= 1.000\n",
      "Epoch: 0413 cost= 0.000027700 accuracy= 1.000\n",
      "Epoch: 0414 cost= 0.000024237 accuracy= 1.000\n",
      "Epoch: 0415 cost= 0.000029347 accuracy= 1.000\n",
      "Epoch: 0416 cost= 0.000027039 accuracy= 1.000\n",
      "Epoch: 0417 cost= 0.000028433 accuracy= 1.000\n",
      "Epoch: 0418 cost= 0.000024931 accuracy= 1.000\n",
      "Epoch: 0419 cost= 0.000019480 accuracy= 1.000\n",
      "Epoch: 0420 cost= 0.000024539 accuracy= 1.000\n",
      "Epoch: 0421 cost= 0.000022597 accuracy= 1.000\n",
      "Epoch: 0422 cost= 0.000019120 accuracy= 1.000\n",
      "Epoch: 0423 cost= 0.000024997 accuracy= 1.000\n",
      "Epoch: 0424 cost= 0.000021640 accuracy= 1.000\n",
      "Epoch: 0425 cost= 0.000020486 accuracy= 1.000\n",
      "Epoch: 0426 cost= 0.000019027 accuracy= 1.000\n",
      "Epoch: 0427 cost= 0.000020166 accuracy= 1.000\n",
      "Epoch: 0428 cost= 0.000024786 accuracy= 1.000\n",
      "Epoch: 0429 cost= 0.000013941 accuracy= 1.000\n",
      "Epoch: 0430 cost= 0.000023172 accuracy= 1.000\n",
      "Epoch: 0431 cost= 0.000019825 accuracy= 1.000\n",
      "Epoch: 0432 cost= 0.000022452 accuracy= 1.000\n",
      "Epoch: 0433 cost= 0.000019482 accuracy= 1.000\n",
      "Epoch: 0434 cost= 0.000020520 accuracy= 1.000\n",
      "Epoch: 0435 cost= 0.000016769 accuracy= 1.000\n",
      "Epoch: 0436 cost= 0.000020260 accuracy= 1.000\n",
      "Epoch: 0437 cost= 0.000016324 accuracy= 1.000\n",
      "Epoch: 0438 cost= 0.000017970 accuracy= 1.000\n",
      "Epoch: 0439 cost= 0.000019996 accuracy= 1.000\n",
      "Epoch: 0440 cost= 0.000016486 accuracy= 1.000\n",
      "Epoch: 0441 cost= 0.000021760 accuracy= 1.000\n",
      "Epoch: 0442 cost= 0.000019776 accuracy= 1.000\n",
      "Epoch: 0443 cost= 0.000024628 accuracy= 1.000\n",
      "Epoch: 0444 cost= 0.000016671 accuracy= 1.000\n",
      "Epoch: 0445 cost= 0.000019190 accuracy= 1.000\n",
      "Epoch: 0446 cost= 0.000019674 accuracy= 1.000\n",
      "Epoch: 0447 cost= 0.000013706 accuracy= 1.000\n",
      "Epoch: 0448 cost= 0.000018335 accuracy= 1.000\n",
      "Epoch: 0449 cost= 0.000015212 accuracy= 1.000\n",
      "Epoch: 0450 cost= 0.000016536 accuracy= 1.000\n",
      "Epoch: 0451 cost= 0.000017108 accuracy= 1.000\n",
      "Epoch: 0452 cost= 0.000023730 accuracy= 1.000\n",
      "Epoch: 0453 cost= 0.000017426 accuracy= 1.000\n",
      "Epoch: 0454 cost= 0.000018978 accuracy= 1.000\n",
      "Epoch: 0455 cost= 0.000015806 accuracy= 1.000\n",
      "Epoch: 0456 cost= 0.000016882 accuracy= 1.000\n",
      "Epoch: 0457 cost= 0.000016164 accuracy= 1.000\n",
      "Epoch: 0458 cost= 0.000015142 accuracy= 1.000\n",
      "Epoch: 0459 cost= 0.000014750 accuracy= 1.000\n",
      "Epoch: 0460 cost= 0.000013672 accuracy= 1.000\n",
      "Epoch: 0461 cost= 0.000018094 accuracy= 1.000\n",
      "Epoch: 0462 cost= 0.000014842 accuracy= 1.000\n",
      "Epoch: 0463 cost= 0.000016055 accuracy= 1.000\n",
      "Epoch: 0464 cost= 0.000012902 accuracy= 1.000\n",
      "Epoch: 0465 cost= 0.000016052 accuracy= 1.000\n",
      "Epoch: 0466 cost= 0.000021337 accuracy= 1.000\n",
      "Epoch: 0467 cost= 0.000019039 accuracy= 1.000\n",
      "Epoch: 0468 cost= 0.000017397 accuracy= 1.000\n",
      "Epoch: 0469 cost= 0.000012922 accuracy= 1.000\n",
      "Epoch: 0470 cost= 0.000014654 accuracy= 1.000\n",
      "Epoch: 0471 cost= 0.000012514 accuracy= 1.000\n",
      "Epoch: 0472 cost= 0.000013256 accuracy= 1.000\n",
      "Epoch: 0473 cost= 0.000012567 accuracy= 1.000\n",
      "Epoch: 0474 cost= 0.000011313 accuracy= 1.000\n",
      "Epoch: 0475 cost= 0.000013385 accuracy= 1.000\n",
      "Epoch: 0476 cost= 0.000012944 accuracy= 1.000\n",
      "Epoch: 0477 cost= 0.000015559 accuracy= 1.000\n",
      "Epoch: 0478 cost= 0.000014015 accuracy= 1.000\n",
      "Epoch: 0479 cost= 0.000018740 accuracy= 1.000\n",
      "Epoch: 0480 cost= 0.000014682 accuracy= 1.000\n",
      "Epoch: 0481 cost= 0.000014432 accuracy= 1.000\n",
      "Epoch: 0482 cost= 0.000014061 accuracy= 1.000\n",
      "Epoch: 0483 cost= 0.000013957 accuracy= 1.000\n",
      "Epoch: 0484 cost= 0.000012025 accuracy= 1.000\n",
      "Epoch: 0485 cost= 0.000010474 accuracy= 1.000\n",
      "Epoch: 0486 cost= 0.000012859 accuracy= 1.000\n",
      "Epoch: 0487 cost= 0.000010190 accuracy= 1.000\n",
      "Epoch: 0488 cost= 0.000011757 accuracy= 1.000\n",
      "Epoch: 0489 cost= 0.000011006 accuracy= 1.000\n",
      "Epoch: 0490 cost= 0.000015258 accuracy= 1.000\n",
      "Epoch: 0491 cost= 0.000013444 accuracy= 1.000\n",
      "Epoch: 0492 cost= 0.000013905 accuracy= 1.000\n",
      "Epoch: 0493 cost= 0.000015603 accuracy= 1.000\n",
      "Epoch: 0494 cost= 0.000013211 accuracy= 1.000\n",
      "Epoch: 0495 cost= 0.000010257 accuracy= 1.000\n",
      "Epoch: 0496 cost= 0.000010575 accuracy= 1.000\n",
      "Epoch: 0497 cost= 0.000010759 accuracy= 1.000\n",
      "Epoch: 0498 cost= 0.000009695 accuracy= 1.000\n",
      "Epoch: 0499 cost= 0.000010111 accuracy= 1.000\n",
      "Epoch: 0500 cost= 0.000010180 accuracy= 1.000\n",
      "Epoch: 0501 cost= 0.000013374 accuracy= 1.000\n",
      "Epoch: 0502 cost= 0.000008885 accuracy= 1.000\n",
      "Epoch: 0503 cost= 0.000011954 accuracy= 1.000\n",
      "Epoch: 0504 cost= 0.000010682 accuracy= 1.000\n",
      "Epoch: 0505 cost= 0.000009926 accuracy= 1.000\n",
      "Epoch: 0506 cost= 0.000012339 accuracy= 1.000\n",
      "Epoch: 0507 cost= 0.000010415 accuracy= 1.000\n",
      "Epoch: 0508 cost= 0.000010152 accuracy= 1.000\n",
      "Epoch: 0509 cost= 0.000012303 accuracy= 1.000\n",
      "Epoch: 0510 cost= 0.000009234 accuracy= 1.000\n",
      "Epoch: 0511 cost= 0.000011403 accuracy= 1.000\n",
      "Epoch: 0512 cost= 0.000009510 accuracy= 1.000\n",
      "Epoch: 0513 cost= 0.000010168 accuracy= 1.000\n",
      "Epoch: 0514 cost= 0.000009890 accuracy= 1.000\n",
      "Epoch: 0515 cost= 0.000010777 accuracy= 1.000\n",
      "Epoch: 0516 cost= 0.000008930 accuracy= 1.000\n",
      "Epoch: 0517 cost= 0.000011461 accuracy= 1.000\n",
      "Epoch: 0518 cost= 0.000011374 accuracy= 1.000\n",
      "Epoch: 0519 cost= 0.000008372 accuracy= 1.000\n",
      "Epoch: 0520 cost= 0.000009049 accuracy= 1.000\n",
      "Epoch: 0521 cost= 0.000008426 accuracy= 1.000\n",
      "Epoch: 0522 cost= 0.000009669 accuracy= 1.000\n",
      "Epoch: 0523 cost= 0.000008598 accuracy= 1.000\n",
      "Epoch: 0524 cost= 0.000008914 accuracy= 1.000\n",
      "Epoch: 0525 cost= 0.000011539 accuracy= 1.000\n",
      "Epoch: 0526 cost= 0.000010238 accuracy= 1.000\n",
      "Epoch: 0527 cost= 0.000009752 accuracy= 1.000\n",
      "Epoch: 0528 cost= 0.000007869 accuracy= 1.000\n",
      "Epoch: 0529 cost= 0.000008568 accuracy= 1.000\n",
      "Epoch: 0530 cost= 0.000007387 accuracy= 1.000\n",
      "Epoch: 0531 cost= 0.000011394 accuracy= 1.000\n",
      "Epoch: 0532 cost= 0.000007443 accuracy= 1.000\n",
      "Epoch: 0533 cost= 0.000007342 accuracy= 1.000\n",
      "Epoch: 0534 cost= 0.000008530 accuracy= 1.000\n",
      "Epoch: 0535 cost= 0.000012077 accuracy= 1.000\n",
      "Epoch: 0536 cost= 0.000007784 accuracy= 1.000\n",
      "Epoch: 0537 cost= 0.000008872 accuracy= 1.000\n",
      "Epoch: 0538 cost= 0.000009016 accuracy= 1.000\n",
      "Epoch: 0539 cost= 0.000006875 accuracy= 1.000\n",
      "Epoch: 0540 cost= 0.000009354 accuracy= 1.000\n",
      "Epoch: 0541 cost= 0.000008671 accuracy= 1.000\n",
      "Epoch: 0542 cost= 0.000008602 accuracy= 1.000\n",
      "Epoch: 0543 cost= 0.000006056 accuracy= 1.000\n",
      "Epoch: 0544 cost= 0.000008688 accuracy= 1.000\n",
      "Epoch: 0545 cost= 0.000007961 accuracy= 1.000\n",
      "Epoch: 0546 cost= 0.000008070 accuracy= 1.000\n",
      "Epoch: 0547 cost= 0.000009894 accuracy= 1.000\n",
      "Epoch: 0548 cost= 0.000008233 accuracy= 1.000\n",
      "Epoch: 0549 cost= 0.000007048 accuracy= 1.000\n",
      "Epoch: 0550 cost= 0.000008377 accuracy= 1.000\n",
      "Epoch: 0551 cost= 0.000006715 accuracy= 1.000\n",
      "Epoch: 0552 cost= 0.000007370 accuracy= 1.000\n",
      "Epoch: 0553 cost= 0.000007801 accuracy= 1.000\n",
      "Epoch: 0554 cost= 0.000007821 accuracy= 1.000\n",
      "Epoch: 0555 cost= 0.000007545 accuracy= 1.000\n",
      "Epoch: 0556 cost= 0.000008486 accuracy= 1.000\n",
      "Epoch: 0557 cost= 0.000006369 accuracy= 1.000\n",
      "Epoch: 0558 cost= 0.000007171 accuracy= 1.000\n",
      "Epoch: 0559 cost= 0.000006635 accuracy= 1.000\n",
      "Epoch: 0560 cost= 0.000007847 accuracy= 1.000\n",
      "Epoch: 0561 cost= 0.000006648 accuracy= 1.000\n",
      "Epoch: 0562 cost= 0.000007603 accuracy= 1.000\n",
      "Epoch: 0563 cost= 0.000006895 accuracy= 1.000\n",
      "Epoch: 0564 cost= 0.000007344 accuracy= 1.000\n",
      "Epoch: 0565 cost= 0.000007313 accuracy= 1.000\n",
      "Epoch: 0566 cost= 0.000006387 accuracy= 1.000\n",
      "Epoch: 0567 cost= 0.000007170 accuracy= 1.000\n",
      "Epoch: 0568 cost= 0.000005500 accuracy= 1.000\n",
      "Epoch: 0569 cost= 0.000005436 accuracy= 1.000\n",
      "Epoch: 0570 cost= 0.000005478 accuracy= 1.000\n",
      "Epoch: 0571 cost= 0.000006640 accuracy= 1.000\n",
      "Epoch: 0572 cost= 0.000006644 accuracy= 1.000\n",
      "Epoch: 0573 cost= 0.000005455 accuracy= 1.000\n",
      "Epoch: 0574 cost= 0.000004722 accuracy= 1.000\n",
      "Epoch: 0575 cost= 0.000006613 accuracy= 1.000\n",
      "Epoch: 0576 cost= 0.000006307 accuracy= 1.000\n",
      "Epoch: 0577 cost= 0.000004288 accuracy= 1.000\n",
      "Epoch: 0578 cost= 0.000006870 accuracy= 1.000\n",
      "Epoch: 0579 cost= 0.000004986 accuracy= 1.000\n",
      "Epoch: 0580 cost= 0.000007572 accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_iris_tree1.ckpt\n",
      "Epoch: 0001 cost= 2.267887101 accuracy= 0.200\n",
      "Epoch: 0002 cost= 1.988059149 accuracy= 0.222\n",
      "Epoch: 0003 cost= 1.787657843 accuracy= 0.327\n",
      "Epoch: 0004 cost= 1.699000842 accuracy= 0.368\n",
      "Epoch: 0005 cost= 1.568429021 accuracy= 0.345\n",
      "Epoch: 0006 cost= 1.459282090 accuracy= 0.389\n",
      "Epoch: 0007 cost= 1.409220653 accuracy= 0.385\n",
      "Epoch: 0008 cost= 1.311286583 accuracy= 0.465\n",
      "Epoch: 0009 cost= 1.253068188 accuracy= 0.512\n",
      "Epoch: 0010 cost= 1.098546487 accuracy= 0.577\n",
      "Epoch: 0011 cost= 1.070584490 accuracy= 0.615\n",
      "Epoch: 0012 cost= 0.870985771 accuracy= 0.668\n",
      "Epoch: 0013 cost= 0.774848728 accuracy= 0.727\n",
      "Epoch: 0014 cost= 0.661993756 accuracy= 0.754\n",
      "Epoch: 0015 cost= 0.616091407 accuracy= 0.822\n",
      "Epoch: 0016 cost= 0.531128968 accuracy= 0.860\n",
      "Epoch: 0017 cost= 0.447250396 accuracy= 0.828\n",
      "Epoch: 0018 cost= 0.405021639 accuracy= 0.869\n",
      "Epoch: 0019 cost= 0.296779360 accuracy= 0.910\n",
      "Epoch: 0020 cost= 0.277191784 accuracy= 0.915\n",
      "Epoch: 0021 cost= 0.232355168 accuracy= 0.935\n",
      "Epoch: 0022 cost= 0.201742686 accuracy= 0.946\n",
      "Epoch: 0023 cost= 0.161486630 accuracy= 0.943\n",
      "Epoch: 0024 cost= 0.197943757 accuracy= 0.943\n",
      "Epoch: 0025 cost= 0.200288617 accuracy= 0.927\n",
      "Epoch: 0026 cost= 0.230777509 accuracy= 0.939\n",
      "Epoch: 0027 cost= 0.159635268 accuracy= 0.949\n",
      "Epoch: 0028 cost= 0.213077184 accuracy= 0.950\n",
      "Epoch: 0029 cost= 0.178100394 accuracy= 0.954\n",
      "Epoch: 0030 cost= 0.145534419 accuracy= 0.957\n",
      "Epoch: 0031 cost= 0.163052329 accuracy= 0.960\n",
      "Epoch: 0032 cost= 0.141682935 accuracy= 0.953\n",
      "Epoch: 0033 cost= 0.153671264 accuracy= 0.956\n",
      "Epoch: 0034 cost= 0.163525205 accuracy= 0.953\n",
      "Epoch: 0035 cost= 0.150915513 accuracy= 0.957\n",
      "Epoch: 0036 cost= 0.102031813 accuracy= 0.957\n",
      "Epoch: 0037 cost= 0.137942008 accuracy= 0.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0038 cost= 0.108406350 accuracy= 0.974\n",
      "Epoch: 0039 cost= 0.113780195 accuracy= 0.962\n",
      "Epoch: 0040 cost= 0.122081064 accuracy= 0.972\n",
      "Epoch: 0041 cost= 0.113794649 accuracy= 0.973\n",
      "Epoch: 0042 cost= 0.100959270 accuracy= 0.973\n",
      "Epoch: 0043 cost= 0.080163239 accuracy= 0.969\n",
      "Epoch: 0044 cost= 0.070631605 accuracy= 0.976\n",
      "Epoch: 0045 cost= 0.078362025 accuracy= 0.964\n",
      "Epoch: 0046 cost= 0.090262516 accuracy= 0.972\n",
      "Epoch: 0047 cost= 0.088807422 accuracy= 0.979\n",
      "Epoch: 0048 cost= 0.092586083 accuracy= 0.966\n",
      "Epoch: 0049 cost= 0.073300653 accuracy= 0.968\n",
      "Epoch: 0050 cost= 0.064146541 accuracy= 0.979\n",
      "Epoch: 0051 cost= 0.079622039 accuracy= 0.981\n",
      "Epoch: 0052 cost= 0.065915951 accuracy= 0.982\n",
      "Epoch: 0053 cost= 0.045730489 accuracy= 0.981\n",
      "Epoch: 0054 cost= 0.039485552 accuracy= 0.981\n",
      "Epoch: 0055 cost= 0.061620996 accuracy= 0.957\n",
      "Epoch: 0056 cost= 0.098213105 accuracy= 0.974\n",
      "Epoch: 0057 cost= 0.105397672 accuracy= 0.964\n",
      "Epoch: 0058 cost= 0.079363908 accuracy= 0.979\n",
      "Epoch: 0059 cost= 0.065224501 accuracy= 0.975\n",
      "Epoch: 0060 cost= 0.060721087 accuracy= 0.966\n",
      "Epoch: 0061 cost= 0.067107406 accuracy= 0.978\n",
      "Epoch: 0062 cost= 0.050792490 accuracy= 0.977\n",
      "Epoch: 0063 cost= 0.049470730 accuracy= 0.981\n",
      "Epoch: 0064 cost= 0.062923079 accuracy= 0.989\n",
      "Epoch: 0065 cost= 0.039622500 accuracy= 0.984\n",
      "Epoch: 0066 cost= 0.076433630 accuracy= 0.980\n",
      "Epoch: 0067 cost= 0.052385065 accuracy= 0.984\n",
      "Epoch: 0068 cost= 0.037960841 accuracy= 0.990\n",
      "Epoch: 0069 cost= 0.041524281 accuracy= 0.989\n",
      "Epoch: 0070 cost= 0.036974977 accuracy= 0.970\n",
      "Epoch: 0071 cost= 0.047877669 accuracy= 0.987\n",
      "Epoch: 0072 cost= 0.035241167 accuracy= 0.984\n",
      "Epoch: 0073 cost= 0.045126361 accuracy= 0.991\n",
      "Epoch: 0074 cost= 0.028964363 accuracy= 0.988\n",
      "Epoch: 0075 cost= 0.033121228 accuracy= 0.985\n",
      "Epoch: 0076 cost= 0.042196467 accuracy= 0.988\n",
      "Epoch: 0077 cost= 0.055008784 accuracy= 0.977\n",
      "Epoch: 0078 cost= 0.052497494 accuracy= 0.984\n",
      "Epoch: 0079 cost= 0.056988009 accuracy= 0.987\n",
      "Epoch: 0080 cost= 0.038190320 accuracy= 0.989\n",
      "Epoch: 0081 cost= 0.047256726 accuracy= 0.995\n",
      "Epoch: 0082 cost= 0.045986434 accuracy= 0.989\n",
      "Epoch: 0083 cost= 0.047450962 accuracy= 0.991\n",
      "Epoch: 0084 cost= 0.035473800 accuracy= 0.990\n",
      "Epoch: 0085 cost= 0.026247639 accuracy= 0.990\n",
      "Epoch: 0086 cost= 0.028969976 accuracy= 0.994\n",
      "Epoch: 0087 cost= 0.019978563 accuracy= 0.996\n",
      "Epoch: 0088 cost= 0.026927221 accuracy= 0.987\n",
      "Epoch: 0089 cost= 0.032254692 accuracy= 0.995\n",
      "Epoch: 0090 cost= 0.027143670 accuracy= 0.990\n",
      "Epoch: 0091 cost= 0.032714255 accuracy= 0.992\n",
      "Epoch: 0092 cost= 0.022508859 accuracy= 0.998\n",
      "Epoch: 0093 cost= 0.021908677 accuracy= 0.996\n",
      "Epoch: 0094 cost= 0.027259895 accuracy= 0.974\n",
      "Epoch: 0095 cost= 0.057716923 accuracy= 0.990\n",
      "Epoch: 0096 cost= 0.042733402 accuracy= 0.989\n",
      "Epoch: 0097 cost= 0.049491831 accuracy= 0.996\n",
      "Epoch: 0098 cost= 0.030759236 accuracy= 0.988\n",
      "Epoch: 0099 cost= 0.057573960 accuracy= 0.985\n",
      "Epoch: 0100 cost= 0.048806260 accuracy= 0.984\n",
      "Epoch: 0101 cost= 0.039483939 accuracy= 0.994\n",
      "Epoch: 0102 cost= 0.058416513 accuracy= 0.988\n",
      "Epoch: 0103 cost= 0.028762711 accuracy= 0.981\n",
      "Epoch: 0104 cost= 0.033637723 accuracy= 0.993\n",
      "Epoch: 0105 cost= 0.023791156 accuracy= 0.996\n",
      "Epoch: 0106 cost= 0.010978076 accuracy= 0.997\n",
      "Epoch: 0107 cost= 0.012638980 accuracy= 0.997\n",
      "Epoch: 0108 cost= 0.038641853 accuracy= 0.989\n",
      "Epoch: 0109 cost= 0.028013940 accuracy= 0.983\n",
      "Epoch: 0110 cost= 0.035711331 accuracy= 0.986\n",
      "Epoch: 0111 cost= 0.038175461 accuracy= 0.988\n",
      "Epoch: 0112 cost= 0.021374481 accuracy= 0.987\n",
      "Epoch: 0113 cost= 0.058997665 accuracy= 0.989\n",
      "Epoch: 0114 cost= 0.021272060 accuracy= 0.989\n",
      "Epoch: 0115 cost= 0.026893063 accuracy= 0.996\n",
      "Epoch: 0116 cost= 0.018157319 accuracy= 0.995\n",
      "Epoch: 0117 cost= 0.017475535 accuracy= 0.988\n",
      "Epoch: 0118 cost= 0.030568006 accuracy= 0.997\n",
      "Epoch: 0119 cost= 0.014598754 accuracy= 0.994\n",
      "Epoch: 0120 cost= 0.006587300 accuracy= 0.997\n",
      "Epoch: 0121 cost= 0.022114121 accuracy= 0.988\n",
      "Epoch: 0122 cost= 0.042994479 accuracy= 0.983\n",
      "Epoch: 0123 cost= 0.029016008 accuracy= 0.991\n",
      "Epoch: 0124 cost= 0.027894181 accuracy= 0.985\n",
      "Epoch: 0125 cost= 0.024331889 accuracy= 0.998\n",
      "Epoch: 0126 cost= 0.013455430 accuracy= 0.995\n",
      "Epoch: 0127 cost= 0.010043040 accuracy= 0.991\n",
      "Epoch: 0128 cost= 0.013891423 accuracy= 0.998\n",
      "Epoch: 0129 cost= 0.019968757 accuracy= 0.996\n",
      "Epoch: 0130 cost= 0.014643094 accuracy= 0.996\n",
      "Epoch: 0131 cost= 0.005410859 accuracy= 0.996\n",
      "Epoch: 0132 cost= 0.006137827 accuracy= 0.999\n",
      "Epoch: 0133 cost= 0.007097617 accuracy= 0.997\n",
      "Epoch: 0134 cost= 0.009246409 accuracy= 0.996\n",
      "Epoch: 0135 cost= 0.007471046 accuracy= 0.997\n",
      "Epoch: 0136 cost= 0.002807747 accuracy= 0.998\n",
      "Epoch: 0137 cost= 0.004444357 accuracy= 0.997\n",
      "Epoch: 0138 cost= 0.005848009 accuracy= 0.999\n",
      "Epoch: 0139 cost= 0.003950769 accuracy= 0.999\n",
      "Epoch: 0140 cost= 0.016182001 accuracy= 0.981\n",
      "Epoch: 0141 cost= 0.030486008 accuracy= 0.963\n",
      "Epoch: 0142 cost= 0.080996765 accuracy= 0.982\n",
      "Epoch: 0143 cost= 0.026601197 accuracy= 0.994\n",
      "Epoch: 0144 cost= 0.017051710 accuracy= 0.998\n",
      "Epoch: 0145 cost= 0.013324627 accuracy= 0.998\n",
      "Epoch: 0146 cost= 0.057928488 accuracy= 0.996\n",
      "Epoch: 0147 cost= 0.019408765 accuracy= 0.995\n",
      "Epoch: 0148 cost= 0.015772431 accuracy= 0.998\n",
      "Epoch: 0149 cost= 0.006908872 accuracy= 0.989\n",
      "Epoch: 0150 cost= 0.018404463 accuracy= 0.955\n",
      "Epoch: 0151 cost= 0.134426727 accuracy= 0.979\n",
      "Epoch: 0152 cost= 0.047545179 accuracy= 0.988\n",
      "Epoch: 0153 cost= 0.046110175 accuracy= 0.982\n",
      "Epoch: 0154 cost= 0.112643296 accuracy= 0.986\n",
      "Epoch: 0155 cost= 0.152299556 accuracy= 0.952\n",
      "Epoch: 0156 cost= 0.084752113 accuracy= 0.987\n",
      "Epoch: 0157 cost= 0.055771934 accuracy= 0.988\n",
      "Epoch: 0158 cost= 0.011906645 accuracy= 0.996\n",
      "Epoch: 0159 cost= 0.014768348 accuracy= 0.998\n",
      "Epoch: 0160 cost= 0.014597320 accuracy= 0.998\n",
      "Epoch: 0161 cost= 0.014448214 accuracy= 0.997\n",
      "Epoch: 0162 cost= 0.014121010 accuracy= 0.999\n",
      "Epoch: 0163 cost= 0.012186509 accuracy= 0.996\n",
      "Epoch: 0164 cost= 0.011302571 accuracy= 0.998\n",
      "Epoch: 0165 cost= 0.005664480 accuracy= 0.999\n",
      "Epoch: 0166 cost= 0.006919452 accuracy= 0.999\n",
      "Epoch: 0167 cost= 0.005199387 accuracy= 0.999\n",
      "Epoch: 0168 cost= 0.007443002 accuracy= 0.999\n",
      "Epoch: 0169 cost= 0.006773685 accuracy= 0.998\n",
      "Epoch: 0170 cost= 0.007703628 accuracy= 0.999\n",
      "Epoch: 0171 cost= 0.011154551 accuracy= 0.999\n",
      "Epoch: 0172 cost= 0.005382074 accuracy= 0.999\n",
      "Epoch: 0173 cost= 0.007192765 accuracy= 0.997\n",
      "Epoch: 0174 cost= 0.010699096 accuracy= 0.999\n",
      "Epoch: 0175 cost= 0.005396588 accuracy= 0.998\n",
      "Epoch: 0176 cost= 0.003158202 accuracy= 0.999\n",
      "Epoch: 0177 cost= 0.006716666 accuracy= 0.999\n",
      "Epoch: 0178 cost= 0.014012960 accuracy= 0.998\n",
      "Epoch: 0179 cost= 0.007097797 accuracy= 0.999\n",
      "Epoch: 0180 cost= 0.006376522 accuracy= 0.996\n",
      "Epoch: 0181 cost= 0.003936840 accuracy= 0.998\n",
      "Epoch: 0182 cost= 0.001694108 accuracy= 0.998\n",
      "Epoch: 0183 cost= 0.011359341 accuracy= 0.997\n",
      "Epoch: 0184 cost= 0.007195991 accuracy= 0.999\n",
      "Epoch: 0185 cost= 0.005910198 accuracy= 0.997\n",
      "Epoch: 0186 cost= 0.002891443 accuracy= 0.999\n",
      "Epoch: 0187 cost= 0.002339250 accuracy= 0.999\n",
      "Epoch: 0188 cost= 0.004248210 accuracy= 0.999\n",
      "Epoch: 0189 cost= 0.002362576 accuracy= 0.999\n",
      "Epoch: 0190 cost= 0.001016754 accuracy= 0.999\n",
      "Epoch: 0191 cost= 0.025594050 accuracy= 0.999\n",
      "Epoch: 0192 cost= 0.005677867 accuracy= 0.996\n",
      "Epoch: 0193 cost= 0.005781999 accuracy= 0.995\n",
      "Epoch: 0194 cost= 0.007721039 accuracy= 0.998\n",
      "Epoch: 0195 cost= 0.006707772 accuracy= 0.999\n",
      "Epoch: 0196 cost= 0.003237055 accuracy= 0.999\n",
      "Epoch: 0197 cost= 0.004723500 accuracy= 0.997\n",
      "Epoch: 0198 cost= 0.003159630 accuracy= 0.998\n",
      "Epoch: 0199 cost= 0.002389225 accuracy= 0.999\n",
      "Epoch: 0200 cost= 0.004573017 accuracy= 0.999\n",
      "Epoch: 0201 cost= 0.018597660 accuracy= 0.999\n",
      "Epoch: 0202 cost= 0.008748640 accuracy= 0.998\n",
      "Epoch: 0203 cost= 0.009085041 accuracy= 0.999\n",
      "Epoch: 0204 cost= 0.007735991 accuracy= 0.996\n",
      "Epoch: 0205 cost= 0.004092391 accuracy= 0.996\n",
      "Epoch: 0206 cost= 0.004764195 accuracy= 0.998\n",
      "Epoch: 0207 cost= 0.008748121 accuracy= 1.000\n",
      "Epoch: 0208 cost= 0.013080667 accuracy= 0.994\n",
      "Epoch: 0209 cost= 0.017668631 accuracy= 0.997\n",
      "Epoch: 0210 cost= 0.013366610 accuracy= 0.994\n",
      "Epoch: 0211 cost= 0.023276901 accuracy= 0.999\n",
      "Epoch: 0212 cost= 0.021727926 accuracy= 0.989\n",
      "Epoch: 0213 cost= 0.016502448 accuracy= 0.997\n",
      "Epoch: 0214 cost= 0.009394245 accuracy= 0.999\n",
      "Epoch: 0215 cost= 0.006052713 accuracy= 0.999\n",
      "Epoch: 0216 cost= 0.004582824 accuracy= 0.999\n",
      "Epoch: 0217 cost= 0.002770005 accuracy= 1.000\n",
      "Epoch: 0218 cost= 0.002452412 accuracy= 0.999\n",
      "Epoch: 0219 cost= 0.001089552 accuracy= 0.999\n",
      "Epoch: 0220 cost= 0.002876389 accuracy= 1.000\n",
      "Epoch: 0221 cost= 0.002885376 accuracy= 1.000\n",
      "Epoch: 0222 cost= 0.006009374 accuracy= 0.996\n",
      "Epoch: 0223 cost= 0.004896240 accuracy= 0.998\n",
      "Epoch: 0224 cost= 0.005451290 accuracy= 0.994\n",
      "Epoch: 0225 cost= 0.012301283 accuracy= 0.991\n",
      "Epoch: 0226 cost= 0.031727153 accuracy= 0.989\n",
      "Epoch: 0227 cost= 0.022548618 accuracy= 0.997\n",
      "Epoch: 0228 cost= 0.017507366 accuracy= 0.994\n",
      "Epoch: 0229 cost= 0.025422579 accuracy= 0.997\n",
      "Epoch: 0230 cost= 0.009379477 accuracy= 0.999\n",
      "Epoch: 0231 cost= 0.008628411 accuracy= 0.988\n",
      "Epoch: 0232 cost= 0.035642702 accuracy= 0.984\n",
      "Epoch: 0233 cost= 0.020425228 accuracy= 0.971\n",
      "Epoch: 0234 cost= 0.026456897 accuracy= 0.998\n",
      "Epoch: 0235 cost= 0.005112318 accuracy= 1.000\n",
      "Epoch: 0236 cost= 0.003145926 accuracy= 0.999\n",
      "Epoch: 0237 cost= 0.003608258 accuracy= 0.999\n",
      "Epoch: 0238 cost= 0.003521115 accuracy= 1.000\n",
      "Epoch: 0239 cost= 0.002420806 accuracy= 0.998\n",
      "Epoch: 0240 cost= 0.001566447 accuracy= 0.999\n",
      "Epoch: 0241 cost= 0.004240630 accuracy= 0.998\n",
      "Epoch: 0242 cost= 0.003135630 accuracy= 1.000\n",
      "Epoch: 0243 cost= 0.001134310 accuracy= 0.999\n",
      "Epoch: 0244 cost= 0.000506810 accuracy= 0.999\n",
      "Epoch: 0245 cost= 0.003129497 accuracy= 1.000\n",
      "Epoch: 0246 cost= 0.003305850 accuracy= 0.997\n",
      "Epoch: 0247 cost= 0.003144725 accuracy= 0.998\n",
      "Epoch: 0248 cost= 0.004696341 accuracy= 1.000\n",
      "Epoch: 0249 cost= 0.002492524 accuracy= 0.999\n",
      "Epoch: 0250 cost= 0.003056016 accuracy= 1.000\n",
      "Epoch: 0251 cost= 0.003688066 accuracy= 0.997\n",
      "Epoch: 0252 cost= 0.009310897 accuracy= 0.985\n",
      "Epoch: 0253 cost= 0.017396853 accuracy= 0.998\n",
      "Epoch: 0254 cost= 0.004061189 accuracy= 0.999\n",
      "Epoch: 0255 cost= 0.009765149 accuracy= 0.995\n",
      "Epoch: 0256 cost= 0.018268913 accuracy= 0.990\n",
      "Epoch: 0257 cost= 0.009394290 accuracy= 0.999\n",
      "Epoch: 0258 cost= 0.014969667 accuracy= 0.996\n",
      "Epoch: 0259 cost= 0.006431121 accuracy= 0.999\n",
      "Epoch: 0260 cost= 0.004593810 accuracy= 0.996\n",
      "Epoch: 0261 cost= 0.012182092 accuracy= 0.998\n",
      "Epoch: 0262 cost= 0.014245105 accuracy= 0.998\n",
      "Epoch: 0263 cost= 0.005972651 accuracy= 0.999\n",
      "Epoch: 0264 cost= 0.004979889 accuracy= 0.997\n",
      "Epoch: 0265 cost= 0.002463550 accuracy= 0.999\n",
      "Epoch: 0266 cost= 0.001332560 accuracy= 0.999\n",
      "Epoch: 0267 cost= 0.007135281 accuracy= 0.997\n",
      "Epoch: 0268 cost= 0.012397834 accuracy= 0.999\n",
      "Epoch: 0269 cost= 0.004671163 accuracy= 0.999\n",
      "Epoch: 0270 cost= 0.001081427 accuracy= 0.999\n",
      "Epoch: 0271 cost= 0.004530220 accuracy= 0.998\n",
      "Epoch: 0272 cost= 0.003863080 accuracy= 1.000\n",
      "Epoch: 0273 cost= 0.001288393 accuracy= 0.999\n",
      "Epoch: 0274 cost= 0.003424153 accuracy= 1.000\n",
      "Epoch: 0275 cost= 0.001175312 accuracy= 0.999\n",
      "Epoch: 0276 cost= 0.003908855 accuracy= 1.000\n",
      "Epoch: 0277 cost= 0.000712044 accuracy= 1.000\n",
      "Epoch: 0278 cost= 0.000797357 accuracy= 1.000\n",
      "Epoch: 0279 cost= 0.000449633 accuracy= 1.000\n",
      "Epoch: 0280 cost= 0.001034691 accuracy= 1.000\n",
      "Epoch: 0281 cost= 0.000438550 accuracy= 1.000\n",
      "Epoch: 0282 cost= 0.000217782 accuracy= 1.000\n",
      "Epoch: 0283 cost= 0.001451761 accuracy= 1.000\n",
      "Epoch: 0284 cost= 0.002555685 accuracy= 0.999\n",
      "Epoch: 0285 cost= 0.001845262 accuracy= 0.999\n",
      "Epoch: 0286 cost= 0.000609559 accuracy= 0.999\n",
      "Epoch: 0287 cost= 0.000379504 accuracy= 0.999\n",
      "Epoch: 0288 cost= 0.000156035 accuracy= 0.999\n",
      "Epoch: 0289 cost= 0.000183066 accuracy= 1.000\n",
      "Epoch: 0290 cost= 0.001325570 accuracy= 1.000\n",
      "Epoch: 0291 cost= 0.002302046 accuracy= 1.000\n",
      "Epoch: 0292 cost= 0.001643042 accuracy= 1.000\n",
      "Epoch: 0293 cost= 0.000257165 accuracy= 1.000\n",
      "Epoch: 0294 cost= 0.000361201 accuracy= 1.000\n",
      "Epoch: 0295 cost= 0.000231421 accuracy= 1.000\n",
      "Epoch: 0296 cost= 0.000369969 accuracy= 1.000\n",
      "Epoch: 0297 cost= 0.000290864 accuracy= 1.000\n",
      "Epoch: 0298 cost= 0.000237017 accuracy= 1.000\n",
      "Epoch: 0299 cost= 0.000448470 accuracy= 1.000\n",
      "Epoch: 0300 cost= 0.000324003 accuracy= 1.000\n",
      "Epoch: 0301 cost= 0.000222636 accuracy= 1.000\n",
      "Epoch: 0302 cost= 0.000176603 accuracy= 1.000\n",
      "Epoch: 0303 cost= 0.000191208 accuracy= 1.000\n",
      "Epoch: 0304 cost= 0.000166467 accuracy= 1.000\n",
      "Epoch: 0305 cost= 0.000237412 accuracy= 1.000\n",
      "Epoch: 0306 cost= 0.000403158 accuracy= 1.000\n",
      "Epoch: 0307 cost= 0.000466936 accuracy= 1.000\n",
      "Epoch: 0308 cost= 0.000175064 accuracy= 1.000\n",
      "Epoch: 0309 cost= 0.000139737 accuracy= 1.000\n",
      "Epoch: 0310 cost= 0.000196957 accuracy= 1.000\n",
      "Epoch: 0311 cost= 0.000438235 accuracy= 1.000\n",
      "Epoch: 0312 cost= 0.000158993 accuracy= 1.000\n",
      "Epoch: 0313 cost= 0.000177723 accuracy= 1.000\n",
      "Epoch: 0314 cost= 0.000313551 accuracy= 1.000\n",
      "Epoch: 0315 cost= 0.000294278 accuracy= 1.000\n",
      "Epoch: 0316 cost= 0.000160762 accuracy= 1.000\n",
      "Epoch: 0317 cost= 0.000150144 accuracy= 1.000\n",
      "Epoch: 0318 cost= 0.000092008 accuracy= 1.000\n",
      "Epoch: 0319 cost= 0.000151240 accuracy= 1.000\n",
      "Epoch: 0320 cost= 0.000090949 accuracy= 1.000\n",
      "Epoch: 0321 cost= 0.000272051 accuracy= 1.000\n",
      "Epoch: 0322 cost= 0.000139351 accuracy= 1.000\n",
      "Epoch: 0323 cost= 0.000092669 accuracy= 1.000\n",
      "Epoch: 0324 cost= 0.000156488 accuracy= 1.000\n",
      "Epoch: 0325 cost= 0.000288325 accuracy= 1.000\n",
      "Epoch: 0326 cost= 0.000268375 accuracy= 1.000\n",
      "Epoch: 0327 cost= 0.000213816 accuracy= 1.000\n",
      "Epoch: 0328 cost= 0.000181318 accuracy= 1.000\n",
      "Epoch: 0329 cost= 0.000155264 accuracy= 1.000\n",
      "Epoch: 0330 cost= 0.000260594 accuracy= 1.000\n",
      "Epoch: 0331 cost= 0.000130739 accuracy= 1.000\n",
      "Epoch: 0332 cost= 0.000135772 accuracy= 1.000\n",
      "Epoch: 0333 cost= 0.000279684 accuracy= 1.000\n",
      "Epoch: 0334 cost= 0.000170987 accuracy= 1.000\n",
      "Epoch: 0335 cost= 0.000062814 accuracy= 1.000\n",
      "Epoch: 0336 cost= 0.000153686 accuracy= 1.000\n",
      "Epoch: 0337 cost= 0.000181369 accuracy= 1.000\n",
      "Epoch: 0338 cost= 0.000155757 accuracy= 1.000\n",
      "Epoch: 0339 cost= 0.000088217 accuracy= 1.000\n",
      "Epoch: 0340 cost= 0.000067822 accuracy= 1.000\n",
      "Epoch: 0341 cost= 0.000132970 accuracy= 1.000\n",
      "Epoch: 0342 cost= 0.000088951 accuracy= 1.000\n",
      "Epoch: 0343 cost= 0.000073886 accuracy= 1.000\n",
      "Epoch: 0344 cost= 0.000088170 accuracy= 1.000\n",
      "Epoch: 0345 cost= 0.000067675 accuracy= 1.000\n",
      "Epoch: 0346 cost= 0.000161947 accuracy= 1.000\n",
      "Epoch: 0347 cost= 0.000103143 accuracy= 1.000\n",
      "Epoch: 0348 cost= 0.000102132 accuracy= 1.000\n",
      "Epoch: 0349 cost= 0.000121872 accuracy= 1.000\n",
      "Epoch: 0350 cost= 0.000093457 accuracy= 1.000\n",
      "Epoch: 0351 cost= 0.000095546 accuracy= 1.000\n",
      "Epoch: 0352 cost= 0.000062914 accuracy= 1.000\n",
      "Epoch: 0353 cost= 0.000072706 accuracy= 1.000\n",
      "Epoch: 0354 cost= 0.000056279 accuracy= 1.000\n",
      "Epoch: 0355 cost= 0.000178074 accuracy= 1.000\n",
      "Epoch: 0356 cost= 0.000160587 accuracy= 1.000\n",
      "Epoch: 0357 cost= 0.000228489 accuracy= 0.998\n",
      "Epoch: 0358 cost= 0.005370239 accuracy= 0.998\n",
      "Epoch: 0359 cost= 0.186164972 accuracy= 0.931\n",
      "Epoch: 0360 cost= 0.133189140 accuracy= 0.920\n",
      "Epoch: 0361 cost= 0.083088465 accuracy= 0.974\n",
      "Epoch: 0362 cost= 0.053812060 accuracy= 0.997\n",
      "Epoch: 0363 cost= 0.050559231 accuracy= 0.983\n",
      "Epoch: 0364 cost= 0.038006740 accuracy= 0.989\n",
      "Epoch: 0365 cost= 0.034509086 accuracy= 0.989\n",
      "Epoch: 0366 cost= 0.032384408 accuracy= 0.996\n",
      "Epoch: 0367 cost= 0.024344862 accuracy= 0.995\n",
      "Epoch: 0368 cost= 0.035245719 accuracy= 0.978\n",
      "Epoch: 0369 cost= 0.078357374 accuracy= 0.978\n",
      "Epoch: 0370 cost= 0.020245170 accuracy= 0.990\n",
      "Epoch: 0371 cost= 0.018480653 accuracy= 0.996\n",
      "Epoch: 0372 cost= 0.012214178 accuracy= 0.999\n",
      "Epoch: 0373 cost= 0.009965248 accuracy= 0.998\n",
      "Epoch: 0374 cost= 0.006976209 accuracy= 0.994\n",
      "Epoch: 0375 cost= 0.009760049 accuracy= 0.996\n",
      "Epoch: 0376 cost= 0.004028525 accuracy= 0.998\n",
      "Epoch: 0377 cost= 0.000980444 accuracy= 0.999\n",
      "Epoch: 0378 cost= 0.006090393 accuracy= 0.999\n",
      "Epoch: 0379 cost= 0.003756014 accuracy= 1.000\n",
      "Epoch: 0380 cost= 0.003802669 accuracy= 0.999\n",
      "Epoch: 0381 cost= 0.007702753 accuracy= 0.997\n",
      "Epoch: 0382 cost= 0.006557093 accuracy= 0.998\n",
      "Epoch: 0383 cost= 0.000956559 accuracy= 1.000\n",
      "Epoch: 0384 cost= 0.001147280 accuracy= 1.000\n",
      "Epoch: 0385 cost= 0.001147178 accuracy= 1.000\n",
      "Epoch: 0386 cost= 0.002545389 accuracy= 0.999\n",
      "Epoch: 0387 cost= 0.000308226 accuracy= 0.998\n",
      "Epoch: 0388 cost= 0.003289284 accuracy= 0.999\n",
      "Epoch: 0389 cost= 0.009462565 accuracy= 0.996\n",
      "Epoch: 0390 cost= 0.014430702 accuracy= 0.998\n",
      "Epoch: 0391 cost= 0.003694102 accuracy= 0.999\n",
      "Epoch: 0392 cost= 0.003940859 accuracy= 0.999\n",
      "Epoch: 0393 cost= 0.005743735 accuracy= 0.999\n",
      "Epoch: 0394 cost= 0.010710191 accuracy= 1.000\n",
      "Epoch: 0395 cost= 0.003998281 accuracy= 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0396 cost= 0.003476316 accuracy= 0.999\n",
      "Epoch: 0397 cost= 0.001689455 accuracy= 1.000\n",
      "Epoch: 0398 cost= 0.002593806 accuracy= 0.999\n",
      "Epoch: 0399 cost= 0.003668959 accuracy= 0.999\n",
      "Epoch: 0400 cost= 0.004394409 accuracy= 0.999\n",
      "Epoch: 0401 cost= 0.025071883 accuracy= 0.977\n",
      "Epoch: 0402 cost= 0.052694263 accuracy= 0.991\n",
      "Epoch: 0403 cost= 0.017772809 accuracy= 0.996\n",
      "Epoch: 0404 cost= 0.009362807 accuracy= 0.997\n",
      "Epoch: 0405 cost= 0.009849513 accuracy= 0.999\n",
      "Epoch: 0406 cost= 0.002168619 accuracy= 0.999\n",
      "Epoch: 0407 cost= 0.000951588 accuracy= 0.999\n",
      "Epoch: 0408 cost= 0.000397193 accuracy= 0.999\n",
      "Epoch: 0409 cost= 0.000331840 accuracy= 0.999\n",
      "Epoch: 0410 cost= 0.007705299 accuracy= 0.999\n",
      "Epoch: 0411 cost= 0.000670383 accuracy= 0.999\n",
      "Epoch: 0412 cost= 0.005287572 accuracy= 0.999\n",
      "Epoch: 0413 cost= 0.001384544 accuracy= 1.000\n",
      "Epoch: 0414 cost= 0.003498717 accuracy= 1.000\n",
      "Epoch: 0415 cost= 0.000851346 accuracy= 0.999\n",
      "Epoch: 0416 cost= 0.002517601 accuracy= 1.000\n",
      "Epoch: 0417 cost= 0.000394904 accuracy= 1.000\n",
      "Epoch: 0418 cost= 0.000281595 accuracy= 1.000\n",
      "Epoch: 0419 cost= 0.000339039 accuracy= 1.000\n",
      "Epoch: 0420 cost= 0.000285809 accuracy= 1.000\n",
      "Epoch: 0421 cost= 0.000223150 accuracy= 1.000\n",
      "Epoch: 0422 cost= 0.000327008 accuracy= 1.000\n",
      "Epoch: 0423 cost= 0.000231013 accuracy= 1.000\n",
      "Epoch: 0424 cost= 0.000359512 accuracy= 1.000\n",
      "Epoch: 0425 cost= 0.000247379 accuracy= 1.000\n",
      "Epoch: 0426 cost= 0.000444501 accuracy= 1.000\n",
      "Epoch: 0427 cost= 0.000288587 accuracy= 1.000\n",
      "Epoch: 0428 cost= 0.000186361 accuracy= 1.000\n",
      "Epoch: 0429 cost= 0.000298431 accuracy= 1.000\n",
      "Epoch: 0430 cost= 0.000285115 accuracy= 1.000\n",
      "Epoch: 0431 cost= 0.000319796 accuracy= 1.000\n",
      "Epoch: 0432 cost= 0.000130346 accuracy= 1.000\n",
      "Epoch: 0433 cost= 0.000206030 accuracy= 1.000\n",
      "Epoch: 0434 cost= 0.000248837 accuracy= 1.000\n",
      "Epoch: 0435 cost= 0.000149447 accuracy= 1.000\n",
      "Epoch: 0436 cost= 0.000243321 accuracy= 1.000\n",
      "Epoch: 0437 cost= 0.000221496 accuracy= 1.000\n",
      "Epoch: 0438 cost= 0.000211668 accuracy= 1.000\n",
      "Epoch: 0439 cost= 0.000183304 accuracy= 1.000\n",
      "Epoch: 0440 cost= 0.000160940 accuracy= 1.000\n",
      "Epoch: 0441 cost= 0.000194592 accuracy= 1.000\n",
      "Epoch: 0442 cost= 0.000136143 accuracy= 1.000\n",
      "Epoch: 0443 cost= 0.000216259 accuracy= 1.000\n",
      "Epoch: 0444 cost= 0.000248549 accuracy= 1.000\n",
      "Epoch: 0445 cost= 0.000285912 accuracy= 1.000\n",
      "Epoch: 0446 cost= 0.000100658 accuracy= 1.000\n",
      "Epoch: 0447 cost= 0.000130879 accuracy= 1.000\n",
      "Epoch: 0448 cost= 0.000128485 accuracy= 1.000\n",
      "Epoch: 0449 cost= 0.000157894 accuracy= 1.000\n",
      "Epoch: 0450 cost= 0.000164574 accuracy= 1.000\n",
      "Epoch: 0451 cost= 0.000228222 accuracy= 1.000\n",
      "Epoch: 0452 cost= 0.000132426 accuracy= 1.000\n",
      "Epoch: 0453 cost= 0.000127611 accuracy= 1.000\n",
      "Epoch: 0454 cost= 0.000191708 accuracy= 1.000\n",
      "Epoch: 0455 cost= 0.000093066 accuracy= 1.000\n",
      "Epoch: 0456 cost= 0.000117796 accuracy= 1.000\n",
      "Epoch: 0457 cost= 0.000094523 accuracy= 1.000\n",
      "Epoch: 0458 cost= 0.000131031 accuracy= 1.000\n",
      "Epoch: 0459 cost= 0.000061468 accuracy= 1.000\n",
      "Epoch: 0460 cost= 0.000082775 accuracy= 1.000\n",
      "Epoch: 0461 cost= 0.000206100 accuracy= 1.000\n",
      "Epoch: 0462 cost= 0.000102660 accuracy= 1.000\n",
      "Epoch: 0463 cost= 0.000153026 accuracy= 1.000\n",
      "Epoch: 0464 cost= 0.000092565 accuracy= 1.000\n",
      "Epoch: 0465 cost= 0.000089511 accuracy= 1.000\n",
      "Epoch: 0466 cost= 0.000141795 accuracy= 1.000\n",
      "Epoch: 0467 cost= 0.000135172 accuracy= 1.000\n",
      "Epoch: 0468 cost= 0.000121373 accuracy= 1.000\n",
      "Epoch: 0469 cost= 0.000124176 accuracy= 1.000\n",
      "Epoch: 0470 cost= 0.000086335 accuracy= 1.000\n",
      "Epoch: 0471 cost= 0.000159393 accuracy= 1.000\n",
      "Epoch: 0472 cost= 0.000060362 accuracy= 1.000\n",
      "Epoch: 0473 cost= 0.000114277 accuracy= 1.000\n",
      "Epoch: 0474 cost= 0.000120393 accuracy= 1.000\n",
      "Epoch: 0475 cost= 0.000109625 accuracy= 1.000\n",
      "Epoch: 0476 cost= 0.000161161 accuracy= 1.000\n",
      "Epoch: 0477 cost= 0.000128264 accuracy= 1.000\n",
      "Epoch: 0478 cost= 0.000146406 accuracy= 1.000\n",
      "Epoch: 0479 cost= 0.000095510 accuracy= 1.000\n",
      "Epoch: 0480 cost= 0.000086996 accuracy= 1.000\n",
      "Epoch: 0481 cost= 0.000127449 accuracy= 1.000\n",
      "Epoch: 0482 cost= 0.000079173 accuracy= 1.000\n",
      "Epoch: 0483 cost= 0.000107066 accuracy= 1.000\n",
      "Epoch: 0484 cost= 0.000064709 accuracy= 1.000\n",
      "Epoch: 0485 cost= 0.000096274 accuracy= 1.000\n",
      "Epoch: 0486 cost= 0.000075310 accuracy= 1.000\n",
      "Epoch: 0487 cost= 0.000060453 accuracy= 1.000\n",
      "Epoch: 0488 cost= 0.000078287 accuracy= 1.000\n",
      "Epoch: 0489 cost= 0.000074419 accuracy= 1.000\n",
      "Epoch: 0490 cost= 0.000046427 accuracy= 1.000\n",
      "Epoch: 0491 cost= 0.000114199 accuracy= 1.000\n",
      "Epoch: 0492 cost= 0.000100856 accuracy= 1.000\n",
      "Epoch: 0493 cost= 0.000086006 accuracy= 1.000\n",
      "Epoch: 0494 cost= 0.000088452 accuracy= 1.000\n",
      "Epoch: 0495 cost= 0.000073716 accuracy= 1.000\n",
      "Epoch: 0496 cost= 0.000048918 accuracy= 1.000\n",
      "Epoch: 0497 cost= 0.000084499 accuracy= 1.000\n",
      "Epoch: 0498 cost= 0.000046314 accuracy= 1.000\n",
      "Epoch: 0499 cost= 0.000068545 accuracy= 1.000\n",
      "Epoch: 0500 cost= 0.000067702 accuracy= 1.000\n",
      "Epoch: 0501 cost= 0.000093248 accuracy= 1.000\n",
      "Epoch: 0502 cost= 0.000094061 accuracy= 1.000\n",
      "Epoch: 0503 cost= 0.000060508 accuracy= 1.000\n",
      "Epoch: 0504 cost= 0.000083476 accuracy= 1.000\n",
      "Epoch: 0505 cost= 0.000057130 accuracy= 1.000\n",
      "Epoch: 0506 cost= 0.000108338 accuracy= 1.000\n",
      "Epoch: 0507 cost= 0.000092477 accuracy= 1.000\n",
      "Epoch: 0508 cost= 0.000083208 accuracy= 1.000\n",
      "Epoch: 0509 cost= 0.000073065 accuracy= 1.000\n",
      "Epoch: 0510 cost= 0.000082723 accuracy= 1.000\n",
      "Epoch: 0511 cost= 0.000086657 accuracy= 1.000\n",
      "Epoch: 0512 cost= 0.000063416 accuracy= 1.000\n",
      "Epoch: 0513 cost= 0.000068972 accuracy= 1.000\n",
      "Epoch: 0514 cost= 0.000052869 accuracy= 1.000\n",
      "Epoch: 0515 cost= 0.000066766 accuracy= 1.000\n",
      "Epoch: 0516 cost= 0.000047048 accuracy= 1.000\n",
      "Epoch: 0517 cost= 0.000087902 accuracy= 1.000\n",
      "Epoch: 0518 cost= 0.000097998 accuracy= 1.000\n",
      "Epoch: 0519 cost= 0.000059714 accuracy= 1.000\n",
      "Epoch: 0520 cost= 0.000061484 accuracy= 1.000\n",
      "Epoch: 0521 cost= 0.000043782 accuracy= 1.000\n",
      "Epoch: 0522 cost= 0.000067477 accuracy= 1.000\n",
      "Epoch: 0523 cost= 0.000069015 accuracy= 1.000\n",
      "Epoch: 0524 cost= 0.000063521 accuracy= 1.000\n",
      "Epoch: 0525 cost= 0.000045934 accuracy= 1.000\n",
      "Epoch: 0526 cost= 0.000072014 accuracy= 1.000\n",
      "Epoch: 0527 cost= 0.000049633 accuracy= 1.000\n",
      "Epoch: 0528 cost= 0.000053168 accuracy= 1.000\n",
      "Epoch: 0529 cost= 0.000046017 accuracy= 1.000\n",
      "Epoch: 0530 cost= 0.000044825 accuracy= 1.000\n",
      "Epoch: 0531 cost= 0.000069057 accuracy= 1.000\n",
      "Epoch: 0532 cost= 0.000050642 accuracy= 1.000\n",
      "Epoch: 0533 cost= 0.000037712 accuracy= 1.000\n",
      "Epoch: 0534 cost= 0.000060336 accuracy= 1.000\n",
      "Epoch: 0535 cost= 0.000042958 accuracy= 1.000\n",
      "Epoch: 0536 cost= 0.000053758 accuracy= 1.000\n",
      "Epoch: 0537 cost= 0.000040291 accuracy= 1.000\n",
      "Epoch: 0538 cost= 0.000046961 accuracy= 1.000\n",
      "Epoch: 0539 cost= 0.000068102 accuracy= 1.000\n",
      "Epoch: 0540 cost= 0.000047694 accuracy= 1.000\n",
      "Epoch: 0541 cost= 0.000044361 accuracy= 1.000\n",
      "Epoch: 0542 cost= 0.000031473 accuracy= 1.000\n",
      "Epoch: 0543 cost= 0.000017928 accuracy= 1.000\n",
      "Epoch: 0544 cost= 0.000035654 accuracy= 1.000\n",
      "Epoch: 0545 cost= 0.000050126 accuracy= 1.000\n",
      "Epoch: 0546 cost= 0.000038600 accuracy= 1.000\n",
      "Epoch: 0547 cost= 0.000062492 accuracy= 1.000\n",
      "Epoch: 0548 cost= 0.000043361 accuracy= 1.000\n",
      "Epoch: 0549 cost= 0.000051756 accuracy= 1.000\n",
      "Epoch: 0550 cost= 0.000052363 accuracy= 1.000\n",
      "Epoch: 0551 cost= 0.000020530 accuracy= 1.000\n",
      "Epoch: 0552 cost= 0.000039370 accuracy= 1.000\n",
      "Epoch: 0553 cost= 0.000036882 accuracy= 1.000\n",
      "Epoch: 0554 cost= 0.000025256 accuracy= 1.000\n",
      "Epoch: 0555 cost= 0.000039758 accuracy= 1.000\n",
      "Epoch: 0556 cost= 0.000031488 accuracy= 1.000\n",
      "Epoch: 0557 cost= 0.000033535 accuracy= 1.000\n",
      "Epoch: 0558 cost= 0.000043693 accuracy= 1.000\n",
      "Epoch: 0559 cost= 0.000037276 accuracy= 1.000\n",
      "Epoch: 0560 cost= 0.000035170 accuracy= 1.000\n",
      "Epoch: 0561 cost= 0.000046514 accuracy= 1.000\n",
      "Epoch: 0562 cost= 0.000042797 accuracy= 1.000\n",
      "Epoch: 0563 cost= 0.000045380 accuracy= 1.000\n",
      "Epoch: 0564 cost= 0.000022940 accuracy= 1.000\n",
      "Epoch: 0565 cost= 0.000035160 accuracy= 1.000\n",
      "Epoch: 0566 cost= 0.000038926 accuracy= 1.000\n",
      "Epoch: 0567 cost= 0.000023822 accuracy= 1.000\n",
      "Epoch: 0568 cost= 0.000038526 accuracy= 1.000\n",
      "Epoch: 0569 cost= 0.000030440 accuracy= 1.000\n",
      "Epoch: 0570 cost= 0.000027575 accuracy= 1.000\n",
      "Epoch: 0571 cost= 0.000049401 accuracy= 1.000\n",
      "Epoch: 0572 cost= 0.000021314 accuracy= 1.000\n",
      "Epoch: 0573 cost= 0.000025715 accuracy= 1.000\n",
      "Epoch: 0574 cost= 0.000024128 accuracy= 1.000\n",
      "Epoch: 0575 cost= 0.000037153 accuracy= 1.000\n",
      "Epoch: 0576 cost= 0.000026899 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0577 cost= 0.000039521 accuracy= 1.000\n",
      "Epoch: 0578 cost= 0.000029428 accuracy= 1.000\n",
      "Epoch: 0579 cost= 0.000024329 accuracy= 1.000\n",
      "Epoch: 0580 cost= 0.000031708 accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_iris_tree2.ckpt\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\Desktop\\cnn\\djinn\\djinn.py:276: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_iris_tree0.ckpt\n",
      "Model 0 restored\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_iris_tree1.ckpt\n",
      "Model 1 restored\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_iris_tree2.ckpt\n",
      "Model 2 restored\n",
      "(3, 540, 10)\n"
     ]
    }
   ],
   "source": [
    "from djinn import djinn\n",
    "\n",
    "print(\"djinn iris\")    \n",
    "modelname=\"class_djinn_iris\"   # name the model\n",
    "ntrees=3                 # number of trees = number of neural nets in ensemble\n",
    "maxdepth=4               # max depth of tree -- optimize this for each data set\n",
    "dropout_keep=1.0 \n",
    "\n",
    "#initialize the model\n",
    "model=djinn.DJINN_Classifier(ntrees,maxdepth,dropout_keep)\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = out_cnn_train, y_train, out_cnn_test, y_test \n",
    "\n",
    "\n",
    "# find optimal settings: this function returns dict with hyper-parameters\n",
    "# each djinn function accepts random seeds for reproducible behavior\n",
    "# optimal=model.get_hyperparameters(x_train, y_train, random_state=1)\n",
    "# batchsize=optimal['batch_size']\n",
    "# learnrate=optimal['learn_rate']\n",
    "#epochs=optimal['epochs']\n",
    "\n",
    "epochs=580\n",
    "learnrate=0.004\n",
    "batchsize=63\n",
    "\n",
    "\n",
    "# epochs=200\n",
    "# learnrate=0.003\n",
    "# batchsize=32\n",
    "# train the model with hyperparameters determined above\n",
    "model.train(x_train,y_train,epochs=epochs,learn_rate=learnrate, batch_size=batchsize, \n",
    "              display_step=1, save_files=True, file_name=modelname, \n",
    "              save_model=True,model_name=modelname, random_state=1)\n",
    "\n",
    "# *note there is a function model.fit(x_train,y_train, ... ) that wraps \n",
    "# get_hyperparameters() and train(), so that you do not have to manually\n",
    "# pass hyperparameters to train(). However, get_hyperparameters() can\n",
    "# be expensive, so I recommend running it once per dataset and using those\n",
    "# hyperparameter values in train() to save computational time\n",
    "# make predictions\n",
    "m=model.predict(x_test) #returns the median prediction if more than one tree\n",
    "\n",
    "import sklearn\n",
    "#evaluate results\n",
    "acc=sklearn.metrics.accuracy_score(y_test,m.flatten())  \n",
    "#close model \n",
    "model.close_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + ( SVM, XGB, DTree, ExtraTrees, RandomFores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to Random Forest Classifier to predict its class\n",
    "predictions = rf.predict(out_cnn_test)\n",
    "accuracy_CNN_RF=accuracy_score(predictions , y_test)\n",
    "#print('CNN+RF : Accuracy:', accuracy_CNN_RF, '%.')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "Ext = ExtraTreesClassifier(n_estimators=100)\n",
    "Ext.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictions = Ext.predict(out_cnn_test)\n",
    "accuracy_CNN_Ext=accuracy_score(predictions , y_test)\n",
    "#print('CNN+Extrat : Accuracy:', accuracy_CNN_Ext, '%.')\n",
    "\n",
    "\n",
    "#Applying SVC (Support Vector Classification)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "svm.fit(out_cnn_train, y_train)\n",
    "#print('The accuracy of the SVM classifier on training data is {:.4f}'.format(svm.score(x_train, y_train)))\n",
    "\n",
    "\n",
    "#Applying XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf = xgb_clf.fit(out_cnn_train, y_train)\n",
    "#print('The accuracy of the XGBoost classifier on training data is {:.4f}'.format(xgb_clf.score(x_train, y_train)))\n",
    "\n",
    "\n",
    "#Applying Decision Tree\n",
    "from sklearn import tree\n",
    "#Create tree object\n",
    "decision_tree = tree.DecisionTreeClassifier()\n",
    "#Train DT based on scaled training set\n",
    "decision_tree.fit(out_cnn_train, y_train)\n",
    "#Print performance\n",
    "#print('The accuracy of the Decision Tree classifier on training data is {:.4f}'.format(decision_tree.score(x_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 10-fold cross-validation with the best KNN model\n",
    "# This will allow us to get a better results\n",
    "cx_train = np.concatenate((x_train, x_test), 0)\n",
    "cy_train = np.concatenate((y_train, y_test), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by RandomForest, ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier : from dataset originl\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train_, y_train_)\n",
    "predictions = rf.predict(x_test_)\n",
    "accuracy_RF=accuracy_score(predictions , y_test_)\n",
    "#print('RF : Accuracy:', accuracy_RF, '%.')\n",
    "\n",
    "# ExtraTreesClassifier : from dataset originl\n",
    "Extra = ExtraTreesClassifier(n_estimators=100)\n",
    "Extra.fit(x_train_, y_train_)\n",
    "predictions = Extra.predict(x_test_)\n",
    "accuracy_Extra=accuracy_score(predictions , y_test_)\n",
    "#print('Extra : Accuracy:', accuracy_Extra, '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RF          :: 0.9815 %.\n",
      "Accuracy Extrat      :: 0.9796 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN         :: 0.9852 %.\n",
      "Accuracy CNN+RF      :: 0.9833 %.\n",
      "Accuracy CNN+Extrat  :: 0.9889 %.\n",
      "Accuracy CNN+SVM     :: 0.3981 %.\n",
      "Accuracy CNN+XGBoost :: 0.9722 %.\n",
      "Accuracy CNN+DTree   :: 0.9630 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN+RF+MLP  :: 0.9796 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN+SVM using cv=10     :: 0.5876 %.\n",
      "Accuracy CNN+rf  using cv=10     :: 0.9872 %.\n",
      "Accuracy CNN+XGBoost using cv=10 :: 0.9805 %.\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy RF          ::',  \"{:.4f}\".format(accuracy_RF),'%.')\n",
    "print('Accuracy Extrat      ::',  \"{:.4f}\".format(accuracy_Extra),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN         ::',  \"{:.4f}\".format(accuracy_CNN), '%.')\n",
    "print('Accuracy CNN+RF      ::',  \"{:.4f}\".format(accuracy_CNN_RF), '%.')\n",
    "print('Accuracy CNN+Extrat  ::',  \"{:.4f}\".format(accuracy_CNN_Ext), '%.')\n",
    "print('Accuracy CNN+SVM     :: {:.4f}'.format(svm.score(x_test, y_test)),'%.')\n",
    "print('Accuracy CNN+XGBoost :: {:.4f}'.format(xgb_clf.score(x_test, y_test)),'%.')\n",
    "print('Accuracy CNN+DTree   :: {:.4f}'.format(decision_tree.score(x_test, y_test)),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN+RF+MLP  ::',  \"{:.4f}\".format(acc),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN+SVM using cv=10     :: {:.4f}' .format(cross_val_score(svm, cx_train, cy_train, cv=10, scoring='accuracy').mean()),'%.')\n",
    "print('Accuracy CNN+rf  using cv=10     :: {:.4f}' .format(cross_val_score(rf, cx_train, cy_train, cv=10, scoring='accuracy').mean() ),'%.')\n",
    "print('Accuracy CNN+XGBoost using cv=10 :: {:.4f}'.format(cross_val_score(xgb_clf, cx_train, cy_train, cv=10, scoring='accuracy').mean() ),'%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/38957/keras-conv1d-for-simple-data-target-prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
