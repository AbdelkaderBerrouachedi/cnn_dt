{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# cnn model\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "try: from sklearn.model_selection import train_test_split\n",
    "except: from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the train & test and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data, split into training/testing groups\n",
    "d=datasets.load_digits()\n",
    "X=d.data\n",
    "Y=d.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.asarray(X), np.asarray(Y), test_size=0.3, shuffle= True)\n",
    "x_train_, x_test_, y_train_, y_test_ = x_train, x_test, y_train, y_test\n",
    "# The known number of output classes.\n",
    "num_classes = len(np.unique(y_train))\n",
    "# Input image dimensions\n",
    "input_shape = (X.shape[1],)\n",
    "\n",
    "# Convert class vectors to binary class matrices. This uses 1 hot encoding.\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], X.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0], X.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def CNN_net():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1],1)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(X.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1257 samples, validate on 540 samples\n",
      "Epoch 1/210\n",
      "1257/1257 [==============================] - 2s 1ms/step - loss: 3.4881 - accuracy: 0.1018 - val_loss: 2.2874 - val_accuracy: 0.1241\n",
      "Epoch 2/210\n",
      "1257/1257 [==============================] - 1s 472us/step - loss: 2.9606 - accuracy: 0.0947 - val_loss: 2.2680 - val_accuracy: 0.1222\n",
      "Epoch 3/210\n",
      "1257/1257 [==============================] - 1s 417us/step - loss: 2.6707 - accuracy: 0.0994 - val_loss: 2.2639 - val_accuracy: 0.1426\n",
      "Epoch 4/210\n",
      "1257/1257 [==============================] - 1s 413us/step - loss: 2.5051 - accuracy: 0.1050 - val_loss: 2.2711 - val_accuracy: 0.1630\n",
      "Epoch 5/210\n",
      "1257/1257 [==============================] - 1s 433us/step - loss: 2.3917 - accuracy: 0.1122 - val_loss: 2.2814 - val_accuracy: 0.1722\n",
      "Epoch 6/210\n",
      "1257/1257 [==============================] - 1s 431us/step - loss: 2.3303 - accuracy: 0.1169 - val_loss: 2.2919 - val_accuracy: 0.1611\n",
      "Epoch 7/210\n",
      "1257/1257 [==============================] - 1s 425us/step - loss: 2.2989 - accuracy: 0.1225 - val_loss: 2.2974 - val_accuracy: 0.1889\n",
      "Epoch 8/210\n",
      "1257/1257 [==============================] - 1s 448us/step - loss: 2.2995 - accuracy: 0.1376 - val_loss: 2.2998 - val_accuracy: 0.1222\n",
      "Epoch 9/210\n",
      "1257/1257 [==============================] - 1s 406us/step - loss: 2.2927 - accuracy: 0.1161 - val_loss: 2.3007 - val_accuracy: 0.1148\n",
      "Epoch 10/210\n",
      "1257/1257 [==============================] - 1s 401us/step - loss: 2.2755 - accuracy: 0.1161 - val_loss: 2.3008 - val_accuracy: 0.1019\n",
      "Epoch 11/210\n",
      "1257/1257 [==============================] - 0s 370us/step - loss: 2.2827 - accuracy: 0.1225 - val_loss: 2.3001 - val_accuracy: 0.0944\n",
      "Epoch 12/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 2.2884 - accuracy: 0.1185 - val_loss: 2.2984 - val_accuracy: 0.0815\n",
      "Epoch 13/210\n",
      "1257/1257 [==============================] - 1s 405us/step - loss: 2.2777 - accuracy: 0.1177 - val_loss: 2.2952 - val_accuracy: 0.0667\n",
      "Epoch 14/210\n",
      "1257/1257 [==============================] - 1s 418us/step - loss: 2.2747 - accuracy: 0.1321 - val_loss: 2.2908 - val_accuracy: 0.1037\n",
      "Epoch 15/210\n",
      "1257/1257 [==============================] - 1s 419us/step - loss: 2.2674 - accuracy: 0.1488 - val_loss: 2.2852 - val_accuracy: 0.1500\n",
      "Epoch 16/210\n",
      "1257/1257 [==============================] - 1s 426us/step - loss: 2.2763 - accuracy: 0.1265 - val_loss: 2.2788 - val_accuracy: 0.1926\n",
      "Epoch 17/210\n",
      "1257/1257 [==============================] - 1s 405us/step - loss: 2.2658 - accuracy: 0.1249 - val_loss: 2.2722 - val_accuracy: 0.1981\n",
      "Epoch 18/210\n",
      "1257/1257 [==============================] - 1s 410us/step - loss: 2.2549 - accuracy: 0.1535 - val_loss: 2.2654 - val_accuracy: 0.2000\n",
      "Epoch 19/210\n",
      "1257/1257 [==============================] - 0s 395us/step - loss: 2.2508 - accuracy: 0.1575 - val_loss: 2.2580 - val_accuracy: 0.1981\n",
      "Epoch 20/210\n",
      "1257/1257 [==============================] - 0s 387us/step - loss: 2.2444 - accuracy: 0.1615 - val_loss: 2.2493 - val_accuracy: 0.1889\n",
      "Epoch 21/210\n",
      "1257/1257 [==============================] - 1s 450us/step - loss: 2.2443 - accuracy: 0.1607 - val_loss: 2.2378 - val_accuracy: 0.2074\n",
      "Epoch 22/210\n",
      "1257/1257 [==============================] - 0s 376us/step - loss: 2.2384 - accuracy: 0.1512 - val_loss: 2.2223 - val_accuracy: 0.2537\n",
      "Epoch 23/210\n",
      "1257/1257 [==============================] - 0s 385us/step - loss: 2.2373 - accuracy: 0.1702 - val_loss: 2.2036 - val_accuracy: 0.2852\n",
      "Epoch 24/210\n",
      "1257/1257 [==============================] - 0s 377us/step - loss: 2.2155 - accuracy: 0.1766 - val_loss: 2.1836 - val_accuracy: 0.3148\n",
      "Epoch 25/210\n",
      "1257/1257 [==============================] - 0s 384us/step - loss: 2.2198 - accuracy: 0.1758 - val_loss: 2.1638 - val_accuracy: 0.3278\n",
      "Epoch 26/210\n",
      "1257/1257 [==============================] - 1s 405us/step - loss: 2.1974 - accuracy: 0.1806 - val_loss: 2.1445 - val_accuracy: 0.3370\n",
      "Epoch 27/210\n",
      "1257/1257 [==============================] - 0s 339us/step - loss: 2.1945 - accuracy: 0.1957 - val_loss: 2.1263 - val_accuracy: 0.3407\n",
      "Epoch 28/210\n",
      "1257/1257 [==============================] - 0s 339us/step - loss: 2.1685 - accuracy: 0.1758 - val_loss: 2.1075 - val_accuracy: 0.3407\n",
      "Epoch 29/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 2.1599 - accuracy: 0.1862 - val_loss: 2.0878 - val_accuracy: 0.3519\n",
      "Epoch 30/210\n",
      "1257/1257 [==============================] - 0s 337us/step - loss: 2.1390 - accuracy: 0.2148 - val_loss: 2.0658 - val_accuracy: 0.3648\n",
      "Epoch 31/210\n",
      "1257/1257 [==============================] - 0s 372us/step - loss: 2.1311 - accuracy: 0.1949 - val_loss: 2.0401 - val_accuracy: 0.3926\n",
      "Epoch 32/210\n",
      "1257/1257 [==============================] - 0s 378us/step - loss: 2.1070 - accuracy: 0.2116 - val_loss: 2.0093 - val_accuracy: 0.4259\n",
      "Epoch 33/210\n",
      "1257/1257 [==============================] - 0s 394us/step - loss: 2.1066 - accuracy: 0.2323 - val_loss: 1.9753 - val_accuracy: 0.4500\n",
      "Epoch 34/210\n",
      "1257/1257 [==============================] - 1s 406us/step - loss: 2.0917 - accuracy: 0.2355 - val_loss: 1.9368 - val_accuracy: 0.4981\n",
      "Epoch 35/210\n",
      "1257/1257 [==============================] - 1s 429us/step - loss: 2.0651 - accuracy: 0.2498 - val_loss: 1.8951 - val_accuracy: 0.5593\n",
      "Epoch 36/210\n",
      "1257/1257 [==============================] - 1s 421us/step - loss: 2.0298 - accuracy: 0.2697 - val_loss: 1.8500 - val_accuracy: 0.6167\n",
      "Epoch 37/210\n",
      "1257/1257 [==============================] - 1s 398us/step - loss: 1.9936 - accuracy: 0.2705 - val_loss: 1.8015 - val_accuracy: 0.6704\n",
      "Epoch 38/210\n",
      "1257/1257 [==============================] - 0s 389us/step - loss: 1.9740 - accuracy: 0.2808 - val_loss: 1.7507 - val_accuracy: 0.7000\n",
      "Epoch 39/210\n",
      "1257/1257 [==============================] - 1s 445us/step - loss: 1.9802 - accuracy: 0.2745 - val_loss: 1.6969 - val_accuracy: 0.7241\n",
      "Epoch 40/210\n",
      "1257/1257 [==============================] - 1s 409us/step - loss: 1.9166 - accuracy: 0.2983 - val_loss: 1.6415 - val_accuracy: 0.7426\n",
      "Epoch 41/210\n",
      "1257/1257 [==============================] - 1s 409us/step - loss: 1.8862 - accuracy: 0.3142 - val_loss: 1.5835 - val_accuracy: 0.7537\n",
      "Epoch 42/210\n",
      "1257/1257 [==============================] - 0s 380us/step - loss: 1.8616 - accuracy: 0.3190 - val_loss: 1.5269 - val_accuracy: 0.7611\n",
      "Epoch 43/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 1.8106 - accuracy: 0.3564 - val_loss: 1.4711 - val_accuracy: 0.7759\n",
      "Epoch 44/210\n",
      "1257/1257 [==============================] - 0s 346us/step - loss: 1.8083 - accuracy: 0.3477 - val_loss: 1.4165 - val_accuracy: 0.7778\n",
      "Epoch 45/210\n",
      "1257/1257 [==============================] - 0s 378us/step - loss: 1.7854 - accuracy: 0.3532 - val_loss: 1.3613 - val_accuracy: 0.7833\n",
      "Epoch 46/210\n",
      "1257/1257 [==============================] - 0s 336us/step - loss: 1.7252 - accuracy: 0.3771 - val_loss: 1.3050 - val_accuracy: 0.7907\n",
      "Epoch 47/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 1.6968 - accuracy: 0.3962 - val_loss: 1.2505 - val_accuracy: 0.7963\n",
      "Epoch 48/210\n",
      "1257/1257 [==============================] - 0s 347us/step - loss: 1.6389 - accuracy: 0.4312 - val_loss: 1.1978 - val_accuracy: 0.8019\n",
      "Epoch 49/210\n",
      "1257/1257 [==============================] - 0s 344us/step - loss: 1.5696 - accuracy: 0.4519 - val_loss: 1.1412 - val_accuracy: 0.8019\n",
      "Epoch 50/210\n",
      "1257/1257 [==============================] - 0s 378us/step - loss: 1.5920 - accuracy: 0.4463 - val_loss: 1.0857 - val_accuracy: 0.8037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/210\n",
      "1257/1257 [==============================] - 0s 344us/step - loss: 1.5524 - accuracy: 0.4487 - val_loss: 1.0294 - val_accuracy: 0.8074\n",
      "Epoch 52/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 1.4921 - accuracy: 0.4956 - val_loss: 0.9716 - val_accuracy: 0.8185\n",
      "Epoch 53/210\n",
      "1257/1257 [==============================] - 0s 342us/step - loss: 1.4689 - accuracy: 0.4821 - val_loss: 0.9165 - val_accuracy: 0.8259\n",
      "Epoch 54/210\n",
      "1257/1257 [==============================] - 0s 339us/step - loss: 1.4318 - accuracy: 0.5084 - val_loss: 0.8658 - val_accuracy: 0.8296\n",
      "Epoch 55/210\n",
      "1257/1257 [==============================] - 0s 332us/step - loss: 1.3842 - accuracy: 0.5322 - val_loss: 0.8187 - val_accuracy: 0.8370\n",
      "Epoch 56/210\n",
      "1257/1257 [==============================] - 0s 351us/step - loss: 1.3398 - accuracy: 0.5426 - val_loss: 0.7747 - val_accuracy: 0.8370\n",
      "Epoch 57/210\n",
      "1257/1257 [==============================] - 0s 347us/step - loss: 1.3276 - accuracy: 0.5322 - val_loss: 0.7329 - val_accuracy: 0.8444\n",
      "Epoch 58/210\n",
      "1257/1257 [==============================] - 0s 336us/step - loss: 1.3433 - accuracy: 0.5449 - val_loss: 0.6989 - val_accuracy: 0.8537\n",
      "Epoch 59/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 1.2614 - accuracy: 0.5537 - val_loss: 0.6671 - val_accuracy: 0.8556\n",
      "Epoch 60/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 1.2041 - accuracy: 0.5982 - val_loss: 0.6353 - val_accuracy: 0.8648\n",
      "Epoch 61/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 1.1945 - accuracy: 0.5839 - val_loss: 0.6059 - val_accuracy: 0.8648\n",
      "Epoch 62/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 1.1763 - accuracy: 0.5967 - val_loss: 0.5787 - val_accuracy: 0.8704\n",
      "Epoch 63/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 1.1714 - accuracy: 0.5943 - val_loss: 0.5536 - val_accuracy: 0.8796\n",
      "Epoch 64/210\n",
      "1257/1257 [==============================] - 0s 349us/step - loss: 1.1199 - accuracy: 0.6245 - val_loss: 0.5251 - val_accuracy: 0.8833\n",
      "Epoch 65/210\n",
      "1257/1257 [==============================] - 0s 354us/step - loss: 1.0731 - accuracy: 0.6404 - val_loss: 0.4991 - val_accuracy: 0.8889\n",
      "Epoch 66/210\n",
      "1257/1257 [==============================] - 0s 376us/step - loss: 1.0197 - accuracy: 0.6643 - val_loss: 0.4719 - val_accuracy: 0.8889\n",
      "Epoch 67/210\n",
      "1257/1257 [==============================] - 1s 409us/step - loss: 1.0375 - accuracy: 0.6547 - val_loss: 0.4469 - val_accuracy: 0.8944\n",
      "Epoch 68/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 1.0209 - accuracy: 0.6436 - val_loss: 0.4233 - val_accuracy: 0.9037\n",
      "Epoch 69/210\n",
      "1257/1257 [==============================] - 0s 392us/step - loss: 0.9455 - accuracy: 0.6810 - val_loss: 0.3982 - val_accuracy: 0.9056\n",
      "Epoch 70/210\n",
      "1257/1257 [==============================] - 1s 407us/step - loss: 0.9263 - accuracy: 0.6762 - val_loss: 0.3767 - val_accuracy: 0.9093\n",
      "Epoch 71/210\n",
      "1257/1257 [==============================] - 1s 420us/step - loss: 0.9300 - accuracy: 0.6977 - val_loss: 0.3576 - val_accuracy: 0.9130\n",
      "Epoch 72/210\n",
      "1257/1257 [==============================] - 1s 440us/step - loss: 0.9148 - accuracy: 0.6921 - val_loss: 0.3413 - val_accuracy: 0.9148\n",
      "Epoch 73/210\n",
      "1257/1257 [==============================] - 1s 444us/step - loss: 0.8752 - accuracy: 0.7056 - val_loss: 0.3274 - val_accuracy: 0.9185\n",
      "Epoch 74/210\n",
      "1257/1257 [==============================] - 0s 392us/step - loss: 0.9170 - accuracy: 0.6874 - val_loss: 0.3184 - val_accuracy: 0.9241\n",
      "Epoch 75/210\n",
      "1257/1257 [==============================] - 1s 417us/step - loss: 0.8240 - accuracy: 0.7232 - val_loss: 0.3110 - val_accuracy: 0.9241\n",
      "Epoch 76/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.7837 - accuracy: 0.7542 - val_loss: 0.3005 - val_accuracy: 0.9259\n",
      "Epoch 77/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 0.7724 - accuracy: 0.7414 - val_loss: 0.2919 - val_accuracy: 0.9296\n",
      "Epoch 78/210\n",
      "1257/1257 [==============================] - 0s 336us/step - loss: 0.7895 - accuracy: 0.7295 - val_loss: 0.2840 - val_accuracy: 0.9278\n",
      "Epoch 79/210\n",
      "1257/1257 [==============================] - 0s 338us/step - loss: 0.7584 - accuracy: 0.7438 - val_loss: 0.2764 - val_accuracy: 0.9352\n",
      "Epoch 80/210\n",
      "1257/1257 [==============================] - 0s 335us/step - loss: 0.7515 - accuracy: 0.7526 - val_loss: 0.2684 - val_accuracy: 0.9370\n",
      "Epoch 81/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.7660 - accuracy: 0.7422 - val_loss: 0.2597 - val_accuracy: 0.9370\n",
      "Epoch 82/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 0.6879 - accuracy: 0.7796 - val_loss: 0.2502 - val_accuracy: 0.9333\n",
      "Epoch 83/210\n",
      "1257/1257 [==============================] - 0s 324us/step - loss: 0.7247 - accuracy: 0.7629 - val_loss: 0.2427 - val_accuracy: 0.9333\n",
      "Epoch 84/210\n",
      "1257/1257 [==============================] - 0s 350us/step - loss: 0.7313 - accuracy: 0.7574 - val_loss: 0.2375 - val_accuracy: 0.9333\n",
      "Epoch 85/210\n",
      "1257/1257 [==============================] - 0s 336us/step - loss: 0.6648 - accuracy: 0.7852 - val_loss: 0.2320 - val_accuracy: 0.9333\n",
      "Epoch 86/210\n",
      "1257/1257 [==============================] - 0s 355us/step - loss: 0.6562 - accuracy: 0.7757 - val_loss: 0.2258 - val_accuracy: 0.9352\n",
      "Epoch 87/210\n",
      "1257/1257 [==============================] - 0s 336us/step - loss: 0.6500 - accuracy: 0.7932 - val_loss: 0.2196 - val_accuracy: 0.9389\n",
      "Epoch 88/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.6729 - accuracy: 0.7733 - val_loss: 0.2144 - val_accuracy: 0.9389\n",
      "Epoch 89/210\n",
      "1257/1257 [==============================] - 0s 355us/step - loss: 0.6449 - accuracy: 0.7844 - val_loss: 0.2092 - val_accuracy: 0.9407\n",
      "Epoch 90/210\n",
      "1257/1257 [==============================] - 0s 337us/step - loss: 0.6160 - accuracy: 0.7955 - val_loss: 0.2041 - val_accuracy: 0.9352\n",
      "Epoch 91/210\n",
      "1257/1257 [==============================] - 0s 341us/step - loss: 0.5838 - accuracy: 0.7995 - val_loss: 0.1993 - val_accuracy: 0.9333\n",
      "Epoch 92/210\n",
      "1257/1257 [==============================] - 0s 366us/step - loss: 0.6079 - accuracy: 0.7860 - val_loss: 0.1949 - val_accuracy: 0.9315\n",
      "Epoch 93/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.5472 - accuracy: 0.8234 - val_loss: 0.1899 - val_accuracy: 0.9333\n",
      "Epoch 94/210\n",
      "1257/1257 [==============================] - 0s 382us/step - loss: 0.5535 - accuracy: 0.8178 - val_loss: 0.1852 - val_accuracy: 0.9352\n",
      "Epoch 95/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 0.6404 - accuracy: 0.7844 - val_loss: 0.1812 - val_accuracy: 0.9370\n",
      "Epoch 96/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 0.5604 - accuracy: 0.8146 - val_loss: 0.1781 - val_accuracy: 0.9407\n",
      "Epoch 97/210\n",
      "1257/1257 [==============================] - 0s 346us/step - loss: 0.5306 - accuracy: 0.8218 - val_loss: 0.1754 - val_accuracy: 0.9407\n",
      "Epoch 98/210\n",
      "1257/1257 [==============================] - 0s 341us/step - loss: 0.5444 - accuracy: 0.8162 - val_loss: 0.1727 - val_accuracy: 0.9463\n",
      "Epoch 99/210\n",
      "1257/1257 [==============================] - 0s 376us/step - loss: 0.5342 - accuracy: 0.8282 - val_loss: 0.1698 - val_accuracy: 0.9463\n",
      "Epoch 100/210\n",
      "1257/1257 [==============================] - 0s 361us/step - loss: 0.5492 - accuracy: 0.8290 - val_loss: 0.1674 - val_accuracy: 0.9444\n",
      "Epoch 101/210\n",
      "1257/1257 [==============================] - 1s 412us/step - loss: 0.5498 - accuracy: 0.8154 - val_loss: 0.1654 - val_accuracy: 0.9444\n",
      "Epoch 102/210\n",
      "1257/1257 [==============================] - 1s 399us/step - loss: 0.4906 - accuracy: 0.8417 - val_loss: 0.1634 - val_accuracy: 0.9444\n",
      "Epoch 103/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.4988 - accuracy: 0.8313 - val_loss: 0.1593 - val_accuracy: 0.9463\n",
      "Epoch 104/210\n",
      "1257/1257 [==============================] - 0s 385us/step - loss: 0.4919 - accuracy: 0.8441 - val_loss: 0.1548 - val_accuracy: 0.9500\n",
      "Epoch 105/210\n",
      "1257/1257 [==============================] - 1s 425us/step - loss: 0.5176 - accuracy: 0.8266 - val_loss: 0.1519 - val_accuracy: 0.9556\n",
      "Epoch 106/210\n",
      "1257/1257 [==============================] - 0s 385us/step - loss: 0.4688 - accuracy: 0.8496 - val_loss: 0.1493 - val_accuracy: 0.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/210\n",
      "1257/1257 [==============================] - 0s 393us/step - loss: 0.4806 - accuracy: 0.8353 - val_loss: 0.1474 - val_accuracy: 0.9593\n",
      "Epoch 108/210\n",
      "1257/1257 [==============================] - 0s 388us/step - loss: 0.5174 - accuracy: 0.8393 - val_loss: 0.1459 - val_accuracy: 0.9574\n",
      "Epoch 109/210\n",
      "1257/1257 [==============================] - 1s 418us/step - loss: 0.4870 - accuracy: 0.8481 - val_loss: 0.1446 - val_accuracy: 0.9556\n",
      "Epoch 110/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 0.4392 - accuracy: 0.8504 - val_loss: 0.1432 - val_accuracy: 0.9537\n",
      "Epoch 111/210\n",
      "1257/1257 [==============================] - 0s 375us/step - loss: 0.4644 - accuracy: 0.8536 - val_loss: 0.1420 - val_accuracy: 0.9537\n",
      "Epoch 112/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 0.4647 - accuracy: 0.8560 - val_loss: 0.1396 - val_accuracy: 0.9556\n",
      "Epoch 113/210\n",
      "1257/1257 [==============================] - 0s 369us/step - loss: 0.4491 - accuracy: 0.8457 - val_loss: 0.1372 - val_accuracy: 0.9537\n",
      "Epoch 114/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.4530 - accuracy: 0.8409 - val_loss: 0.1358 - val_accuracy: 0.9556\n",
      "Epoch 115/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.4484 - accuracy: 0.8584 - val_loss: 0.1344 - val_accuracy: 0.9556\n",
      "Epoch 116/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 0.4248 - accuracy: 0.8512 - val_loss: 0.1322 - val_accuracy: 0.9574\n",
      "Epoch 117/210\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 0.4631 - accuracy: 0.8528 - val_loss: 0.1304 - val_accuracy: 0.9556\n",
      "Epoch 118/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.4452 - accuracy: 0.8584 - val_loss: 0.1290 - val_accuracy: 0.9574\n",
      "Epoch 119/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.4438 - accuracy: 0.8560 - val_loss: 0.1277 - val_accuracy: 0.9630\n",
      "Epoch 120/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.4113 - accuracy: 0.8823 - val_loss: 0.1263 - val_accuracy: 0.9648\n",
      "Epoch 121/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 0.3967 - accuracy: 0.8687 - val_loss: 0.1253 - val_accuracy: 0.9667\n",
      "Epoch 122/210\n",
      "1257/1257 [==============================] - 0s 382us/step - loss: 0.4198 - accuracy: 0.8695 - val_loss: 0.1243 - val_accuracy: 0.9648\n",
      "Epoch 123/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.4382 - accuracy: 0.8544 - val_loss: 0.1226 - val_accuracy: 0.9630\n",
      "Epoch 124/210\n",
      "1257/1257 [==============================] - 0s 351us/step - loss: 0.3654 - accuracy: 0.8815 - val_loss: 0.1208 - val_accuracy: 0.9648\n",
      "Epoch 125/210\n",
      "1257/1257 [==============================] - 0s 342us/step - loss: 0.3865 - accuracy: 0.8703 - val_loss: 0.1191 - val_accuracy: 0.9648\n",
      "Epoch 126/210\n",
      "1257/1257 [==============================] - 0s 332us/step - loss: 0.3930 - accuracy: 0.8743 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
      "Epoch 127/210\n",
      "1257/1257 [==============================] - 0s 349us/step - loss: 0.3567 - accuracy: 0.8815 - val_loss: 0.1165 - val_accuracy: 0.9667\n",
      "Epoch 128/210\n",
      "1257/1257 [==============================] - 0s 345us/step - loss: 0.3912 - accuracy: 0.8695 - val_loss: 0.1155 - val_accuracy: 0.9648\n",
      "Epoch 129/210\n",
      "1257/1257 [==============================] - 0s 372us/step - loss: 0.3673 - accuracy: 0.8831 - val_loss: 0.1144 - val_accuracy: 0.9648\n",
      "Epoch 130/210\n",
      "1257/1257 [==============================] - 0s 337us/step - loss: 0.3721 - accuracy: 0.8759 - val_loss: 0.1135 - val_accuracy: 0.9685\n",
      "Epoch 131/210\n",
      "1257/1257 [==============================] - 0s 352us/step - loss: 0.3562 - accuracy: 0.8846 - val_loss: 0.1130 - val_accuracy: 0.9685\n",
      "Epoch 132/210\n",
      "1257/1257 [==============================] - 0s 345us/step - loss: 0.3957 - accuracy: 0.8608 - val_loss: 0.1121 - val_accuracy: 0.9667\n",
      "Epoch 133/210\n",
      "1257/1257 [==============================] - 0s 343us/step - loss: 0.3630 - accuracy: 0.8751 - val_loss: 0.1108 - val_accuracy: 0.9685\n",
      "Epoch 134/210\n",
      "1257/1257 [==============================] - 0s 366us/step - loss: 0.3572 - accuracy: 0.8823 - val_loss: 0.1086 - val_accuracy: 0.9704\n",
      "Epoch 135/210\n",
      "1257/1257 [==============================] - 1s 410us/step - loss: 0.3575 - accuracy: 0.8815 - val_loss: 0.1067 - val_accuracy: 0.9722\n",
      "Epoch 136/210\n",
      "1257/1257 [==============================] - 0s 394us/step - loss: 0.3766 - accuracy: 0.8703 - val_loss: 0.1054 - val_accuracy: 0.9741\n",
      "Epoch 137/210\n",
      "1257/1257 [==============================] - 0s 384us/step - loss: 0.3503 - accuracy: 0.8799 - val_loss: 0.1048 - val_accuracy: 0.9741\n",
      "Epoch 138/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 0.3503 - accuracy: 0.8934 - val_loss: 0.1045 - val_accuracy: 0.9741\n",
      "Epoch 139/210\n",
      "1257/1257 [==============================] - 1s 398us/step - loss: 0.3246 - accuracy: 0.8982 - val_loss: 0.1038 - val_accuracy: 0.9759\n",
      "Epoch 140/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.3222 - accuracy: 0.8878 - val_loss: 0.1028 - val_accuracy: 0.9759\n",
      "Epoch 141/210\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.3438 - accuracy: 0.9014 - val_loss: 0.1021 - val_accuracy: 0.9722\n",
      "Epoch 142/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 0.3409 - accuracy: 0.8982 - val_loss: 0.1024 - val_accuracy: 0.9704\n",
      "Epoch 143/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 0.3469 - accuracy: 0.8902 - val_loss: 0.1026 - val_accuracy: 0.9704\n",
      "Epoch 144/210\n",
      "1257/1257 [==============================] - 0s 391us/step - loss: 0.3640 - accuracy: 0.8831 - val_loss: 0.1018 - val_accuracy: 0.9704\n",
      "Epoch 145/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 0.3299 - accuracy: 0.8862 - val_loss: 0.1002 - val_accuracy: 0.9704\n",
      "Epoch 146/210\n",
      "1257/1257 [==============================] - 0s 355us/step - loss: 0.3330 - accuracy: 0.8942 - val_loss: 0.0984 - val_accuracy: 0.9741\n",
      "Epoch 147/210\n",
      "1257/1257 [==============================] - 0s 325us/step - loss: 0.3228 - accuracy: 0.8870 - val_loss: 0.0970 - val_accuracy: 0.9741\n",
      "Epoch 148/210\n",
      "1257/1257 [==============================] - 0s 342us/step - loss: 0.3176 - accuracy: 0.8918 - val_loss: 0.0957 - val_accuracy: 0.9741\n",
      "Epoch 149/210\n",
      "1257/1257 [==============================] - 0s 345us/step - loss: 0.3017 - accuracy: 0.8918 - val_loss: 0.0950 - val_accuracy: 0.9741\n",
      "Epoch 150/210\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 0.3093 - accuracy: 0.8950 - val_loss: 0.0944 - val_accuracy: 0.9741\n",
      "Epoch 151/210\n",
      "1257/1257 [==============================] - 0s 342us/step - loss: 0.3216 - accuracy: 0.9069 - val_loss: 0.0939 - val_accuracy: 0.9741\n",
      "Epoch 152/210\n",
      "1257/1257 [==============================] - 0s 376us/step - loss: 0.3510 - accuracy: 0.8878 - val_loss: 0.0933 - val_accuracy: 0.9741\n",
      "Epoch 153/210\n",
      "1257/1257 [==============================] - 0s 345us/step - loss: 0.2971 - accuracy: 0.9014 - val_loss: 0.0926 - val_accuracy: 0.9722\n",
      "Epoch 154/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 0.3354 - accuracy: 0.8910 - val_loss: 0.0920 - val_accuracy: 0.9722\n",
      "Epoch 155/210\n",
      "1257/1257 [==============================] - 0s 367us/step - loss: 0.3275 - accuracy: 0.8870 - val_loss: 0.0913 - val_accuracy: 0.9741\n",
      "Epoch 156/210\n",
      "1257/1257 [==============================] - 0s 338us/step - loss: 0.2667 - accuracy: 0.9212 - val_loss: 0.0905 - val_accuracy: 0.9741\n",
      "Epoch 157/210\n",
      "1257/1257 [==============================] - 0s 362us/step - loss: 0.2914 - accuracy: 0.8974 - val_loss: 0.0892 - val_accuracy: 0.9741\n",
      "Epoch 158/210\n",
      "1257/1257 [==============================] - 0s 334us/step - loss: 0.3369 - accuracy: 0.8902 - val_loss: 0.0880 - val_accuracy: 0.9741\n",
      "Epoch 159/210\n",
      "1257/1257 [==============================] - 0s 348us/step - loss: 0.2939 - accuracy: 0.9029 - val_loss: 0.0867 - val_accuracy: 0.9741\n",
      "Epoch 160/210\n",
      "1257/1257 [==============================] - 0s 328us/step - loss: 0.2947 - accuracy: 0.9029 - val_loss: 0.0858 - val_accuracy: 0.9741\n",
      "Epoch 161/210\n",
      "1257/1257 [==============================] - 0s 368us/step - loss: 0.3193 - accuracy: 0.8870 - val_loss: 0.0852 - val_accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 0.3054 - accuracy: 0.8966 - val_loss: 0.0846 - val_accuracy: 0.9741\n",
      "Epoch 163/210\n",
      "1257/1257 [==============================] - 0s 371us/step - loss: 0.2952 - accuracy: 0.8958 - val_loss: 0.0836 - val_accuracy: 0.9741\n",
      "Epoch 164/210\n",
      "1257/1257 [==============================] - 0s 358us/step - loss: 0.3003 - accuracy: 0.9061 - val_loss: 0.0827 - val_accuracy: 0.9759\n",
      "Epoch 165/210\n",
      "1257/1257 [==============================] - 1s 399us/step - loss: 0.2908 - accuracy: 0.9069 - val_loss: 0.0820 - val_accuracy: 0.9759\n",
      "Epoch 166/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 0.2589 - accuracy: 0.9181 - val_loss: 0.0812 - val_accuracy: 0.9759\n",
      "Epoch 167/210\n",
      "1257/1257 [==============================] - 0s 342us/step - loss: 0.2852 - accuracy: 0.9109 - val_loss: 0.0806 - val_accuracy: 0.9722\n",
      "Epoch 168/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 0.2955 - accuracy: 0.9053 - val_loss: 0.0798 - val_accuracy: 0.9741\n",
      "Epoch 169/210\n",
      "1257/1257 [==============================] - 1s 408us/step - loss: 0.3127 - accuracy: 0.8982 - val_loss: 0.0790 - val_accuracy: 0.9759\n",
      "Epoch 170/210\n",
      "1257/1257 [==============================] - 0s 393us/step - loss: 0.2745 - accuracy: 0.9069 - val_loss: 0.0785 - val_accuracy: 0.9759\n",
      "Epoch 171/210\n",
      "1257/1257 [==============================] - 0s 389us/step - loss: 0.2893 - accuracy: 0.9029 - val_loss: 0.0779 - val_accuracy: 0.9759\n",
      "Epoch 172/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 0.2661 - accuracy: 0.9133 - val_loss: 0.0775 - val_accuracy: 0.9796\n",
      "Epoch 173/210\n",
      "1257/1257 [==============================] - 0s 396us/step - loss: 0.2998 - accuracy: 0.9021 - val_loss: 0.0771 - val_accuracy: 0.9796\n",
      "Epoch 174/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.2967 - accuracy: 0.9093 - val_loss: 0.0768 - val_accuracy: 0.9778\n",
      "Epoch 175/210\n",
      "1257/1257 [==============================] - 1s 406us/step - loss: 0.3321 - accuracy: 0.8894 - val_loss: 0.0766 - val_accuracy: 0.9778\n",
      "Epoch 176/210\n",
      "1257/1257 [==============================] - 1s 472us/step - loss: 0.2476 - accuracy: 0.9244 - val_loss: 0.0764 - val_accuracy: 0.9759\n",
      "Epoch 177/210\n",
      "1257/1257 [==============================] - 0s 383us/step - loss: 0.2550 - accuracy: 0.9204 - val_loss: 0.0761 - val_accuracy: 0.9778\n",
      "Epoch 178/210\n",
      "1257/1257 [==============================] - 1s 410us/step - loss: 0.2977 - accuracy: 0.9021 - val_loss: 0.0759 - val_accuracy: 0.9778\n",
      "Epoch 179/210\n",
      "1257/1257 [==============================] - 0s 345us/step - loss: 0.2887 - accuracy: 0.9029 - val_loss: 0.0758 - val_accuracy: 0.9778\n",
      "Epoch 180/210\n",
      "1257/1257 [==============================] - 0s 378us/step - loss: 0.3061 - accuracy: 0.9045 - val_loss: 0.0759 - val_accuracy: 0.9778\n",
      "Epoch 181/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 0.2684 - accuracy: 0.9061 - val_loss: 0.0758 - val_accuracy: 0.9778\n",
      "Epoch 182/210\n",
      "1257/1257 [==============================] - 0s 374us/step - loss: 0.2479 - accuracy: 0.9236 - val_loss: 0.0758 - val_accuracy: 0.9778\n",
      "Epoch 183/210\n",
      "1257/1257 [==============================] - 0s 344us/step - loss: 0.2564 - accuracy: 0.9109 - val_loss: 0.0755 - val_accuracy: 0.9796\n",
      "Epoch 184/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 0.2563 - accuracy: 0.9141 - val_loss: 0.0751 - val_accuracy: 0.9778\n",
      "Epoch 185/210\n",
      "1257/1257 [==============================] - 0s 341us/step - loss: 0.2568 - accuracy: 0.9196 - val_loss: 0.0752 - val_accuracy: 0.9778\n",
      "Epoch 186/210\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 0.2551 - accuracy: 0.9189 - val_loss: 0.0752 - val_accuracy: 0.9796\n",
      "Epoch 187/210\n",
      "1257/1257 [==============================] - 0s 366us/step - loss: 0.2926 - accuracy: 0.9157 - val_loss: 0.0749 - val_accuracy: 0.9796\n",
      "Epoch 188/210\n",
      "1257/1257 [==============================] - 1s 399us/step - loss: 0.2486 - accuracy: 0.9196 - val_loss: 0.0745 - val_accuracy: 0.9796\n",
      "Epoch 189/210\n",
      "1257/1257 [==============================] - 0s 344us/step - loss: 0.2641 - accuracy: 0.9061 - val_loss: 0.0738 - val_accuracy: 0.9778\n",
      "Epoch 190/210\n",
      "1257/1257 [==============================] - 0s 360us/step - loss: 0.2836 - accuracy: 0.9053 - val_loss: 0.0733 - val_accuracy: 0.9778\n",
      "Epoch 191/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 0.2453 - accuracy: 0.9189 - val_loss: 0.0727 - val_accuracy: 0.9759\n",
      "Epoch 192/210\n",
      "1257/1257 [==============================] - 0s 363us/step - loss: 0.2818 - accuracy: 0.9077 - val_loss: 0.0725 - val_accuracy: 0.9778\n",
      "Epoch 193/210\n",
      "1257/1257 [==============================] - 0s 345us/step - loss: 0.2474 - accuracy: 0.9141 - val_loss: 0.0720 - val_accuracy: 0.9778\n",
      "Epoch 194/210\n",
      "1257/1257 [==============================] - 0s 365us/step - loss: 0.2240 - accuracy: 0.9260 - val_loss: 0.0718 - val_accuracy: 0.9759\n",
      "Epoch 195/210\n",
      "1257/1257 [==============================] - 0s 358us/step - loss: 0.2591 - accuracy: 0.9252 - val_loss: 0.0717 - val_accuracy: 0.9778\n",
      "Epoch 196/210\n",
      "1257/1257 [==============================] - 0s 353us/step - loss: 0.2383 - accuracy: 0.9181 - val_loss: 0.0714 - val_accuracy: 0.9815\n",
      "Epoch 197/210\n",
      "1257/1257 [==============================] - 0s 359us/step - loss: 0.2550 - accuracy: 0.9173 - val_loss: 0.0709 - val_accuracy: 0.9815\n",
      "Epoch 198/210\n",
      "1257/1257 [==============================] - 0s 354us/step - loss: 0.2520 - accuracy: 0.9069 - val_loss: 0.0701 - val_accuracy: 0.9815\n",
      "Epoch 199/210\n",
      "1257/1257 [==============================] - 0s 358us/step - loss: 0.2502 - accuracy: 0.9236 - val_loss: 0.0696 - val_accuracy: 0.9815\n",
      "Epoch 200/210\n",
      "1257/1257 [==============================] - 0s 372us/step - loss: 0.2441 - accuracy: 0.9196 - val_loss: 0.0697 - val_accuracy: 0.9796\n",
      "Epoch 201/210\n",
      "1257/1257 [==============================] - 0s 349us/step - loss: 0.2266 - accuracy: 0.9236 - val_loss: 0.0692 - val_accuracy: 0.9796\n",
      "Epoch 202/210\n",
      "1257/1257 [==============================] - 0s 373us/step - loss: 0.2431 - accuracy: 0.9077 - val_loss: 0.0688 - val_accuracy: 0.9796\n",
      "Epoch 203/210\n",
      "1257/1257 [==============================] - 0s 394us/step - loss: 0.2574 - accuracy: 0.9101 - val_loss: 0.0683 - val_accuracy: 0.9796\n",
      "Epoch 204/210\n",
      "1257/1257 [==============================] - 1s 408us/step - loss: 0.2432 - accuracy: 0.9149 - val_loss: 0.0680 - val_accuracy: 0.9796\n",
      "Epoch 205/210\n",
      "1257/1257 [==============================] - 0s 397us/step - loss: 0.2404 - accuracy: 0.9133 - val_loss: 0.0681 - val_accuracy: 0.9796\n",
      "Epoch 206/210\n",
      "1257/1257 [==============================] - 1s 406us/step - loss: 0.2181 - accuracy: 0.9268 - val_loss: 0.0676 - val_accuracy: 0.9833\n",
      "Epoch 207/210\n",
      "1257/1257 [==============================] - 1s 421us/step - loss: 0.2399 - accuracy: 0.9189 - val_loss: 0.0662 - val_accuracy: 0.9833\n",
      "Epoch 208/210\n",
      "1257/1257 [==============================] - 0s 396us/step - loss: 0.2257 - accuracy: 0.9244 - val_loss: 0.0651 - val_accuracy: 0.9852\n",
      "Epoch 209/210\n",
      "1257/1257 [==============================] - 1s 414us/step - loss: 0.2437 - accuracy: 0.9252 - val_loss: 0.0650 - val_accuracy: 0.9833\n",
      "Epoch 210/210\n",
      "1257/1257 [==============================] - 1s 404us/step - loss: 0.2148 - accuracy: 0.9324 - val_loss: 0.0658 - val_accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# Parametres\n",
    "verbose, epochs, batch_size = 1, 210, 1750\n",
    "# initialize the model object\n",
    "clf_cnn = CNN_net()\n",
    "# fit network\n",
    "history = clf_cnn.fit(x_train, y_train_binary, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=verbose, validation_data=(x_test, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 [==============================] - 0s 116us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06584806663018686, 0.9814814925193787]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call predict to get predictions Report the accuracy\n",
    "clf_cnn.evaluate(x_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN : Accuracy: 0.9740740740740741\n"
     ]
    }
   ],
   "source": [
    "# call predict to get predictions\n",
    "y_pred = clf_cnn.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Report the accuracy\n",
    "accuracy_CNN = accuracy_score(y_test_binary, y_pred)\n",
    "print(\"CNN : Accuracy: \" + str(accuracy_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 62, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 60, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 28, 64)            6208      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 26, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 832)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                53312     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 75,754\n",
      "Trainable params: 75,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss','Validation loss'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training acc','Validation acc'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# from the first Fully-Connected layer \n",
    "layer_name = 'dense_1'\n",
    "intermediate_layer_model = Model(inputs=clf_cnn.input,\n",
    "                                 outputs=clf_cnn.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features of the train dataset to use it in future.\n",
    "out_cnn_train = intermediate_layer_model.predict(x_train)\n",
    "# Save the features of the test dataset to use it in future.\n",
    "out_cnn_test = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (from CNN) Shape: (1257, 64)\n",
      "Training Labels (from CNN) Shape: (1257,)\n",
      "Test Features (from CNN) Shape: (540, 64)\n",
      "Test Labels (from CNN) Shape: (540,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features (from CNN) Shape:', out_cnn_train.shape)\n",
    "print('Training Labels (from CNN) Shape:', y_train.shape)\n",
    "\n",
    "print('Test Features (from CNN) Shape:', out_cnn_test.shape)\n",
    "print('Test Labels (from CNN) Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + Random Forest + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djinn iris\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:262: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:263: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:266: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:302: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:286: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:320: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:328: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:333: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:334: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:335: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch: 0001 cost= 2.311291877 accuracy= 0.230\n",
      "Epoch: 0002 cost= 2.217436019 accuracy= 0.217\n",
      "Epoch: 0003 cost= 1.937738229 accuracy= 0.206\n",
      "Epoch: 0004 cost= 1.745113148 accuracy= 0.318\n",
      "Epoch: 0005 cost= 1.553093111 accuracy= 0.376\n",
      "Epoch: 0006 cost= 1.314202568 accuracy= 0.599\n",
      "Epoch: 0007 cost= 1.094884269 accuracy= 0.650\n",
      "Epoch: 0008 cost= 0.854238812 accuracy= 0.714\n",
      "Epoch: 0009 cost= 0.760141506 accuracy= 0.688\n",
      "Epoch: 0010 cost= 0.632661518 accuracy= 0.785\n",
      "Epoch: 0011 cost= 0.704604073 accuracy= 0.767\n",
      "Epoch: 0012 cost= 0.545114917 accuracy= 0.810\n",
      "Epoch: 0013 cost= 0.568049697 accuracy= 0.827\n",
      "Epoch: 0014 cost= 0.557421369 accuracy= 0.827\n",
      "Epoch: 0015 cost= 0.552245634 accuracy= 0.831\n",
      "Epoch: 0016 cost= 0.471545349 accuracy= 0.856\n",
      "Epoch: 0017 cost= 0.439255593 accuracy= 0.876\n",
      "Epoch: 0018 cost= 0.325758853 accuracy= 0.866\n",
      "Epoch: 0019 cost= 0.353504021 accuracy= 0.874\n",
      "Epoch: 0020 cost= 0.330207422 accuracy= 0.894\n",
      "Epoch: 0021 cost= 0.312709795 accuracy= 0.903\n",
      "Epoch: 0022 cost= 0.264975090 accuracy= 0.920\n",
      "Epoch: 0023 cost= 0.203992525 accuracy= 0.943\n",
      "Epoch: 0024 cost= 0.167763703 accuracy= 0.951\n",
      "Epoch: 0025 cost= 0.153740847 accuracy= 0.949\n",
      "Epoch: 0026 cost= 0.149287390 accuracy= 0.955\n",
      "Epoch: 0027 cost= 0.127324232 accuracy= 0.958\n",
      "Epoch: 0028 cost= 0.116844674 accuracy= 0.963\n",
      "Epoch: 0029 cost= 0.096876435 accuracy= 0.973\n",
      "Epoch: 0030 cost= 0.098363331 accuracy= 0.971\n",
      "Epoch: 0031 cost= 0.094827737 accuracy= 0.966\n",
      "Epoch: 0032 cost= 0.069614988 accuracy= 0.977\n",
      "Epoch: 0033 cost= 0.076286784 accuracy= 0.974\n",
      "Epoch: 0034 cost= 0.074812136 accuracy= 0.978\n",
      "Epoch: 0035 cost= 0.071518959 accuracy= 0.978\n",
      "Epoch: 0036 cost= 0.065838915 accuracy= 0.974\n",
      "Epoch: 0037 cost= 0.055277225 accuracy= 0.979\n",
      "Epoch: 0038 cost= 0.060057015 accuracy= 0.981\n",
      "Epoch: 0039 cost= 0.077116712 accuracy= 0.982\n",
      "Epoch: 0040 cost= 0.051024108 accuracy= 0.984\n",
      "Epoch: 0041 cost= 0.051962730 accuracy= 0.973\n",
      "Epoch: 0042 cost= 0.068046495 accuracy= 0.977\n",
      "Epoch: 0043 cost= 0.065749547 accuracy= 0.980\n",
      "Epoch: 0044 cost= 0.061521864 accuracy= 0.981\n",
      "Epoch: 0045 cost= 0.044741359 accuracy= 0.989\n",
      "Epoch: 0046 cost= 0.035344519 accuracy= 0.987\n",
      "Epoch: 0047 cost= 0.053149747 accuracy= 0.983\n",
      "Epoch: 0048 cost= 0.066354805 accuracy= 0.975\n",
      "Epoch: 0049 cost= 0.051369051 accuracy= 0.985\n",
      "Epoch: 0050 cost= 0.060733950 accuracy= 0.987\n",
      "Epoch: 0051 cost= 0.052115269 accuracy= 0.989\n",
      "Epoch: 0052 cost= 0.028792388 accuracy= 0.990\n",
      "Epoch: 0053 cost= 0.035506598 accuracy= 0.983\n",
      "Epoch: 0054 cost= 0.041145366 accuracy= 0.989\n",
      "Epoch: 0055 cost= 0.035081993 accuracy= 0.986\n",
      "Epoch: 0056 cost= 0.049583532 accuracy= 0.981\n",
      "Epoch: 0057 cost= 0.041107798 accuracy= 0.984\n",
      "Epoch: 0058 cost= 0.058606871 accuracy= 0.989\n",
      "Epoch: 0059 cost= 0.033610083 accuracy= 0.992\n",
      "Epoch: 0060 cost= 0.032922598 accuracy= 0.981\n",
      "Epoch: 0061 cost= 0.037349872 accuracy= 0.985\n",
      "Epoch: 0062 cost= 0.052658034 accuracy= 0.989\n",
      "Epoch: 0063 cost= 0.048432182 accuracy= 0.988\n",
      "Epoch: 0064 cost= 0.039277755 accuracy= 0.986\n",
      "Epoch: 0065 cost= 0.042601243 accuracy= 0.987\n",
      "Epoch: 0066 cost= 0.030045017 accuracy= 0.995\n",
      "Epoch: 0067 cost= 0.033594410 accuracy= 0.993\n",
      "Epoch: 0068 cost= 0.020900298 accuracy= 0.996\n",
      "Epoch: 0069 cost= 0.017782949 accuracy= 0.994\n",
      "Epoch: 0070 cost= 0.018196699 accuracy= 0.993\n",
      "Epoch: 0071 cost= 0.022879730 accuracy= 0.996\n",
      "Epoch: 0072 cost= 0.012460100 accuracy= 0.995\n",
      "Epoch: 0073 cost= 0.016572008 accuracy= 0.994\n",
      "Epoch: 0074 cost= 0.018413864 accuracy= 0.984\n",
      "Epoch: 0075 cost= 0.033627805 accuracy= 0.990\n",
      "Epoch: 0076 cost= 0.026008913 accuracy= 0.992\n",
      "Epoch: 0077 cost= 0.022355161 accuracy= 0.990\n",
      "Epoch: 0078 cost= 0.027673487 accuracy= 0.989\n",
      "Epoch: 0079 cost= 0.033661460 accuracy= 0.981\n",
      "Epoch: 0080 cost= 0.063328051 accuracy= 0.973\n",
      "Epoch: 0081 cost= 0.080977836 accuracy= 0.987\n",
      "Epoch: 0082 cost= 0.040371704 accuracy= 0.990\n",
      "Epoch: 0083 cost= 0.027804602 accuracy= 0.985\n",
      "Epoch: 0084 cost= 0.019626896 accuracy= 0.994\n",
      "Epoch: 0085 cost= 0.042584889 accuracy= 0.990\n",
      "Epoch: 0086 cost= 0.036972213 accuracy= 0.982\n",
      "Epoch: 0087 cost= 0.032514534 accuracy= 0.990\n",
      "Epoch: 0088 cost= 0.033249368 accuracy= 0.990\n",
      "Epoch: 0089 cost= 0.035821090 accuracy= 0.989\n",
      "Epoch: 0090 cost= 0.047067055 accuracy= 0.976\n",
      "Epoch: 0091 cost= 0.031424959 accuracy= 0.987\n",
      "Epoch: 0092 cost= 0.022863092 accuracy= 0.975\n",
      "Epoch: 0093 cost= 0.040218867 accuracy= 0.982\n",
      "Epoch: 0094 cost= 0.013264779 accuracy= 0.992\n",
      "Epoch: 0095 cost= 0.014539503 accuracy= 0.994\n",
      "Epoch: 0096 cost= 0.018991117 accuracy= 0.996\n",
      "Epoch: 0097 cost= 0.019719190 accuracy= 0.992\n",
      "Epoch: 0098 cost= 0.025355755 accuracy= 0.993\n",
      "Epoch: 0099 cost= 0.028609989 accuracy= 0.989\n",
      "Epoch: 0100 cost= 0.020964290 accuracy= 0.992\n",
      "Epoch: 0101 cost= 0.016881150 accuracy= 0.988\n",
      "Epoch: 0102 cost= 0.018695496 accuracy= 0.996\n",
      "Epoch: 0103 cost= 0.023354991 accuracy= 0.994\n",
      "Epoch: 0104 cost= 0.007889504 accuracy= 0.992\n",
      "Epoch: 0105 cost= 0.027698546 accuracy= 0.989\n",
      "Epoch: 0106 cost= 0.034278251 accuracy= 0.986\n",
      "Epoch: 0107 cost= 0.024699792 accuracy= 0.996\n",
      "Epoch: 0108 cost= 0.024354276 accuracy= 0.989\n",
      "Epoch: 0109 cost= 0.011135204 accuracy= 0.995\n",
      "Epoch: 0110 cost= 0.015309104 accuracy= 0.996\n",
      "Epoch: 0111 cost= 0.011132557 accuracy= 0.994\n",
      "Epoch: 0112 cost= 0.012736450 accuracy= 0.996\n",
      "Epoch: 0113 cost= 0.019032019 accuracy= 0.996\n",
      "Epoch: 0114 cost= 0.012435089 accuracy= 0.997\n",
      "Epoch: 0115 cost= 0.012249336 accuracy= 0.998\n",
      "Epoch: 0116 cost= 0.025661876 accuracy= 0.994\n",
      "Epoch: 0117 cost= 0.010107172 accuracy= 0.996\n",
      "Epoch: 0118 cost= 0.025951216 accuracy= 0.996\n",
      "Epoch: 0119 cost= 0.027104812 accuracy= 0.995\n",
      "Epoch: 0120 cost= 0.011509483 accuracy= 0.992\n",
      "Epoch: 0121 cost= 0.030549568 accuracy= 0.996\n",
      "Epoch: 0122 cost= 0.012801407 accuracy= 0.998\n",
      "Epoch: 0123 cost= 0.007283350 accuracy= 0.996\n",
      "Epoch: 0124 cost= 0.006745547 accuracy= 0.998\n",
      "Epoch: 0125 cost= 0.013990588 accuracy= 0.999\n",
      "Epoch: 0126 cost= 0.006529806 accuracy= 0.996\n",
      "Epoch: 0127 cost= 0.010163607 accuracy= 0.999\n",
      "Epoch: 0128 cost= 0.009210853 accuracy= 0.999\n",
      "Epoch: 0129 cost= 0.011794605 accuracy= 0.996\n",
      "Epoch: 0130 cost= 0.004841893 accuracy= 0.998\n",
      "Epoch: 0131 cost= 0.008188264 accuracy= 0.999\n",
      "Epoch: 0132 cost= 0.003933171 accuracy= 0.999\n",
      "Epoch: 0133 cost= 0.002118120 accuracy= 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0134 cost= 0.011052110 accuracy= 0.999\n",
      "Epoch: 0135 cost= 0.010226996 accuracy= 0.997\n",
      "Epoch: 0136 cost= 0.007809873 accuracy= 0.999\n",
      "Epoch: 0137 cost= 0.004804355 accuracy= 0.999\n",
      "Epoch: 0138 cost= 0.007631208 accuracy= 0.999\n",
      "Epoch: 0139 cost= 0.006648646 accuracy= 0.996\n",
      "Epoch: 0140 cost= 0.002612055 accuracy= 0.996\n",
      "Epoch: 0141 cost= 0.009703147 accuracy= 0.993\n",
      "Epoch: 0142 cost= 0.004414251 accuracy= 0.998\n",
      "Epoch: 0143 cost= 0.006496818 accuracy= 0.998\n",
      "Epoch: 0144 cost= 0.004296198 accuracy= 0.999\n",
      "Epoch: 0145 cost= 0.011147335 accuracy= 0.998\n",
      "Epoch: 0146 cost= 0.025761138 accuracy= 0.977\n",
      "Epoch: 0147 cost= 0.034474484 accuracy= 0.985\n",
      "Epoch: 0148 cost= 0.038099997 accuracy= 0.994\n",
      "Epoch: 0149 cost= 0.051228153 accuracy= 0.988\n",
      "Epoch: 0150 cost= 0.064533675 accuracy= 0.974\n",
      "Epoch: 0151 cost= 0.035662121 accuracy= 0.993\n",
      "Epoch: 0152 cost= 0.044009485 accuracy= 0.973\n",
      "Epoch: 0153 cost= 0.127612423 accuracy= 0.950\n",
      "Epoch: 0154 cost= 0.112359513 accuracy= 0.958\n",
      "Epoch: 0155 cost= 0.066308771 accuracy= 0.986\n",
      "Epoch: 0156 cost= 0.042932125 accuracy= 0.990\n",
      "Epoch: 0157 cost= 0.025188486 accuracy= 0.990\n",
      "Epoch: 0158 cost= 0.011055377 accuracy= 0.998\n",
      "Epoch: 0159 cost= 0.008485403 accuracy= 0.998\n",
      "Epoch: 0160 cost= 0.005539840 accuracy= 0.997\n",
      "Epoch: 0161 cost= 0.006753645 accuracy= 0.999\n",
      "Epoch: 0162 cost= 0.007896218 accuracy= 0.999\n",
      "Epoch: 0163 cost= 0.006435114 accuracy= 0.997\n",
      "Epoch: 0164 cost= 0.010234253 accuracy= 0.998\n",
      "Epoch: 0165 cost= 0.007620291 accuracy= 0.999\n",
      "Epoch: 0166 cost= 0.005486691 accuracy= 1.000\n",
      "Epoch: 0167 cost= 0.004184683 accuracy= 0.999\n",
      "Epoch: 0168 cost= 0.004775833 accuracy= 0.999\n",
      "Epoch: 0169 cost= 0.002127606 accuracy= 0.999\n",
      "Epoch: 0170 cost= 0.003269525 accuracy= 0.998\n",
      "Epoch: 0171 cost= 0.004126250 accuracy= 0.999\n",
      "Epoch: 0172 cost= 0.006329569 accuracy= 0.999\n",
      "Epoch: 0173 cost= 0.002801360 accuracy= 0.998\n",
      "Epoch: 0174 cost= 0.005640009 accuracy= 0.999\n",
      "Epoch: 0175 cost= 0.005179875 accuracy= 0.999\n",
      "Epoch: 0176 cost= 0.002404520 accuracy= 0.995\n",
      "Epoch: 0177 cost= 0.004923364 accuracy= 0.999\n",
      "Epoch: 0178 cost= 0.004619117 accuracy= 1.000\n",
      "Epoch: 0179 cost= 0.004394115 accuracy= 1.000\n",
      "Epoch: 0180 cost= 0.002761297 accuracy= 1.000\n",
      "Epoch: 0181 cost= 0.002365589 accuracy= 1.000\n",
      "Epoch: 0182 cost= 0.002914444 accuracy= 0.999\n",
      "Epoch: 0183 cost= 0.001911039 accuracy= 1.000\n",
      "Epoch: 0184 cost= 0.001107039 accuracy= 0.999\n",
      "Epoch: 0185 cost= 0.000526588 accuracy= 0.999\n",
      "Epoch: 0186 cost= 0.000974173 accuracy= 0.999\n",
      "Epoch: 0187 cost= 0.001120775 accuracy= 0.999\n",
      "Epoch: 0188 cost= 0.000468665 accuracy= 0.999\n",
      "Epoch: 0189 cost= 0.005503965 accuracy= 1.000\n",
      "Epoch: 0190 cost= 0.001585350 accuracy= 0.999\n",
      "Epoch: 0191 cost= 0.001029638 accuracy= 1.000\n",
      "Epoch: 0192 cost= 0.001348939 accuracy= 0.999\n",
      "Epoch: 0193 cost= 0.002189759 accuracy= 1.000\n",
      "Epoch: 0194 cost= 0.000924689 accuracy= 1.000\n",
      "Epoch: 0195 cost= 0.001299055 accuracy= 0.999\n",
      "Epoch: 0196 cost= 0.003849006 accuracy= 0.996\n",
      "Epoch: 0197 cost= 0.005303241 accuracy= 1.000\n",
      "Epoch: 0198 cost= 0.008681376 accuracy= 0.999\n",
      "Epoch: 0199 cost= 0.002458587 accuracy= 0.999\n",
      "Epoch: 0200 cost= 0.000822750 accuracy= 0.998\n",
      "Epoch: 0201 cost= 0.004716357 accuracy= 1.000\n",
      "Epoch: 0202 cost= 0.002947633 accuracy= 1.000\n",
      "Epoch: 0203 cost= 0.002031117 accuracy= 0.999\n",
      "Epoch: 0204 cost= 0.003146650 accuracy= 1.000\n",
      "Epoch: 0205 cost= 0.002135229 accuracy= 1.000\n",
      "Epoch: 0206 cost= 0.001383227 accuracy= 1.000\n",
      "Epoch: 0207 cost= 0.000940049 accuracy= 1.000\n",
      "Epoch: 0208 cost= 0.000907491 accuracy= 1.000\n",
      "Epoch: 0209 cost= 0.000790381 accuracy= 1.000\n",
      "Epoch: 0210 cost= 0.001654207 accuracy= 1.000\n",
      "Epoch: 0211 cost= 0.000959307 accuracy= 1.000\n",
      "Epoch: 0212 cost= 0.000745992 accuracy= 1.000\n",
      "Epoch: 0213 cost= 0.000635142 accuracy= 0.999\n",
      "Epoch: 0214 cost= 0.001740605 accuracy= 1.000\n",
      "Epoch: 0215 cost= 0.001005256 accuracy= 1.000\n",
      "Epoch: 0216 cost= 0.000777595 accuracy= 1.000\n",
      "Epoch: 0217 cost= 0.000554447 accuracy= 1.000\n",
      "Epoch: 0218 cost= 0.000538154 accuracy= 1.000\n",
      "Epoch: 0219 cost= 0.001346833 accuracy= 1.000\n",
      "Epoch: 0220 cost= 0.000899126 accuracy= 1.000\n",
      "Epoch: 0221 cost= 0.000452325 accuracy= 1.000\n",
      "Epoch: 0222 cost= 0.000373834 accuracy= 1.000\n",
      "Epoch: 0223 cost= 0.000870317 accuracy= 1.000\n",
      "Epoch: 0224 cost= 0.000411634 accuracy= 1.000\n",
      "Epoch: 0225 cost= 0.000532265 accuracy= 1.000\n",
      "Epoch: 0226 cost= 0.000512604 accuracy= 1.000\n",
      "Epoch: 0227 cost= 0.000728938 accuracy= 1.000\n",
      "Epoch: 0228 cost= 0.000770176 accuracy= 1.000\n",
      "Epoch: 0229 cost= 0.000444185 accuracy= 1.000\n",
      "Epoch: 0230 cost= 0.000379144 accuracy= 1.000\n",
      "Epoch: 0231 cost= 0.001292324 accuracy= 1.000\n",
      "Epoch: 0232 cost= 0.001102951 accuracy= 1.000\n",
      "Epoch: 0233 cost= 0.000364736 accuracy= 1.000\n",
      "Epoch: 0234 cost= 0.000533351 accuracy= 1.000\n",
      "Epoch: 0235 cost= 0.000405076 accuracy= 1.000\n",
      "Epoch: 0236 cost= 0.000296575 accuracy= 1.000\n",
      "Epoch: 0237 cost= 0.000355017 accuracy= 1.000\n",
      "Epoch: 0238 cost= 0.000443229 accuracy= 1.000\n",
      "Epoch: 0239 cost= 0.000315014 accuracy= 1.000\n",
      "Epoch: 0240 cost= 0.000633092 accuracy= 1.000\n",
      "Epoch: 0241 cost= 0.001121793 accuracy= 1.000\n",
      "Epoch: 0242 cost= 0.000460386 accuracy= 1.000\n",
      "Epoch: 0243 cost= 0.000353282 accuracy= 1.000\n",
      "Epoch: 0244 cost= 0.000470558 accuracy= 1.000\n",
      "Epoch: 0245 cost= 0.000711939 accuracy= 1.000\n",
      "Epoch: 0246 cost= 0.000245677 accuracy= 1.000\n",
      "Epoch: 0247 cost= 0.000381831 accuracy= 1.000\n",
      "Epoch: 0248 cost= 0.000294576 accuracy= 1.000\n",
      "Epoch: 0249 cost= 0.000396844 accuracy= 1.000\n",
      "Epoch: 0250 cost= 0.000471699 accuracy= 1.000\n",
      "Epoch: 0251 cost= 0.000434842 accuracy= 1.000\n",
      "Epoch: 0252 cost= 0.000574042 accuracy= 1.000\n",
      "Epoch: 0253 cost= 0.000382932 accuracy= 1.000\n",
      "Epoch: 0254 cost= 0.000301043 accuracy= 1.000\n",
      "Epoch: 0255 cost= 0.000427800 accuracy= 1.000\n",
      "Epoch: 0256 cost= 0.000191672 accuracy= 1.000\n",
      "Epoch: 0257 cost= 0.000178416 accuracy= 1.000\n",
      "Epoch: 0258 cost= 0.000373223 accuracy= 1.000\n",
      "Epoch: 0259 cost= 0.000284606 accuracy= 1.000\n",
      "Epoch: 0260 cost= 0.000159514 accuracy= 1.000\n",
      "Epoch: 0261 cost= 0.000168197 accuracy= 1.000\n",
      "Epoch: 0262 cost= 0.000252325 accuracy= 1.000\n",
      "Epoch: 0263 cost= 0.000166401 accuracy= 1.000\n",
      "Epoch: 0264 cost= 0.000226981 accuracy= 1.000\n",
      "Epoch: 0265 cost= 0.000173463 accuracy= 1.000\n",
      "Epoch: 0266 cost= 0.000144840 accuracy= 1.000\n",
      "Epoch: 0267 cost= 0.000282009 accuracy= 1.000\n",
      "Epoch: 0268 cost= 0.000196881 accuracy= 1.000\n",
      "Epoch: 0269 cost= 0.000236151 accuracy= 1.000\n",
      "Epoch: 0270 cost= 0.000282225 accuracy= 1.000\n",
      "Epoch: 0271 cost= 0.000217475 accuracy= 1.000\n",
      "Epoch: 0272 cost= 0.000097613 accuracy= 1.000\n",
      "Epoch: 0273 cost= 0.000177248 accuracy= 1.000\n",
      "Epoch: 0274 cost= 0.000259953 accuracy= 1.000\n",
      "Epoch: 0275 cost= 0.000161157 accuracy= 1.000\n",
      "Epoch: 0276 cost= 0.000215334 accuracy= 1.000\n",
      "Epoch: 0277 cost= 0.000236742 accuracy= 1.000\n",
      "Epoch: 0278 cost= 0.000182837 accuracy= 1.000\n",
      "Epoch: 0279 cost= 0.000148016 accuracy= 1.000\n",
      "Epoch: 0280 cost= 0.000142337 accuracy= 1.000\n",
      "Epoch: 0281 cost= 0.000189906 accuracy= 1.000\n",
      "Epoch: 0282 cost= 0.000186532 accuracy= 1.000\n",
      "Epoch: 0283 cost= 0.000116052 accuracy= 1.000\n",
      "Epoch: 0284 cost= 0.000137017 accuracy= 1.000\n",
      "Epoch: 0285 cost= 0.000131028 accuracy= 1.000\n",
      "Epoch: 0286 cost= 0.000169182 accuracy= 1.000\n",
      "Epoch: 0287 cost= 0.000213954 accuracy= 1.000\n",
      "Epoch: 0288 cost= 0.000215277 accuracy= 1.000\n",
      "Epoch: 0289 cost= 0.000198162 accuracy= 1.000\n",
      "Epoch: 0290 cost= 0.000145686 accuracy= 1.000\n",
      "Epoch: 0291 cost= 0.000175328 accuracy= 1.000\n",
      "Epoch: 0292 cost= 0.000098019 accuracy= 1.000\n",
      "Epoch: 0293 cost= 0.000109833 accuracy= 1.000\n",
      "Epoch: 0294 cost= 0.000136311 accuracy= 1.000\n",
      "Epoch: 0295 cost= 0.000143317 accuracy= 1.000\n",
      "Epoch: 0296 cost= 0.000117033 accuracy= 1.000\n",
      "Epoch: 0297 cost= 0.000132006 accuracy= 1.000\n",
      "Epoch: 0298 cost= 0.000096212 accuracy= 1.000\n",
      "Epoch: 0299 cost= 0.000157307 accuracy= 1.000\n",
      "Epoch: 0300 cost= 0.000168763 accuracy= 1.000\n",
      "Epoch: 0301 cost= 0.000123903 accuracy= 1.000\n",
      "Epoch: 0302 cost= 0.000117060 accuracy= 1.000\n",
      "Epoch: 0303 cost= 0.000133723 accuracy= 1.000\n",
      "Epoch: 0304 cost= 0.000143668 accuracy= 1.000\n",
      "Epoch: 0305 cost= 0.000091624 accuracy= 1.000\n",
      "Epoch: 0306 cost= 0.000114909 accuracy= 1.000\n",
      "Epoch: 0307 cost= 0.000147749 accuracy= 1.000\n",
      "Epoch: 0308 cost= 0.000126937 accuracy= 1.000\n",
      "Epoch: 0309 cost= 0.000138530 accuracy= 1.000\n",
      "Epoch: 0310 cost= 0.000091676 accuracy= 1.000\n",
      "Epoch: 0311 cost= 0.000084432 accuracy= 1.000\n",
      "Epoch: 0312 cost= 0.000066475 accuracy= 1.000\n",
      "Epoch: 0313 cost= 0.000055309 accuracy= 1.000\n",
      "Epoch: 0314 cost= 0.000135948 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0315 cost= 0.000099734 accuracy= 1.000\n",
      "Epoch: 0316 cost= 0.000116273 accuracy= 1.000\n",
      "Epoch: 0317 cost= 0.000088039 accuracy= 1.000\n",
      "Epoch: 0318 cost= 0.000103091 accuracy= 1.000\n",
      "Epoch: 0319 cost= 0.000094710 accuracy= 1.000\n",
      "Epoch: 0320 cost= 0.000104461 accuracy= 1.000\n",
      "Epoch: 0321 cost= 0.000107214 accuracy= 1.000\n",
      "Epoch: 0322 cost= 0.000108396 accuracy= 1.000\n",
      "Epoch: 0323 cost= 0.000095580 accuracy= 1.000\n",
      "Epoch: 0324 cost= 0.000076381 accuracy= 1.000\n",
      "Epoch: 0325 cost= 0.000069315 accuracy= 1.000\n",
      "Epoch: 0326 cost= 0.000066701 accuracy= 1.000\n",
      "Epoch: 0327 cost= 0.000099358 accuracy= 1.000\n",
      "Epoch: 0328 cost= 0.000084501 accuracy= 1.000\n",
      "Epoch: 0329 cost= 0.000051896 accuracy= 1.000\n",
      "Epoch: 0330 cost= 0.000061296 accuracy= 1.000\n",
      "Epoch: 0331 cost= 0.000064929 accuracy= 1.000\n",
      "Epoch: 0332 cost= 0.000047296 accuracy= 1.000\n",
      "Epoch: 0333 cost= 0.000140677 accuracy= 1.000\n",
      "Epoch: 0334 cost= 0.000111956 accuracy= 1.000\n",
      "Epoch: 0335 cost= 0.000111213 accuracy= 1.000\n",
      "Epoch: 0336 cost= 0.000077719 accuracy= 1.000\n",
      "Epoch: 0337 cost= 0.000060568 accuracy= 1.000\n",
      "Epoch: 0338 cost= 0.000062510 accuracy= 1.000\n",
      "Epoch: 0339 cost= 0.000060803 accuracy= 1.000\n",
      "Epoch: 0340 cost= 0.000053526 accuracy= 1.000\n",
      "Epoch: 0341 cost= 0.000080347 accuracy= 1.000\n",
      "Epoch: 0342 cost= 0.000083265 accuracy= 1.000\n",
      "Epoch: 0343 cost= 0.000084337 accuracy= 1.000\n",
      "Epoch: 0344 cost= 0.000076044 accuracy= 1.000\n",
      "Epoch: 0345 cost= 0.000061026 accuracy= 1.000\n",
      "Epoch: 0346 cost= 0.000077979 accuracy= 1.000\n",
      "Epoch: 0347 cost= 0.000062953 accuracy= 1.000\n",
      "Epoch: 0348 cost= 0.000049645 accuracy= 1.000\n",
      "Epoch: 0349 cost= 0.000055057 accuracy= 1.000\n",
      "Epoch: 0350 cost= 0.000058243 accuracy= 1.000\n",
      "Epoch: 0351 cost= 0.000060653 accuracy= 1.000\n",
      "Epoch: 0352 cost= 0.000047221 accuracy= 1.000\n",
      "Epoch: 0353 cost= 0.000064809 accuracy= 1.000\n",
      "Epoch: 0354 cost= 0.000060875 accuracy= 1.000\n",
      "Epoch: 0355 cost= 0.000054748 accuracy= 1.000\n",
      "Epoch: 0356 cost= 0.000058039 accuracy= 1.000\n",
      "Epoch: 0357 cost= 0.000098651 accuracy= 1.000\n",
      "Epoch: 0358 cost= 0.000085076 accuracy= 1.000\n",
      "Epoch: 0359 cost= 0.000056245 accuracy= 1.000\n",
      "Epoch: 0360 cost= 0.000074545 accuracy= 1.000\n",
      "Epoch: 0361 cost= 0.000051521 accuracy= 1.000\n",
      "Epoch: 0362 cost= 0.000049251 accuracy= 1.000\n",
      "Epoch: 0363 cost= 0.000043416 accuracy= 1.000\n",
      "Epoch: 0364 cost= 0.000038062 accuracy= 1.000\n",
      "Epoch: 0365 cost= 0.000101066 accuracy= 1.000\n",
      "Epoch: 0366 cost= 0.000049666 accuracy= 1.000\n",
      "Epoch: 0367 cost= 0.000062127 accuracy= 1.000\n",
      "Epoch: 0368 cost= 0.000051913 accuracy= 1.000\n",
      "Epoch: 0369 cost= 0.000064592 accuracy= 1.000\n",
      "Epoch: 0370 cost= 0.000046849 accuracy= 1.000\n",
      "Epoch: 0371 cost= 0.000039752 accuracy= 1.000\n",
      "Epoch: 0372 cost= 0.000049164 accuracy= 1.000\n",
      "Epoch: 0373 cost= 0.000071152 accuracy= 1.000\n",
      "Epoch: 0374 cost= 0.000044887 accuracy= 1.000\n",
      "Epoch: 0375 cost= 0.000052932 accuracy= 1.000\n",
      "Epoch: 0376 cost= 0.000038518 accuracy= 1.000\n",
      "Epoch: 0377 cost= 0.000044745 accuracy= 1.000\n",
      "Epoch: 0378 cost= 0.000051393 accuracy= 1.000\n",
      "Epoch: 0379 cost= 0.000059394 accuracy= 1.000\n",
      "Epoch: 0380 cost= 0.000053371 accuracy= 1.000\n",
      "Epoch: 0381 cost= 0.000037586 accuracy= 1.000\n",
      "Epoch: 0382 cost= 0.000031340 accuracy= 1.000\n",
      "Epoch: 0383 cost= 0.000051118 accuracy= 1.000\n",
      "Epoch: 0384 cost= 0.000050529 accuracy= 1.000\n",
      "Epoch: 0385 cost= 0.000058683 accuracy= 1.000\n",
      "Epoch: 0386 cost= 0.000047625 accuracy= 1.000\n",
      "Epoch: 0387 cost= 0.000050962 accuracy= 1.000\n",
      "Epoch: 0388 cost= 0.000067659 accuracy= 1.000\n",
      "Epoch: 0389 cost= 0.000040175 accuracy= 1.000\n",
      "Epoch: 0390 cost= 0.000034337 accuracy= 1.000\n",
      "Epoch: 0391 cost= 0.000034855 accuracy= 1.000\n",
      "Epoch: 0392 cost= 0.000037900 accuracy= 1.000\n",
      "Epoch: 0393 cost= 0.000032279 accuracy= 1.000\n",
      "Epoch: 0394 cost= 0.000054173 accuracy= 1.000\n",
      "Epoch: 0395 cost= 0.000035167 accuracy= 1.000\n",
      "Epoch: 0396 cost= 0.000031139 accuracy= 1.000\n",
      "Epoch: 0397 cost= 0.000037572 accuracy= 1.000\n",
      "Epoch: 0398 cost= 0.000043141 accuracy= 1.000\n",
      "Epoch: 0399 cost= 0.000038702 accuracy= 1.000\n",
      "Epoch: 0400 cost= 0.000036128 accuracy= 1.000\n",
      "Epoch: 0401 cost= 0.000033925 accuracy= 1.000\n",
      "Epoch: 0402 cost= 0.000050811 accuracy= 1.000\n",
      "Epoch: 0403 cost= 0.000044702 accuracy= 1.000\n",
      "Epoch: 0404 cost= 0.000039143 accuracy= 1.000\n",
      "Epoch: 0405 cost= 0.000039730 accuracy= 1.000\n",
      "Epoch: 0406 cost= 0.000033970 accuracy= 1.000\n",
      "Epoch: 0407 cost= 0.000033927 accuracy= 1.000\n",
      "Epoch: 0408 cost= 0.000034787 accuracy= 1.000\n",
      "Epoch: 0409 cost= 0.000042407 accuracy= 1.000\n",
      "Epoch: 0410 cost= 0.000028205 accuracy= 1.000\n",
      "Epoch: 0411 cost= 0.000048776 accuracy= 1.000\n",
      "Epoch: 0412 cost= 0.000038491 accuracy= 1.000\n",
      "Epoch: 0413 cost= 0.000032536 accuracy= 1.000\n",
      "Epoch: 0414 cost= 0.000037956 accuracy= 1.000\n",
      "Epoch: 0415 cost= 0.000022563 accuracy= 1.000\n",
      "Epoch: 0416 cost= 0.000037123 accuracy= 1.000\n",
      "Epoch: 0417 cost= 0.000040399 accuracy= 1.000\n",
      "Epoch: 0418 cost= 0.000028632 accuracy= 1.000\n",
      "Epoch: 0419 cost= 0.000032444 accuracy= 1.000\n",
      "Epoch: 0420 cost= 0.000028145 accuracy= 1.000\n",
      "Epoch: 0421 cost= 0.000025075 accuracy= 1.000\n",
      "Epoch: 0422 cost= 0.000025671 accuracy= 1.000\n",
      "Epoch: 0423 cost= 0.000026653 accuracy= 1.000\n",
      "Epoch: 0424 cost= 0.000027462 accuracy= 1.000\n",
      "Epoch: 0425 cost= 0.000024443 accuracy= 1.000\n",
      "Epoch: 0426 cost= 0.000026553 accuracy= 1.000\n",
      "Epoch: 0427 cost= 0.000038292 accuracy= 1.000\n",
      "Epoch: 0428 cost= 0.000024465 accuracy= 1.000\n",
      "Epoch: 0429 cost= 0.000025328 accuracy= 1.000\n",
      "Epoch: 0430 cost= 0.000027773 accuracy= 1.000\n",
      "Epoch: 0431 cost= 0.000023443 accuracy= 1.000\n",
      "Epoch: 0432 cost= 0.000029303 accuracy= 1.000\n",
      "Epoch: 0433 cost= 0.000024311 accuracy= 1.000\n",
      "Epoch: 0434 cost= 0.000022785 accuracy= 1.000\n",
      "Epoch: 0435 cost= 0.000029584 accuracy= 1.000\n",
      "Epoch: 0436 cost= 0.000026144 accuracy= 1.000\n",
      "Epoch: 0437 cost= 0.000030089 accuracy= 1.000\n",
      "Epoch: 0438 cost= 0.000022942 accuracy= 1.000\n",
      "Epoch: 0439 cost= 0.000031750 accuracy= 1.000\n",
      "Epoch: 0440 cost= 0.000025255 accuracy= 1.000\n",
      "Epoch: 0441 cost= 0.000024489 accuracy= 1.000\n",
      "Epoch: 0442 cost= 0.000021544 accuracy= 1.000\n",
      "Epoch: 0443 cost= 0.000025565 accuracy= 1.000\n",
      "Epoch: 0444 cost= 0.000023966 accuracy= 1.000\n",
      "Epoch: 0445 cost= 0.000016879 accuracy= 1.000\n",
      "Epoch: 0446 cost= 0.000020238 accuracy= 1.000\n",
      "Epoch: 0447 cost= 0.000027360 accuracy= 1.000\n",
      "Epoch: 0448 cost= 0.000027061 accuracy= 1.000\n",
      "Epoch: 0449 cost= 0.000017439 accuracy= 1.000\n",
      "Epoch: 0450 cost= 0.000020363 accuracy= 1.000\n",
      "Epoch: 0451 cost= 0.000019654 accuracy= 1.000\n",
      "Epoch: 0452 cost= 0.000018501 accuracy= 1.000\n",
      "Epoch: 0453 cost= 0.000016919 accuracy= 1.000\n",
      "Epoch: 0454 cost= 0.000017944 accuracy= 1.000\n",
      "Epoch: 0455 cost= 0.000025356 accuracy= 1.000\n",
      "Epoch: 0456 cost= 0.000023163 accuracy= 1.000\n",
      "Epoch: 0457 cost= 0.000017361 accuracy= 1.000\n",
      "Epoch: 0458 cost= 0.000018112 accuracy= 1.000\n",
      "Epoch: 0459 cost= 0.000023697 accuracy= 1.000\n",
      "Epoch: 0460 cost= 0.000015185 accuracy= 1.000\n",
      "Epoch: 0461 cost= 0.000017893 accuracy= 1.000\n",
      "Epoch: 0462 cost= 0.000017699 accuracy= 1.000\n",
      "Epoch: 0463 cost= 0.000024008 accuracy= 1.000\n",
      "Epoch: 0464 cost= 0.000016110 accuracy= 1.000\n",
      "Epoch: 0465 cost= 0.000019813 accuracy= 1.000\n",
      "Epoch: 0466 cost= 0.000016360 accuracy= 1.000\n",
      "Epoch: 0467 cost= 0.000017398 accuracy= 1.000\n",
      "Epoch: 0468 cost= 0.000017249 accuracy= 1.000\n",
      "Epoch: 0469 cost= 0.000020712 accuracy= 1.000\n",
      "Epoch: 0470 cost= 0.000018048 accuracy= 1.000\n",
      "Epoch: 0471 cost= 0.000020198 accuracy= 1.000\n",
      "Epoch: 0472 cost= 0.000016128 accuracy= 1.000\n",
      "Epoch: 0473 cost= 0.000014395 accuracy= 1.000\n",
      "Epoch: 0474 cost= 0.000020492 accuracy= 1.000\n",
      "Epoch: 0475 cost= 0.000016417 accuracy= 1.000\n",
      "Epoch: 0476 cost= 0.000014504 accuracy= 1.000\n",
      "Epoch: 0477 cost= 0.000015575 accuracy= 1.000\n",
      "Epoch: 0478 cost= 0.000018279 accuracy= 1.000\n",
      "Epoch: 0479 cost= 0.000020446 accuracy= 1.000\n",
      "Epoch: 0480 cost= 0.000017768 accuracy= 1.000\n",
      "Epoch: 0481 cost= 0.000017222 accuracy= 1.000\n",
      "Epoch: 0482 cost= 0.000014961 accuracy= 1.000\n",
      "Epoch: 0483 cost= 0.000014329 accuracy= 1.000\n",
      "Epoch: 0484 cost= 0.000015955 accuracy= 1.000\n",
      "Epoch: 0485 cost= 0.000013648 accuracy= 1.000\n",
      "Epoch: 0486 cost= 0.000014163 accuracy= 1.000\n",
      "Epoch: 0487 cost= 0.000022007 accuracy= 1.000\n",
      "Epoch: 0488 cost= 0.000017443 accuracy= 1.000\n",
      "Epoch: 0489 cost= 0.000015406 accuracy= 1.000\n",
      "Epoch: 0490 cost= 0.000016456 accuracy= 1.000\n",
      "Epoch: 0491 cost= 0.000019329 accuracy= 1.000\n",
      "Epoch: 0492 cost= 0.000011034 accuracy= 1.000\n",
      "Epoch: 0493 cost= 0.000015152 accuracy= 1.000\n",
      "Epoch: 0494 cost= 0.000016865 accuracy= 1.000\n",
      "Epoch: 0495 cost= 0.000013636 accuracy= 1.000\n",
      "Epoch: 0496 cost= 0.000012906 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0497 cost= 0.000010865 accuracy= 1.000\n",
      "Epoch: 0498 cost= 0.000014266 accuracy= 1.000\n",
      "Epoch: 0499 cost= 0.000012982 accuracy= 1.000\n",
      "Epoch: 0500 cost= 0.000009246 accuracy= 1.000\n",
      "Epoch: 0501 cost= 0.000010837 accuracy= 1.000\n",
      "Epoch: 0502 cost= 0.000010704 accuracy= 1.000\n",
      "Epoch: 0503 cost= 0.000016570 accuracy= 1.000\n",
      "Epoch: 0504 cost= 0.000014153 accuracy= 1.000\n",
      "Epoch: 0505 cost= 0.000010802 accuracy= 1.000\n",
      "Epoch: 0506 cost= 0.000012903 accuracy= 1.000\n",
      "Epoch: 0507 cost= 0.000012858 accuracy= 1.000\n",
      "Epoch: 0508 cost= 0.000015158 accuracy= 1.000\n",
      "Epoch: 0509 cost= 0.000013920 accuracy= 1.000\n",
      "Epoch: 0510 cost= 0.000013442 accuracy= 1.000\n",
      "Epoch: 0511 cost= 0.000013048 accuracy= 1.000\n",
      "Epoch: 0512 cost= 0.000011262 accuracy= 1.000\n",
      "Epoch: 0513 cost= 0.000010252 accuracy= 1.000\n",
      "Epoch: 0514 cost= 0.000012901 accuracy= 1.000\n",
      "Epoch: 0515 cost= 0.000010842 accuracy= 1.000\n",
      "Epoch: 0516 cost= 0.000010361 accuracy= 1.000\n",
      "Epoch: 0517 cost= 0.000011451 accuracy= 1.000\n",
      "Epoch: 0518 cost= 0.000011350 accuracy= 1.000\n",
      "Epoch: 0519 cost= 0.000010935 accuracy= 1.000\n",
      "Epoch: 0520 cost= 0.000013007 accuracy= 1.000\n",
      "Epoch: 0521 cost= 0.000010485 accuracy= 1.000\n",
      "Epoch: 0522 cost= 0.000008863 accuracy= 1.000\n",
      "Epoch: 0523 cost= 0.000014994 accuracy= 1.000\n",
      "Epoch: 0524 cost= 0.000008712 accuracy= 1.000\n",
      "Epoch: 0525 cost= 0.000010211 accuracy= 1.000\n",
      "Epoch: 0526 cost= 0.000010579 accuracy= 1.000\n",
      "Epoch: 0527 cost= 0.000013575 accuracy= 1.000\n",
      "Epoch: 0528 cost= 0.000007957 accuracy= 1.000\n",
      "Epoch: 0529 cost= 0.000007636 accuracy= 1.000\n",
      "Epoch: 0530 cost= 0.000010277 accuracy= 1.000\n",
      "Epoch: 0531 cost= 0.000009563 accuracy= 1.000\n",
      "Epoch: 0532 cost= 0.000009590 accuracy= 1.000\n",
      "Epoch: 0533 cost= 0.000013257 accuracy= 1.000\n",
      "Epoch: 0534 cost= 0.000010120 accuracy= 1.000\n",
      "Epoch: 0535 cost= 0.000008706 accuracy= 1.000\n",
      "Epoch: 0536 cost= 0.000008225 accuracy= 1.000\n",
      "Epoch: 0537 cost= 0.000009052 accuracy= 1.000\n",
      "Epoch: 0538 cost= 0.000008907 accuracy= 1.000\n",
      "Epoch: 0539 cost= 0.000010134 accuracy= 1.000\n",
      "Epoch: 0540 cost= 0.000012577 accuracy= 1.000\n",
      "Epoch: 0541 cost= 0.000009762 accuracy= 1.000\n",
      "Epoch: 0542 cost= 0.000009979 accuracy= 1.000\n",
      "Epoch: 0543 cost= 0.000011786 accuracy= 1.000\n",
      "Epoch: 0544 cost= 0.000009015 accuracy= 1.000\n",
      "Epoch: 0545 cost= 0.000009679 accuracy= 1.000\n",
      "Epoch: 0546 cost= 0.000009438 accuracy= 1.000\n",
      "Epoch: 0547 cost= 0.000007759 accuracy= 1.000\n",
      "Epoch: 0548 cost= 0.000008561 accuracy= 1.000\n",
      "Epoch: 0549 cost= 0.000008770 accuracy= 1.000\n",
      "Epoch: 0550 cost= 0.000009287 accuracy= 1.000\n",
      "Epoch: 0551 cost= 0.000008695 accuracy= 1.000\n",
      "Epoch: 0552 cost= 0.000009564 accuracy= 1.000\n",
      "Epoch: 0553 cost= 0.000008422 accuracy= 1.000\n",
      "Epoch: 0554 cost= 0.000008170 accuracy= 1.000\n",
      "Epoch: 0555 cost= 0.000006553 accuracy= 1.000\n",
      "Epoch: 0556 cost= 0.000006194 accuracy= 1.000\n",
      "Epoch: 0557 cost= 0.000005758 accuracy= 1.000\n",
      "Epoch: 0558 cost= 0.000008213 accuracy= 1.000\n",
      "Epoch: 0559 cost= 0.000009568 accuracy= 1.000\n",
      "Epoch: 0560 cost= 0.000006930 accuracy= 1.000\n",
      "Epoch: 0561 cost= 0.000007326 accuracy= 1.000\n",
      "Epoch: 0562 cost= 0.000005544 accuracy= 1.000\n",
      "Epoch: 0563 cost= 0.000008625 accuracy= 1.000\n",
      "Epoch: 0564 cost= 0.000005818 accuracy= 1.000\n",
      "Epoch: 0565 cost= 0.000008190 accuracy= 1.000\n",
      "Epoch: 0566 cost= 0.000007918 accuracy= 1.000\n",
      "Epoch: 0567 cost= 0.000007972 accuracy= 1.000\n",
      "Epoch: 0568 cost= 0.000007580 accuracy= 1.000\n",
      "Epoch: 0569 cost= 0.000007880 accuracy= 1.000\n",
      "Epoch: 0570 cost= 0.000008573 accuracy= 1.000\n",
      "Epoch: 0571 cost= 0.000008449 accuracy= 1.000\n",
      "Epoch: 0572 cost= 0.000006206 accuracy= 1.000\n",
      "Epoch: 0573 cost= 0.000006528 accuracy= 1.000\n",
      "Epoch: 0574 cost= 0.000008251 accuracy= 1.000\n",
      "Epoch: 0575 cost= 0.000007387 accuracy= 1.000\n",
      "Epoch: 0576 cost= 0.000008524 accuracy= 1.000\n",
      "Epoch: 0577 cost= 0.000008903 accuracy= 1.000\n",
      "Epoch: 0578 cost= 0.000007221 accuracy= 1.000\n",
      "Epoch: 0579 cost= 0.000005807 accuracy= 1.000\n",
      "Epoch: 0580 cost= 0.000008163 accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_iris_tree0.ckpt\n",
      "Epoch: 0001 cost= 2.309446195 accuracy= 0.188\n",
      "Epoch: 0002 cost= 2.009221505 accuracy= 0.208\n",
      "Epoch: 0003 cost= 1.755655078 accuracy= 0.267\n",
      "Epoch: 0004 cost= 1.642316678 accuracy= 0.284\n",
      "Epoch: 0005 cost= 1.541252536 accuracy= 0.321\n",
      "Epoch: 0006 cost= 1.503792615 accuracy= 0.392\n",
      "Epoch: 0007 cost= 1.445654722 accuracy= 0.387\n",
      "Epoch: 0008 cost= 1.384082899 accuracy= 0.405\n",
      "Epoch: 0009 cost= 1.387260318 accuracy= 0.441\n",
      "Epoch: 0010 cost= 1.285257564 accuracy= 0.450\n",
      "Epoch: 0011 cost= 1.278924570 accuracy= 0.539\n",
      "Epoch: 0012 cost= 1.270446308 accuracy= 0.500\n",
      "Epoch: 0013 cost= 1.209409370 accuracy= 0.528\n",
      "Epoch: 0014 cost= 1.168807938 accuracy= 0.594\n",
      "Epoch: 0015 cost= 1.045389565 accuracy= 0.686\n",
      "Epoch: 0016 cost= 0.853298555 accuracy= 0.671\n",
      "Epoch: 0017 cost= 0.763815922 accuracy= 0.715\n",
      "Epoch: 0018 cost= 0.685610939 accuracy= 0.721\n",
      "Epoch: 0019 cost= 0.654062483 accuracy= 0.755\n",
      "Epoch: 0020 cost= 0.603106101 accuracy= 0.773\n",
      "Epoch: 0021 cost= 0.608595846 accuracy= 0.767\n",
      "Epoch: 0022 cost= 0.579723414 accuracy= 0.790\n",
      "Epoch: 0023 cost= 0.515718273 accuracy= 0.827\n",
      "Epoch: 0024 cost= 0.462650450 accuracy= 0.772\n",
      "Epoch: 0025 cost= 0.435395898 accuracy= 0.856\n",
      "Epoch: 0026 cost= 0.428974390 accuracy= 0.851\n",
      "Epoch: 0027 cost= 0.400270751 accuracy= 0.880\n",
      "Epoch: 0028 cost= 0.448347399 accuracy= 0.874\n",
      "Epoch: 0029 cost= 0.344130839 accuracy= 0.886\n",
      "Epoch: 0030 cost= 0.335665309 accuracy= 0.877\n",
      "Epoch: 0031 cost= 0.310103128 accuracy= 0.923\n",
      "Epoch: 0032 cost= 0.324427255 accuracy= 0.917\n",
      "Epoch: 0033 cost= 0.246898172 accuracy= 0.902\n",
      "Epoch: 0034 cost= 0.258370527 accuracy= 0.922\n",
      "Epoch: 0035 cost= 0.235215326 accuracy= 0.930\n",
      "Epoch: 0036 cost= 0.274457320 accuracy= 0.939\n",
      "Epoch: 0037 cost= 0.193895838 accuracy= 0.937\n",
      "Epoch: 0038 cost= 0.164184883 accuracy= 0.945\n",
      "Epoch: 0039 cost= 0.206819008 accuracy= 0.921\n",
      "Epoch: 0040 cost= 0.195383531 accuracy= 0.934\n",
      "Epoch: 0041 cost= 0.197065416 accuracy= 0.946\n",
      "Epoch: 0042 cost= 0.143891916 accuracy= 0.943\n",
      "Epoch: 0043 cost= 0.169166199 accuracy= 0.949\n",
      "Epoch: 0044 cost= 0.162125227 accuracy= 0.954\n",
      "Epoch: 0045 cost= 0.150914527 accuracy= 0.953\n",
      "Epoch: 0046 cost= 0.127141894 accuracy= 0.961\n",
      "Epoch: 0047 cost= 0.133490461 accuracy= 0.958\n",
      "Epoch: 0048 cost= 0.139547045 accuracy= 0.958\n",
      "Epoch: 0049 cost= 0.098331468 accuracy= 0.958\n",
      "Epoch: 0050 cost= 0.134710340 accuracy= 0.964\n",
      "Epoch: 0051 cost= 0.112035329 accuracy= 0.950\n",
      "Epoch: 0052 cost= 0.119834479 accuracy= 0.953\n",
      "Epoch: 0053 cost= 0.105685320 accuracy= 0.949\n",
      "Epoch: 0054 cost= 0.101807713 accuracy= 0.954\n",
      "Epoch: 0055 cost= 0.123991466 accuracy= 0.964\n",
      "Epoch: 0056 cost= 0.111582319 accuracy= 0.957\n",
      "Epoch: 0057 cost= 0.114722828 accuracy= 0.960\n",
      "Epoch: 0058 cost= 0.117788776 accuracy= 0.967\n",
      "Epoch: 0059 cost= 0.085295508 accuracy= 0.974\n",
      "Epoch: 0060 cost= 0.104778125 accuracy= 0.966\n",
      "Epoch: 0061 cost= 0.101023985 accuracy= 0.966\n",
      "Epoch: 0062 cost= 0.105026203 accuracy= 0.971\n",
      "Epoch: 0063 cost= 0.077710934 accuracy= 0.967\n",
      "Epoch: 0064 cost= 0.133397057 accuracy= 0.908\n",
      "Epoch: 0065 cost= 0.132892076 accuracy= 0.966\n",
      "Epoch: 0066 cost= 0.097125516 accuracy= 0.960\n",
      "Epoch: 0067 cost= 0.129816335 accuracy= 0.978\n",
      "Epoch: 0068 cost= 0.087188772 accuracy= 0.974\n",
      "Epoch: 0069 cost= 0.064117188 accuracy= 0.979\n",
      "Epoch: 0070 cost= 0.086647087 accuracy= 0.975\n",
      "Epoch: 0071 cost= 0.079307711 accuracy= 0.966\n",
      "Epoch: 0072 cost= 0.094720615 accuracy= 0.977\n",
      "Epoch: 0073 cost= 0.057042915 accuracy= 0.976\n",
      "Epoch: 0074 cost= 0.086032916 accuracy= 0.977\n",
      "Epoch: 0075 cost= 0.078692569 accuracy= 0.982\n",
      "Epoch: 0076 cost= 0.066487000 accuracy= 0.975\n",
      "Epoch: 0077 cost= 0.059864310 accuracy= 0.984\n",
      "Epoch: 0078 cost= 0.058800104 accuracy= 0.982\n",
      "Epoch: 0079 cost= 0.061215169 accuracy= 0.973\n",
      "Epoch: 0080 cost= 0.074421201 accuracy= 0.984\n",
      "Epoch: 0081 cost= 0.057056660 accuracy= 0.983\n",
      "Epoch: 0082 cost= 0.053910543 accuracy= 0.974\n",
      "Epoch: 0083 cost= 0.061999517 accuracy= 0.979\n",
      "Epoch: 0084 cost= 0.042140655 accuracy= 0.984\n",
      "Epoch: 0085 cost= 0.049876280 accuracy= 0.987\n",
      "Epoch: 0086 cost= 0.062454451 accuracy= 0.980\n",
      "Epoch: 0087 cost= 0.043085060 accuracy= 0.985\n",
      "Epoch: 0088 cost= 0.042190540 accuracy= 0.988\n",
      "Epoch: 0089 cost= 0.040598443 accuracy= 0.989\n",
      "Epoch: 0090 cost= 0.055942229 accuracy= 0.980\n",
      "Epoch: 0091 cost= 0.085570302 accuracy= 0.977\n",
      "Epoch: 0092 cost= 0.081770876 accuracy= 0.973\n",
      "Epoch: 0093 cost= 0.057165776 accuracy= 0.985\n",
      "Epoch: 0094 cost= 0.072406836 accuracy= 0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0095 cost= 0.046185081 accuracy= 0.977\n",
      "Epoch: 0096 cost= 0.043500614 accuracy= 0.982\n",
      "Epoch: 0097 cost= 0.046657428 accuracy= 0.985\n",
      "Epoch: 0098 cost= 0.033410610 accuracy= 0.990\n",
      "Epoch: 0099 cost= 0.026493706 accuracy= 0.985\n",
      "Epoch: 0100 cost= 0.039552045 accuracy= 0.991\n",
      "Epoch: 0101 cost= 0.037659254 accuracy= 0.986\n",
      "Epoch: 0102 cost= 0.035862749 accuracy= 0.989\n",
      "Epoch: 0103 cost= 0.026379148 accuracy= 0.990\n",
      "Epoch: 0104 cost= 0.020148698 accuracy= 0.982\n",
      "Epoch: 0105 cost= 0.027367238 accuracy= 0.988\n",
      "Epoch: 0106 cost= 0.023328721 accuracy= 0.985\n",
      "Epoch: 0107 cost= 0.068065127 accuracy= 0.989\n",
      "Epoch: 0108 cost= 0.034704479 accuracy= 0.992\n",
      "Epoch: 0109 cost= 0.038038484 accuracy= 0.985\n",
      "Epoch: 0110 cost= 0.039570274 accuracy= 0.991\n",
      "Epoch: 0111 cost= 0.051098380 accuracy= 0.989\n",
      "Epoch: 0112 cost= 0.033417154 accuracy= 0.993\n",
      "Epoch: 0113 cost= 0.018850599 accuracy= 0.993\n",
      "Epoch: 0114 cost= 0.015941674 accuracy= 0.994\n",
      "Epoch: 0115 cost= 0.022618770 accuracy= 0.994\n",
      "Epoch: 0116 cost= 0.028334604 accuracy= 0.994\n",
      "Epoch: 0117 cost= 0.011512288 accuracy= 0.992\n",
      "Epoch: 0118 cost= 0.033735878 accuracy= 0.985\n",
      "Epoch: 0119 cost= 0.047731386 accuracy= 0.994\n",
      "Epoch: 0120 cost= 0.040542013 accuracy= 0.996\n",
      "Epoch: 0121 cost= 0.029324204 accuracy= 0.994\n",
      "Epoch: 0122 cost= 0.017868905 accuracy= 0.991\n",
      "Epoch: 0123 cost= 0.030479108 accuracy= 0.992\n",
      "Epoch: 0124 cost= 0.022709773 accuracy= 0.993\n",
      "Epoch: 0125 cost= 0.022208305 accuracy= 0.994\n",
      "Epoch: 0126 cost= 0.014678236 accuracy= 0.994\n",
      "Epoch: 0127 cost= 0.026576467 accuracy= 0.994\n",
      "Epoch: 0128 cost= 0.019289374 accuracy= 0.991\n",
      "Epoch: 0129 cost= 0.010765400 accuracy= 0.994\n",
      "Epoch: 0130 cost= 0.016093009 accuracy= 0.995\n",
      "Epoch: 0131 cost= 0.025367675 accuracy= 0.995\n",
      "Epoch: 0132 cost= 0.011565162 accuracy= 0.997\n",
      "Epoch: 0133 cost= 0.009030599 accuracy= 0.996\n",
      "Epoch: 0134 cost= 0.025909493 accuracy= 0.996\n",
      "Epoch: 0135 cost= 0.011268403 accuracy= 0.994\n",
      "Epoch: 0136 cost= 0.020255209 accuracy= 0.995\n",
      "Epoch: 0137 cost= 0.027438754 accuracy= 0.995\n",
      "Epoch: 0138 cost= 0.014885997 accuracy= 0.998\n",
      "Epoch: 0139 cost= 0.024455841 accuracy= 0.994\n",
      "Epoch: 0140 cost= 0.015774322 accuracy= 0.997\n",
      "Epoch: 0141 cost= 0.009108437 accuracy= 0.998\n",
      "Epoch: 0142 cost= 0.016056922 accuracy= 0.997\n",
      "Epoch: 0143 cost= 0.024586899 accuracy= 0.991\n",
      "Epoch: 0144 cost= 0.040729357 accuracy= 0.989\n",
      "Epoch: 0145 cost= 0.035350354 accuracy= 0.990\n",
      "Epoch: 0146 cost= 0.012105039 accuracy= 0.996\n",
      "Epoch: 0147 cost= 0.009912553 accuracy= 0.996\n",
      "Epoch: 0148 cost= 0.016411427 accuracy= 0.994\n",
      "Epoch: 0149 cost= 0.009006234 accuracy= 0.994\n",
      "Epoch: 0150 cost= 0.018935876 accuracy= 0.995\n",
      "Epoch: 0151 cost= 0.022501760 accuracy= 0.997\n",
      "Epoch: 0152 cost= 0.012181179 accuracy= 0.996\n",
      "Epoch: 0153 cost= 0.012346005 accuracy= 0.996\n",
      "Epoch: 0154 cost= 0.014322373 accuracy= 0.997\n",
      "Epoch: 0155 cost= 0.012165306 accuracy= 0.998\n",
      "Epoch: 0156 cost= 0.011050574 accuracy= 0.997\n",
      "Epoch: 0157 cost= 0.010280889 accuracy= 0.998\n",
      "Epoch: 0158 cost= 0.008486938 accuracy= 0.996\n",
      "Epoch: 0159 cost= 0.014255915 accuracy= 0.998\n",
      "Epoch: 0160 cost= 0.007770995 accuracy= 0.995\n",
      "Epoch: 0161 cost= 0.008872512 accuracy= 0.997\n",
      "Epoch: 0162 cost= 0.008485169 accuracy= 0.998\n",
      "Epoch: 0163 cost= 0.017186574 accuracy= 0.996\n",
      "Epoch: 0164 cost= 0.019578057 accuracy= 0.996\n",
      "Epoch: 0165 cost= 0.021054927 accuracy= 0.984\n",
      "Epoch: 0166 cost= 0.036253651 accuracy= 0.985\n",
      "Epoch: 0167 cost= 0.037195660 accuracy= 0.993\n",
      "Epoch: 0168 cost= 0.020748819 accuracy= 0.996\n",
      "Epoch: 0169 cost= 0.021605211 accuracy= 0.996\n",
      "Epoch: 0170 cost= 0.014478298 accuracy= 0.998\n",
      "Epoch: 0171 cost= 0.057216669 accuracy= 0.983\n",
      "Epoch: 0172 cost= 0.023356445 accuracy= 0.996\n",
      "Epoch: 0173 cost= 0.037759979 accuracy= 0.985\n",
      "Epoch: 0174 cost= 0.034083658 accuracy= 0.991\n",
      "Epoch: 0175 cost= 0.014518255 accuracy= 0.989\n",
      "Epoch: 0176 cost= 0.034191693 accuracy= 0.984\n",
      "Epoch: 0177 cost= 0.048456438 accuracy= 0.987\n",
      "Epoch: 0178 cost= 0.053742276 accuracy= 0.983\n",
      "Epoch: 0179 cost= 0.027138210 accuracy= 0.996\n",
      "Epoch: 0180 cost= 0.022425805 accuracy= 0.990\n",
      "Epoch: 0181 cost= 0.027777423 accuracy= 0.996\n",
      "Epoch: 0182 cost= 0.010617652 accuracy= 0.996\n",
      "Epoch: 0183 cost= 0.013384826 accuracy= 0.988\n",
      "Epoch: 0184 cost= 0.019190905 accuracy= 0.996\n",
      "Epoch: 0185 cost= 0.018295626 accuracy= 0.998\n",
      "Epoch: 0186 cost= 0.015564696 accuracy= 0.986\n",
      "Epoch: 0187 cost= 0.029782410 accuracy= 0.986\n",
      "Epoch: 0188 cost= 0.045805771 accuracy= 0.997\n",
      "Epoch: 0189 cost= 0.062386836 accuracy= 0.986\n",
      "Epoch: 0190 cost= 0.084931220 accuracy= 0.981\n",
      "Epoch: 0191 cost= 0.039772099 accuracy= 0.989\n",
      "Epoch: 0192 cost= 0.043701677 accuracy= 0.985\n",
      "Epoch: 0193 cost= 0.025755296 accuracy= 0.986\n",
      "Epoch: 0194 cost= 0.022210283 accuracy= 0.995\n",
      "Epoch: 0195 cost= 0.018334597 accuracy= 0.996\n",
      "Epoch: 0196 cost= 0.007822932 accuracy= 0.998\n",
      "Epoch: 0197 cost= 0.011067018 accuracy= 0.998\n",
      "Epoch: 0198 cost= 0.006370982 accuracy= 0.997\n",
      "Epoch: 0199 cost= 0.006233579 accuracy= 0.997\n",
      "Epoch: 0200 cost= 0.006066886 accuracy= 0.997\n",
      "Epoch: 0201 cost= 0.004326024 accuracy= 0.998\n",
      "Epoch: 0202 cost= 0.005149111 accuracy= 0.998\n",
      "Epoch: 0203 cost= 0.008009328 accuracy= 0.998\n",
      "Epoch: 0204 cost= 0.008110686 accuracy= 0.997\n",
      "Epoch: 0205 cost= 0.007403587 accuracy= 0.997\n",
      "Epoch: 0206 cost= 0.004349270 accuracy= 0.998\n",
      "Epoch: 0207 cost= 0.003634894 accuracy= 0.998\n",
      "Epoch: 0208 cost= 0.004577174 accuracy= 1.000\n",
      "Epoch: 0209 cost= 0.001782290 accuracy= 0.999\n",
      "Epoch: 0210 cost= 0.001718485 accuracy= 0.997\n",
      "Epoch: 0211 cost= 0.006780056 accuracy= 0.998\n",
      "Epoch: 0212 cost= 0.016026682 accuracy= 0.995\n",
      "Epoch: 0213 cost= 0.007756248 accuracy= 0.996\n",
      "Epoch: 0214 cost= 0.006985679 accuracy= 0.999\n",
      "Epoch: 0215 cost= 0.004723646 accuracy= 0.997\n",
      "Epoch: 0216 cost= 0.011080070 accuracy= 0.997\n",
      "Epoch: 0217 cost= 0.009372184 accuracy= 0.996\n",
      "Epoch: 0218 cost= 0.024253355 accuracy= 0.996\n",
      "Epoch: 0219 cost= 0.011950683 accuracy= 0.989\n",
      "Epoch: 0220 cost= 0.025161686 accuracy= 0.972\n",
      "Epoch: 0221 cost= 0.046931475 accuracy= 0.991\n",
      "Epoch: 0222 cost= 0.012309211 accuracy= 0.996\n",
      "Epoch: 0223 cost= 0.015497410 accuracy= 0.994\n",
      "Epoch: 0224 cost= 0.011067733 accuracy= 0.998\n",
      "Epoch: 0225 cost= 0.006033420 accuracy= 0.996\n",
      "Epoch: 0226 cost= 0.011008686 accuracy= 0.999\n",
      "Epoch: 0227 cost= 0.004133493 accuracy= 0.996\n",
      "Epoch: 0228 cost= 0.013738313 accuracy= 0.996\n",
      "Epoch: 0229 cost= 0.006808868 accuracy= 0.996\n",
      "Epoch: 0230 cost= 0.005807549 accuracy= 0.998\n",
      "Epoch: 0231 cost= 0.001684658 accuracy= 0.998\n",
      "Epoch: 0232 cost= 0.000806137 accuracy= 0.998\n",
      "Epoch: 0233 cost= 0.004338857 accuracy= 0.998\n",
      "Epoch: 0234 cost= 0.001742002 accuracy= 0.999\n",
      "Epoch: 0235 cost= 0.003231417 accuracy= 1.000\n",
      "Epoch: 0236 cost= 0.001469624 accuracy= 0.999\n",
      "Epoch: 0237 cost= 0.002170558 accuracy= 1.000\n",
      "Epoch: 0238 cost= 0.001272671 accuracy= 1.000\n",
      "Epoch: 0239 cost= 0.002453296 accuracy= 0.999\n",
      "Epoch: 0240 cost= 0.001819773 accuracy= 1.000\n",
      "Epoch: 0241 cost= 0.001268118 accuracy= 1.000\n",
      "Epoch: 0242 cost= 0.002670937 accuracy= 1.000\n",
      "Epoch: 0243 cost= 0.004879100 accuracy= 1.000\n",
      "Epoch: 0244 cost= 0.003257769 accuracy= 0.997\n",
      "Epoch: 0245 cost= 0.003905098 accuracy= 0.998\n",
      "Epoch: 0246 cost= 0.002176936 accuracy= 0.999\n",
      "Epoch: 0247 cost= 0.003944989 accuracy= 0.999\n",
      "Epoch: 0248 cost= 0.011083690 accuracy= 0.998\n",
      "Epoch: 0249 cost= 0.012968209 accuracy= 0.999\n",
      "Epoch: 0250 cost= 0.004415390 accuracy= 1.000\n",
      "Epoch: 0251 cost= 0.004304998 accuracy= 0.999\n",
      "Epoch: 0252 cost= 0.001704037 accuracy= 0.997\n",
      "Epoch: 0253 cost= 0.002618046 accuracy= 1.000\n",
      "Epoch: 0254 cost= 0.003134548 accuracy= 1.000\n",
      "Epoch: 0255 cost= 0.002429884 accuracy= 1.000\n",
      "Epoch: 0256 cost= 0.004181904 accuracy= 0.996\n",
      "Epoch: 0257 cost= 0.012148657 accuracy= 0.983\n",
      "Epoch: 0258 cost= 0.050881401 accuracy= 0.992\n",
      "Epoch: 0259 cost= 0.064018022 accuracy= 0.989\n",
      "Epoch: 0260 cost= 0.036172435 accuracy= 0.998\n",
      "Epoch: 0261 cost= 0.011525846 accuracy= 0.997\n",
      "Epoch: 0262 cost= 0.007763478 accuracy= 0.998\n",
      "Epoch: 0263 cost= 0.005298167 accuracy= 0.998\n",
      "Epoch: 0264 cost= 0.005713932 accuracy= 0.996\n",
      "Epoch: 0265 cost= 0.008424939 accuracy= 0.996\n",
      "Epoch: 0266 cost= 0.004618885 accuracy= 1.000\n",
      "Epoch: 0267 cost= 0.017789987 accuracy= 0.999\n",
      "Epoch: 0268 cost= 0.050985353 accuracy= 0.978\n",
      "Epoch: 0269 cost= 0.062270374 accuracy= 0.990\n",
      "Epoch: 0270 cost= 0.042483336 accuracy= 0.994\n",
      "Epoch: 0271 cost= 0.010570601 accuracy= 0.996\n",
      "Epoch: 0272 cost= 0.015252087 accuracy= 0.992\n",
      "Epoch: 0273 cost= 0.010805243 accuracy= 0.997\n",
      "Epoch: 0274 cost= 0.007498639 accuracy= 0.998\n",
      "Epoch: 0275 cost= 0.008682222 accuracy= 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0276 cost= 0.004274534 accuracy= 0.996\n",
      "Epoch: 0277 cost= 0.009005564 accuracy= 0.999\n",
      "Epoch: 0278 cost= 0.005026705 accuracy= 1.000\n",
      "Epoch: 0279 cost= 0.001531266 accuracy= 1.000\n",
      "Epoch: 0280 cost= 0.001041330 accuracy= 1.000\n",
      "Epoch: 0281 cost= 0.002808394 accuracy= 1.000\n",
      "Epoch: 0282 cost= 0.001689326 accuracy= 1.000\n",
      "Epoch: 0283 cost= 0.001385817 accuracy= 1.000\n",
      "Epoch: 0284 cost= 0.001297605 accuracy= 1.000\n",
      "Epoch: 0285 cost= 0.000687234 accuracy= 1.000\n",
      "Epoch: 0286 cost= 0.001242542 accuracy= 1.000\n",
      "Epoch: 0287 cost= 0.001452166 accuracy= 1.000\n",
      "Epoch: 0288 cost= 0.001351907 accuracy= 1.000\n",
      "Epoch: 0289 cost= 0.001569509 accuracy= 1.000\n",
      "Epoch: 0290 cost= 0.001876863 accuracy= 1.000\n",
      "Epoch: 0291 cost= 0.000673138 accuracy= 1.000\n",
      "Epoch: 0292 cost= 0.000538926 accuracy= 1.000\n",
      "Epoch: 0293 cost= 0.000962099 accuracy= 1.000\n",
      "Epoch: 0294 cost= 0.000784821 accuracy= 1.000\n",
      "Epoch: 0295 cost= 0.000258703 accuracy= 1.000\n",
      "Epoch: 0296 cost= 0.000563800 accuracy= 1.000\n",
      "Epoch: 0297 cost= 0.000605371 accuracy= 1.000\n",
      "Epoch: 0298 cost= 0.000523913 accuracy= 1.000\n",
      "Epoch: 0299 cost= 0.000619975 accuracy= 1.000\n",
      "Epoch: 0300 cost= 0.000407211 accuracy= 1.000\n",
      "Epoch: 0301 cost= 0.000335861 accuracy= 1.000\n",
      "Epoch: 0302 cost= 0.000340575 accuracy= 1.000\n",
      "Epoch: 0303 cost= 0.000301852 accuracy= 1.000\n",
      "Epoch: 0304 cost= 0.000272692 accuracy= 1.000\n",
      "Epoch: 0305 cost= 0.000271984 accuracy= 1.000\n",
      "Epoch: 0306 cost= 0.000168260 accuracy= 1.000\n",
      "Epoch: 0307 cost= 0.000298761 accuracy= 1.000\n",
      "Epoch: 0308 cost= 0.000190514 accuracy= 1.000\n",
      "Epoch: 0309 cost= 0.000183429 accuracy= 1.000\n",
      "Epoch: 0310 cost= 0.000266180 accuracy= 1.000\n",
      "Epoch: 0311 cost= 0.000293911 accuracy= 1.000\n",
      "Epoch: 0312 cost= 0.000150995 accuracy= 1.000\n",
      "Epoch: 0313 cost= 0.000426078 accuracy= 1.000\n",
      "Epoch: 0314 cost= 0.000217252 accuracy= 1.000\n",
      "Epoch: 0315 cost= 0.000273176 accuracy= 1.000\n",
      "Epoch: 0316 cost= 0.000319964 accuracy= 1.000\n",
      "Epoch: 0317 cost= 0.000147223 accuracy= 1.000\n",
      "Epoch: 0318 cost= 0.000211220 accuracy= 1.000\n",
      "Epoch: 0319 cost= 0.000244311 accuracy= 1.000\n",
      "Epoch: 0320 cost= 0.000174253 accuracy= 1.000\n",
      "Epoch: 0321 cost= 0.000192461 accuracy= 1.000\n",
      "Epoch: 0322 cost= 0.000135708 accuracy= 1.000\n",
      "Epoch: 0323 cost= 0.000195756 accuracy= 1.000\n",
      "Epoch: 0324 cost= 0.000241986 accuracy= 1.000\n",
      "Epoch: 0325 cost= 0.000131741 accuracy= 1.000\n",
      "Epoch: 0326 cost= 0.000167533 accuracy= 1.000\n",
      "Epoch: 0327 cost= 0.000186377 accuracy= 1.000\n",
      "Epoch: 0328 cost= 0.000151343 accuracy= 1.000\n",
      "Epoch: 0329 cost= 0.000184772 accuracy= 1.000\n",
      "Epoch: 0330 cost= 0.000163736 accuracy= 1.000\n",
      "Epoch: 0331 cost= 0.000140175 accuracy= 1.000\n",
      "Epoch: 0332 cost= 0.000126139 accuracy= 1.000\n",
      "Epoch: 0333 cost= 0.000154361 accuracy= 1.000\n",
      "Epoch: 0334 cost= 0.000101451 accuracy= 1.000\n",
      "Epoch: 0335 cost= 0.000071933 accuracy= 1.000\n",
      "Epoch: 0336 cost= 0.000222672 accuracy= 1.000\n",
      "Epoch: 0337 cost= 0.000108590 accuracy= 1.000\n",
      "Epoch: 0338 cost= 0.000151609 accuracy= 1.000\n",
      "Epoch: 0339 cost= 0.000169694 accuracy= 1.000\n",
      "Epoch: 0340 cost= 0.000181173 accuracy= 1.000\n",
      "Epoch: 0341 cost= 0.000156132 accuracy= 1.000\n",
      "Epoch: 0342 cost= 0.000088167 accuracy= 1.000\n",
      "Epoch: 0343 cost= 0.000083441 accuracy= 1.000\n",
      "Epoch: 0344 cost= 0.000143823 accuracy= 1.000\n",
      "Epoch: 0345 cost= 0.000132318 accuracy= 1.000\n",
      "Epoch: 0346 cost= 0.000125637 accuracy= 1.000\n",
      "Epoch: 0347 cost= 0.000111198 accuracy= 1.000\n",
      "Epoch: 0348 cost= 0.000135970 accuracy= 1.000\n",
      "Epoch: 0349 cost= 0.000174229 accuracy= 1.000\n",
      "Epoch: 0350 cost= 0.000196235 accuracy= 1.000\n",
      "Epoch: 0351 cost= 0.000130939 accuracy= 1.000\n",
      "Epoch: 0352 cost= 0.000064928 accuracy= 1.000\n",
      "Epoch: 0353 cost= 0.000089770 accuracy= 1.000\n",
      "Epoch: 0354 cost= 0.000127467 accuracy= 1.000\n",
      "Epoch: 0355 cost= 0.000126072 accuracy= 1.000\n",
      "Epoch: 0356 cost= 0.000155039 accuracy= 1.000\n",
      "Epoch: 0357 cost= 0.000087355 accuracy= 1.000\n",
      "Epoch: 0358 cost= 0.000110103 accuracy= 1.000\n",
      "Epoch: 0359 cost= 0.000088842 accuracy= 1.000\n",
      "Epoch: 0360 cost= 0.000066373 accuracy= 1.000\n",
      "Epoch: 0361 cost= 0.000087295 accuracy= 1.000\n",
      "Epoch: 0362 cost= 0.000159225 accuracy= 1.000\n",
      "Epoch: 0363 cost= 0.000100305 accuracy= 1.000\n",
      "Epoch: 0364 cost= 0.000091674 accuracy= 1.000\n",
      "Epoch: 0365 cost= 0.000176668 accuracy= 1.000\n",
      "Epoch: 0366 cost= 0.000098047 accuracy= 1.000\n",
      "Epoch: 0367 cost= 0.000096137 accuracy= 1.000\n",
      "Epoch: 0368 cost= 0.000058931 accuracy= 1.000\n",
      "Epoch: 0369 cost= 0.000089613 accuracy= 1.000\n",
      "Epoch: 0370 cost= 0.000073530 accuracy= 1.000\n",
      "Epoch: 0371 cost= 0.000131904 accuracy= 1.000\n",
      "Epoch: 0372 cost= 0.000084104 accuracy= 1.000\n",
      "Epoch: 0373 cost= 0.000082990 accuracy= 1.000\n",
      "Epoch: 0374 cost= 0.000131329 accuracy= 1.000\n",
      "Epoch: 0375 cost= 0.000089720 accuracy= 1.000\n",
      "Epoch: 0376 cost= 0.000099510 accuracy= 1.000\n",
      "Epoch: 0377 cost= 0.000076470 accuracy= 1.000\n",
      "Epoch: 0378 cost= 0.000075512 accuracy= 1.000\n",
      "Epoch: 0379 cost= 0.000102682 accuracy= 1.000\n",
      "Epoch: 0380 cost= 0.000118092 accuracy= 1.000\n",
      "Epoch: 0381 cost= 0.000115865 accuracy= 1.000\n",
      "Epoch: 0382 cost= 0.000110116 accuracy= 1.000\n",
      "Epoch: 0383 cost= 0.000087756 accuracy= 1.000\n",
      "Epoch: 0384 cost= 0.000081138 accuracy= 1.000\n",
      "Epoch: 0385 cost= 0.000064331 accuracy= 1.000\n",
      "Epoch: 0386 cost= 0.000083503 accuracy= 1.000\n",
      "Epoch: 0387 cost= 0.000091908 accuracy= 1.000\n",
      "Epoch: 0388 cost= 0.000097577 accuracy= 1.000\n",
      "Epoch: 0389 cost= 0.000055321 accuracy= 1.000\n",
      "Epoch: 0390 cost= 0.000059712 accuracy= 1.000\n",
      "Epoch: 0391 cost= 0.000055708 accuracy= 1.000\n",
      "Epoch: 0392 cost= 0.000059539 accuracy= 1.000\n",
      "Epoch: 0393 cost= 0.000103912 accuracy= 1.000\n",
      "Epoch: 0394 cost= 0.000087726 accuracy= 1.000\n",
      "Epoch: 0395 cost= 0.000095774 accuracy= 1.000\n",
      "Epoch: 0396 cost= 0.000096063 accuracy= 1.000\n",
      "Epoch: 0397 cost= 0.000064841 accuracy= 1.000\n",
      "Epoch: 0398 cost= 0.000034287 accuracy= 1.000\n",
      "Epoch: 0399 cost= 0.000108001 accuracy= 1.000\n",
      "Epoch: 0400 cost= 0.000070846 accuracy= 1.000\n",
      "Epoch: 0401 cost= 0.000075309 accuracy= 1.000\n",
      "Epoch: 0402 cost= 0.000043231 accuracy= 1.000\n",
      "Epoch: 0403 cost= 0.000063587 accuracy= 1.000\n",
      "Epoch: 0404 cost= 0.000079739 accuracy= 1.000\n",
      "Epoch: 0405 cost= 0.000055566 accuracy= 1.000\n",
      "Epoch: 0406 cost= 0.000057733 accuracy= 1.000\n",
      "Epoch: 0407 cost= 0.000053070 accuracy= 1.000\n",
      "Epoch: 0408 cost= 0.000068372 accuracy= 1.000\n",
      "Epoch: 0409 cost= 0.000044786 accuracy= 1.000\n",
      "Epoch: 0410 cost= 0.000072541 accuracy= 1.000\n",
      "Epoch: 0411 cost= 0.000061317 accuracy= 1.000\n",
      "Epoch: 0412 cost= 0.000045637 accuracy= 1.000\n",
      "Epoch: 0413 cost= 0.000053521 accuracy= 1.000\n",
      "Epoch: 0414 cost= 0.000070760 accuracy= 1.000\n",
      "Epoch: 0415 cost= 0.000062049 accuracy= 1.000\n",
      "Epoch: 0416 cost= 0.000046911 accuracy= 1.000\n",
      "Epoch: 0417 cost= 0.000062342 accuracy= 1.000\n",
      "Epoch: 0418 cost= 0.000066512 accuracy= 1.000\n",
      "Epoch: 0419 cost= 0.000049374 accuracy= 1.000\n",
      "Epoch: 0420 cost= 0.000064474 accuracy= 1.000\n",
      "Epoch: 0421 cost= 0.000048180 accuracy= 1.000\n",
      "Epoch: 0422 cost= 0.000054605 accuracy= 1.000\n",
      "Epoch: 0423 cost= 0.000048541 accuracy= 1.000\n",
      "Epoch: 0424 cost= 0.000061701 accuracy= 1.000\n",
      "Epoch: 0425 cost= 0.000040517 accuracy= 1.000\n",
      "Epoch: 0426 cost= 0.000049937 accuracy= 1.000\n",
      "Epoch: 0427 cost= 0.000039213 accuracy= 1.000\n",
      "Epoch: 0428 cost= 0.000050848 accuracy= 1.000\n",
      "Epoch: 0429 cost= 0.000042210 accuracy= 1.000\n",
      "Epoch: 0430 cost= 0.000060891 accuracy= 1.000\n",
      "Epoch: 0431 cost= 0.000066222 accuracy= 1.000\n",
      "Epoch: 0432 cost= 0.000057951 accuracy= 1.000\n",
      "Epoch: 0433 cost= 0.000048829 accuracy= 1.000\n",
      "Epoch: 0434 cost= 0.000031284 accuracy= 1.000\n",
      "Epoch: 0435 cost= 0.000033961 accuracy= 1.000\n",
      "Epoch: 0436 cost= 0.000060220 accuracy= 1.000\n",
      "Epoch: 0437 cost= 0.000059502 accuracy= 1.000\n",
      "Epoch: 0438 cost= 0.000040339 accuracy= 1.000\n",
      "Epoch: 0439 cost= 0.000034950 accuracy= 1.000\n",
      "Epoch: 0440 cost= 0.000050567 accuracy= 1.000\n",
      "Epoch: 0441 cost= 0.000040876 accuracy= 1.000\n",
      "Epoch: 0442 cost= 0.000025496 accuracy= 1.000\n",
      "Epoch: 0443 cost= 0.000057162 accuracy= 1.000\n",
      "Epoch: 0444 cost= 0.000035628 accuracy= 1.000\n",
      "Epoch: 0445 cost= 0.000048108 accuracy= 1.000\n",
      "Epoch: 0446 cost= 0.000045069 accuracy= 1.000\n",
      "Epoch: 0447 cost= 0.000051982 accuracy= 1.000\n",
      "Epoch: 0448 cost= 0.000040191 accuracy= 1.000\n",
      "Epoch: 0449 cost= 0.000039143 accuracy= 1.000\n",
      "Epoch: 0450 cost= 0.000040088 accuracy= 1.000\n",
      "Epoch: 0451 cost= 0.000033438 accuracy= 1.000\n",
      "Epoch: 0452 cost= 0.000027441 accuracy= 1.000\n",
      "Epoch: 0453 cost= 0.000039369 accuracy= 1.000\n",
      "Epoch: 0454 cost= 0.000052088 accuracy= 1.000\n",
      "Epoch: 0455 cost= 0.000028752 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0456 cost= 0.000045698 accuracy= 1.000\n",
      "Epoch: 0457 cost= 0.000047372 accuracy= 1.000\n",
      "Epoch: 0458 cost= 0.000025160 accuracy= 1.000\n",
      "Epoch: 0459 cost= 0.000035192 accuracy= 1.000\n",
      "Epoch: 0460 cost= 0.000027872 accuracy= 1.000\n",
      "Epoch: 0461 cost= 0.000039868 accuracy= 1.000\n",
      "Epoch: 0462 cost= 0.000030349 accuracy= 1.000\n",
      "Epoch: 0463 cost= 0.000024871 accuracy= 1.000\n",
      "Epoch: 0464 cost= 0.000026259 accuracy= 1.000\n",
      "Epoch: 0465 cost= 0.000023228 accuracy= 1.000\n",
      "Epoch: 0466 cost= 0.000059070 accuracy= 1.000\n",
      "Epoch: 0467 cost= 0.000036102 accuracy= 1.000\n",
      "Epoch: 0468 cost= 0.000034697 accuracy= 1.000\n",
      "Epoch: 0469 cost= 0.000035551 accuracy= 1.000\n",
      "Epoch: 0470 cost= 0.000027644 accuracy= 1.000\n",
      "Epoch: 0471 cost= 0.000038423 accuracy= 1.000\n",
      "Epoch: 0472 cost= 0.000033276 accuracy= 1.000\n",
      "Epoch: 0473 cost= 0.000036072 accuracy= 1.000\n",
      "Epoch: 0474 cost= 0.000021628 accuracy= 1.000\n",
      "Epoch: 0475 cost= 0.000036454 accuracy= 1.000\n",
      "Epoch: 0476 cost= 0.000031240 accuracy= 1.000\n",
      "Epoch: 0477 cost= 0.000025620 accuracy= 1.000\n",
      "Epoch: 0478 cost= 0.000025180 accuracy= 1.000\n",
      "Epoch: 0479 cost= 0.000026039 accuracy= 1.000\n",
      "Epoch: 0480 cost= 0.000031542 accuracy= 1.000\n",
      "Epoch: 0481 cost= 0.000031439 accuracy= 1.000\n",
      "Epoch: 0482 cost= 0.000026692 accuracy= 1.000\n",
      "Epoch: 0483 cost= 0.000037868 accuracy= 1.000\n",
      "Epoch: 0484 cost= 0.000022346 accuracy= 1.000\n",
      "Epoch: 0485 cost= 0.000033771 accuracy= 1.000\n",
      "Epoch: 0486 cost= 0.000021675 accuracy= 1.000\n",
      "Epoch: 0487 cost= 0.000029699 accuracy= 1.000\n",
      "Epoch: 0488 cost= 0.000030691 accuracy= 1.000\n",
      "Epoch: 0489 cost= 0.000031866 accuracy= 1.000\n",
      "Epoch: 0490 cost= 0.000020768 accuracy= 1.000\n",
      "Epoch: 0491 cost= 0.000039772 accuracy= 1.000\n",
      "Epoch: 0492 cost= 0.000030218 accuracy= 1.000\n",
      "Epoch: 0493 cost= 0.000028353 accuracy= 1.000\n",
      "Epoch: 0494 cost= 0.000026052 accuracy= 1.000\n",
      "Epoch: 0495 cost= 0.000027425 accuracy= 1.000\n",
      "Epoch: 0496 cost= 0.000017631 accuracy= 1.000\n",
      "Epoch: 0497 cost= 0.000019741 accuracy= 1.000\n",
      "Epoch: 0498 cost= 0.000020334 accuracy= 1.000\n",
      "Epoch: 0499 cost= 0.000033512 accuracy= 1.000\n",
      "Epoch: 0500 cost= 0.000036365 accuracy= 1.000\n",
      "Epoch: 0501 cost= 0.000035549 accuracy= 1.000\n",
      "Epoch: 0502 cost= 0.000034459 accuracy= 1.000\n",
      "Epoch: 0503 cost= 0.000026703 accuracy= 1.000\n",
      "Epoch: 0504 cost= 0.000028042 accuracy= 1.000\n",
      "Epoch: 0505 cost= 0.000023376 accuracy= 1.000\n",
      "Epoch: 0506 cost= 0.000025950 accuracy= 1.000\n",
      "Epoch: 0507 cost= 0.000023239 accuracy= 1.000\n",
      "Epoch: 0508 cost= 0.000016209 accuracy= 1.000\n",
      "Epoch: 0509 cost= 0.000026008 accuracy= 1.000\n",
      "Epoch: 0510 cost= 0.000020090 accuracy= 1.000\n",
      "Epoch: 0511 cost= 0.000023184 accuracy= 1.000\n",
      "Epoch: 0512 cost= 0.000017230 accuracy= 1.000\n",
      "Epoch: 0513 cost= 0.000018220 accuracy= 1.000\n",
      "Epoch: 0514 cost= 0.000022540 accuracy= 1.000\n",
      "Epoch: 0515 cost= 0.000024002 accuracy= 1.000\n",
      "Epoch: 0516 cost= 0.000021541 accuracy= 1.000\n",
      "Epoch: 0517 cost= 0.000016630 accuracy= 1.000\n",
      "Epoch: 0518 cost= 0.000018936 accuracy= 1.000\n",
      "Epoch: 0519 cost= 0.000024496 accuracy= 1.000\n",
      "Epoch: 0520 cost= 0.000020115 accuracy= 1.000\n",
      "Epoch: 0521 cost= 0.000026346 accuracy= 1.000\n",
      "Epoch: 0522 cost= 0.000017086 accuracy= 1.000\n",
      "Epoch: 0523 cost= 0.000025249 accuracy= 1.000\n",
      "Epoch: 0524 cost= 0.000024356 accuracy= 1.000\n",
      "Epoch: 0525 cost= 0.000019896 accuracy= 1.000\n",
      "Epoch: 0526 cost= 0.000021306 accuracy= 1.000\n",
      "Epoch: 0527 cost= 0.000017083 accuracy= 1.000\n",
      "Epoch: 0528 cost= 0.000020455 accuracy= 1.000\n",
      "Epoch: 0529 cost= 0.000018640 accuracy= 1.000\n",
      "Epoch: 0530 cost= 0.000020048 accuracy= 1.000\n",
      "Epoch: 0531 cost= 0.000024705 accuracy= 1.000\n",
      "Epoch: 0532 cost= 0.000018381 accuracy= 1.000\n",
      "Epoch: 0533 cost= 0.000019412 accuracy= 1.000\n",
      "Epoch: 0534 cost= 0.000016476 accuracy= 1.000\n",
      "Epoch: 0535 cost= 0.000018275 accuracy= 1.000\n",
      "Epoch: 0536 cost= 0.000018814 accuracy= 1.000\n",
      "Epoch: 0537 cost= 0.000018057 accuracy= 1.000\n",
      "Epoch: 0538 cost= 0.000020432 accuracy= 1.000\n",
      "Epoch: 0539 cost= 0.000017635 accuracy= 1.000\n",
      "Epoch: 0540 cost= 0.000016635 accuracy= 1.000\n",
      "Epoch: 0541 cost= 0.000020399 accuracy= 1.000\n",
      "Epoch: 0542 cost= 0.000016260 accuracy= 1.000\n",
      "Epoch: 0543 cost= 0.000019280 accuracy= 1.000\n",
      "Epoch: 0544 cost= 0.000013264 accuracy= 1.000\n",
      "Epoch: 0545 cost= 0.000013789 accuracy= 1.000\n",
      "Epoch: 0546 cost= 0.000016548 accuracy= 1.000\n",
      "Epoch: 0547 cost= 0.000020219 accuracy= 1.000\n",
      "Epoch: 0548 cost= 0.000015543 accuracy= 1.000\n",
      "Epoch: 0549 cost= 0.000012737 accuracy= 1.000\n",
      "Epoch: 0550 cost= 0.000025258 accuracy= 1.000\n",
      "Epoch: 0551 cost= 0.000021875 accuracy= 1.000\n",
      "Epoch: 0552 cost= 0.000015516 accuracy= 1.000\n",
      "Epoch: 0553 cost= 0.000019313 accuracy= 1.000\n",
      "Epoch: 0554 cost= 0.000014164 accuracy= 1.000\n",
      "Epoch: 0555 cost= 0.000017536 accuracy= 1.000\n",
      "Epoch: 0556 cost= 0.000016577 accuracy= 1.000\n",
      "Epoch: 0557 cost= 0.000018595 accuracy= 1.000\n",
      "Epoch: 0558 cost= 0.000014502 accuracy= 1.000\n",
      "Epoch: 0559 cost= 0.000017108 accuracy= 1.000\n",
      "Epoch: 0560 cost= 0.000016353 accuracy= 1.000\n",
      "Epoch: 0561 cost= 0.000019358 accuracy= 1.000\n",
      "Epoch: 0562 cost= 0.000020236 accuracy= 1.000\n",
      "Epoch: 0563 cost= 0.000017065 accuracy= 1.000\n",
      "Epoch: 0564 cost= 0.000014175 accuracy= 1.000\n",
      "Epoch: 0565 cost= 0.000018686 accuracy= 1.000\n",
      "Epoch: 0566 cost= 0.000012537 accuracy= 1.000\n",
      "Epoch: 0567 cost= 0.000010918 accuracy= 1.000\n",
      "Epoch: 0568 cost= 0.000015250 accuracy= 1.000\n",
      "Epoch: 0569 cost= 0.000010612 accuracy= 1.000\n",
      "Epoch: 0570 cost= 0.000016788 accuracy= 1.000\n",
      "Epoch: 0571 cost= 0.000016196 accuracy= 1.000\n",
      "Epoch: 0572 cost= 0.000012326 accuracy= 1.000\n",
      "Epoch: 0573 cost= 0.000011128 accuracy= 1.000\n",
      "Epoch: 0574 cost= 0.000008003 accuracy= 1.000\n",
      "Epoch: 0575 cost= 0.000010613 accuracy= 1.000\n",
      "Epoch: 0576 cost= 0.000015919 accuracy= 1.000\n",
      "Epoch: 0577 cost= 0.000011326 accuracy= 1.000\n",
      "Epoch: 0578 cost= 0.000012962 accuracy= 1.000\n",
      "Epoch: 0579 cost= 0.000014775 accuracy= 1.000\n",
      "Epoch: 0580 cost= 0.000012366 accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_iris_tree1.ckpt\n",
      "Epoch: 0001 cost= 2.280236132 accuracy= 0.156\n",
      "Epoch: 0002 cost= 1.992348790 accuracy= 0.275\n",
      "Epoch: 0003 cost= 1.815157953 accuracy= 0.384\n",
      "Epoch: 0004 cost= 1.715292622 accuracy= 0.401\n",
      "Epoch: 0005 cost= 1.531242988 accuracy= 0.436\n",
      "Epoch: 0006 cost= 1.352158939 accuracy= 0.547\n",
      "Epoch: 0007 cost= 1.147693729 accuracy= 0.639\n",
      "Epoch: 0008 cost= 0.946533957 accuracy= 0.700\n",
      "Epoch: 0009 cost= 0.800971862 accuracy= 0.750\n",
      "Epoch: 0010 cost= 0.672501676 accuracy= 0.771\n",
      "Epoch: 0011 cost= 0.598251790 accuracy= 0.839\n",
      "Epoch: 0012 cost= 0.541089377 accuracy= 0.833\n",
      "Epoch: 0013 cost= 0.491078172 accuracy= 0.851\n",
      "Epoch: 0014 cost= 0.492172734 accuracy= 0.738\n",
      "Epoch: 0015 cost= 0.461735117 accuracy= 0.859\n",
      "Epoch: 0016 cost= 0.409371898 accuracy= 0.836\n",
      "Epoch: 0017 cost= 0.396148640 accuracy= 0.821\n",
      "Epoch: 0018 cost= 0.417932328 accuracy= 0.859\n",
      "Epoch: 0019 cost= 0.441853238 accuracy= 0.876\n",
      "Epoch: 0020 cost= 0.371040895 accuracy= 0.873\n",
      "Epoch: 0021 cost= 0.329585131 accuracy= 0.877\n",
      "Epoch: 0022 cost= 0.312871191 accuracy= 0.895\n",
      "Epoch: 0023 cost= 0.346613764 accuracy= 0.875\n",
      "Epoch: 0024 cost= 0.338665330 accuracy= 0.889\n",
      "Epoch: 0025 cost= 0.298448980 accuracy= 0.897\n",
      "Epoch: 0026 cost= 0.278322945 accuracy= 0.899\n",
      "Epoch: 0027 cost= 0.318736306 accuracy= 0.908\n",
      "Epoch: 0028 cost= 0.355720091 accuracy= 0.861\n",
      "Epoch: 0029 cost= 0.316848935 accuracy= 0.910\n",
      "Epoch: 0030 cost= 0.260368225 accuracy= 0.923\n",
      "Epoch: 0031 cost= 0.230177779 accuracy= 0.924\n",
      "Epoch: 0032 cost= 0.225204584 accuracy= 0.920\n",
      "Epoch: 0033 cost= 0.254322367 accuracy= 0.924\n",
      "Epoch: 0034 cost= 0.227944660 accuracy= 0.935\n",
      "Epoch: 0035 cost= 0.195890185 accuracy= 0.943\n",
      "Epoch: 0036 cost= 0.206882928 accuracy= 0.942\n",
      "Epoch: 0037 cost= 0.189785350 accuracy= 0.925\n",
      "Epoch: 0038 cost= 0.196950028 accuracy= 0.918\n",
      "Epoch: 0039 cost= 0.234273233 accuracy= 0.935\n",
      "Epoch: 0040 cost= 0.189323957 accuracy= 0.950\n",
      "Epoch: 0041 cost= 0.178088811 accuracy= 0.948\n",
      "Epoch: 0042 cost= 0.134552320 accuracy= 0.957\n",
      "Epoch: 0043 cost= 0.123096486 accuracy= 0.962\n",
      "Epoch: 0044 cost= 0.131459808 accuracy= 0.965\n",
      "Epoch: 0045 cost= 0.117501080 accuracy= 0.960\n",
      "Epoch: 0046 cost= 0.133511898 accuracy= 0.956\n",
      "Epoch: 0047 cost= 0.105276388 accuracy= 0.967\n",
      "Epoch: 0048 cost= 0.131341752 accuracy= 0.966\n",
      "Epoch: 0049 cost= 0.104561406 accuracy= 0.953\n",
      "Epoch: 0050 cost= 0.143366018 accuracy= 0.948\n",
      "Epoch: 0051 cost= 0.124194450 accuracy= 0.973\n",
      "Epoch: 0052 cost= 0.098860657 accuracy= 0.960\n",
      "Epoch: 0053 cost= 0.134088496 accuracy= 0.966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0054 cost= 0.081608388 accuracy= 0.968\n",
      "Epoch: 0055 cost= 0.078158164 accuracy= 0.984\n",
      "Epoch: 0056 cost= 0.068330782 accuracy= 0.973\n",
      "Epoch: 0057 cost= 0.095931174 accuracy= 0.982\n",
      "Epoch: 0058 cost= 0.078282670 accuracy= 0.983\n",
      "Epoch: 0059 cost= 0.089112095 accuracy= 0.971\n",
      "Epoch: 0060 cost= 0.066578492 accuracy= 0.983\n",
      "Epoch: 0061 cost= 0.107669971 accuracy= 0.948\n",
      "Epoch: 0062 cost= 0.089355421 accuracy= 0.967\n",
      "Epoch: 0063 cost= 0.094915268 accuracy= 0.969\n",
      "Epoch: 0064 cost= 0.079318046 accuracy= 0.978\n",
      "Epoch: 0065 cost= 0.054327146 accuracy= 0.985\n",
      "Epoch: 0066 cost= 0.056220130 accuracy= 0.987\n",
      "Epoch: 0067 cost= 0.047481527 accuracy= 0.989\n",
      "Epoch: 0068 cost= 0.089276662 accuracy= 0.978\n",
      "Epoch: 0069 cost= 0.071698944 accuracy= 0.980\n",
      "Epoch: 0070 cost= 0.054029373 accuracy= 0.972\n",
      "Epoch: 0071 cost= 0.119288356 accuracy= 0.958\n",
      "Epoch: 0072 cost= 0.070068285 accuracy= 0.981\n",
      "Epoch: 0073 cost= 0.049130113 accuracy= 0.984\n",
      "Epoch: 0074 cost= 0.054424460 accuracy= 0.991\n",
      "Epoch: 0075 cost= 0.036302267 accuracy= 0.986\n",
      "Epoch: 0076 cost= 0.050320226 accuracy= 0.989\n",
      "Epoch: 0077 cost= 0.042987082 accuracy= 0.989\n",
      "Epoch: 0078 cost= 0.029620597 accuracy= 0.987\n",
      "Epoch: 0079 cost= 0.037581267 accuracy= 0.985\n",
      "Epoch: 0080 cost= 0.061061768 accuracy= 0.987\n",
      "Epoch: 0081 cost= 0.040147547 accuracy= 0.989\n",
      "Epoch: 0082 cost= 0.040443882 accuracy= 0.990\n",
      "Epoch: 0083 cost= 0.039046650 accuracy= 0.991\n",
      "Epoch: 0084 cost= 0.032514539 accuracy= 0.989\n",
      "Epoch: 0085 cost= 0.040163574 accuracy= 0.982\n",
      "Epoch: 0086 cost= 0.025923680 accuracy= 0.989\n",
      "Epoch: 0087 cost= 0.035027012 accuracy= 0.988\n",
      "Epoch: 0088 cost= 0.051105235 accuracy= 0.990\n",
      "Epoch: 0089 cost= 0.037375996 accuracy= 0.993\n",
      "Epoch: 0090 cost= 0.021269136 accuracy= 0.989\n",
      "Epoch: 0091 cost= 0.024220822 accuracy= 0.989\n",
      "Epoch: 0092 cost= 0.024017624 accuracy= 0.978\n",
      "Epoch: 0093 cost= 0.035965454 accuracy= 0.990\n",
      "Epoch: 0094 cost= 0.036590584 accuracy= 0.987\n",
      "Epoch: 0095 cost= 0.052430599 accuracy= 0.956\n",
      "Epoch: 0096 cost= 0.044777509 accuracy= 0.989\n",
      "Epoch: 0097 cost= 0.056800035 accuracy= 0.990\n",
      "Epoch: 0098 cost= 0.044798328 accuracy= 0.993\n",
      "Epoch: 0099 cost= 0.043718478 accuracy= 0.989\n",
      "Epoch: 0100 cost= 0.038989263 accuracy= 0.988\n",
      "Epoch: 0101 cost= 0.063882728 accuracy= 0.984\n",
      "Epoch: 0102 cost= 0.076601314 accuracy= 0.991\n",
      "Epoch: 0103 cost= 0.048091731 accuracy= 0.990\n",
      "Epoch: 0104 cost= 0.030809364 accuracy= 0.988\n",
      "Epoch: 0105 cost= 0.015231434 accuracy= 0.994\n",
      "Epoch: 0106 cost= 0.023175964 accuracy= 0.994\n",
      "Epoch: 0107 cost= 0.024733769 accuracy= 0.993\n",
      "Epoch: 0108 cost= 0.021716117 accuracy= 0.995\n",
      "Epoch: 0109 cost= 0.024268624 accuracy= 0.990\n",
      "Epoch: 0110 cost= 0.018009055 accuracy= 0.996\n",
      "Epoch: 0111 cost= 0.017139032 accuracy= 0.996\n",
      "Epoch: 0112 cost= 0.007901596 accuracy= 0.996\n",
      "Epoch: 0113 cost= 0.019039044 accuracy= 0.998\n",
      "Epoch: 0114 cost= 0.014195209 accuracy= 0.994\n",
      "Epoch: 0115 cost= 0.014743600 accuracy= 0.992\n",
      "Epoch: 0116 cost= 0.018481343 accuracy= 0.989\n",
      "Epoch: 0117 cost= 0.026353289 accuracy= 0.975\n",
      "Epoch: 0118 cost= 0.046251313 accuracy= 0.992\n",
      "Epoch: 0119 cost= 0.038906715 accuracy= 0.984\n",
      "Epoch: 0120 cost= 0.044748204 accuracy= 0.978\n",
      "Epoch: 0121 cost= 0.050952551 accuracy= 0.989\n",
      "Epoch: 0122 cost= 0.017642804 accuracy= 0.996\n",
      "Epoch: 0123 cost= 0.019204116 accuracy= 0.992\n",
      "Epoch: 0124 cost= 0.017614147 accuracy= 0.996\n",
      "Epoch: 0125 cost= 0.018580714 accuracy= 0.996\n",
      "Epoch: 0126 cost= 0.013189354 accuracy= 0.998\n",
      "Epoch: 0127 cost= 0.015100178 accuracy= 0.989\n",
      "Epoch: 0128 cost= 0.022924603 accuracy= 0.976\n",
      "Epoch: 0129 cost= 0.047076224 accuracy= 0.987\n",
      "Epoch: 0130 cost= 0.032539106 accuracy= 0.988\n",
      "Epoch: 0131 cost= 0.025652233 accuracy= 0.990\n",
      "Epoch: 0132 cost= 0.021169570 accuracy= 0.995\n",
      "Epoch: 0133 cost= 0.017705754 accuracy= 0.996\n",
      "Epoch: 0134 cost= 0.014716912 accuracy= 0.993\n",
      "Epoch: 0135 cost= 0.012361043 accuracy= 0.997\n",
      "Epoch: 0136 cost= 0.011269276 accuracy= 0.994\n",
      "Epoch: 0137 cost= 0.030807951 accuracy= 0.993\n",
      "Epoch: 0138 cost= 0.025511040 accuracy= 0.996\n",
      "Epoch: 0139 cost= 0.013612873 accuracy= 0.997\n",
      "Epoch: 0140 cost= 0.008073753 accuracy= 0.996\n",
      "Epoch: 0141 cost= 0.010509019 accuracy= 0.991\n",
      "Epoch: 0142 cost= 0.031168680 accuracy= 0.991\n",
      "Epoch: 0143 cost= 0.027883736 accuracy= 0.987\n",
      "Epoch: 0144 cost= 0.026156632 accuracy= 0.984\n",
      "Epoch: 0145 cost= 0.023123591 accuracy= 0.981\n",
      "Epoch: 0146 cost= 0.022587355 accuracy= 0.994\n",
      "Epoch: 0147 cost= 0.020322731 accuracy= 0.988\n",
      "Epoch: 0148 cost= 0.028995623 accuracy= 0.979\n",
      "Epoch: 0149 cost= 0.037671934 accuracy= 0.995\n",
      "Epoch: 0150 cost= 0.029976108 accuracy= 0.993\n",
      "Epoch: 0151 cost= 0.027336649 accuracy= 0.992\n",
      "Epoch: 0152 cost= 0.016691126 accuracy= 0.989\n",
      "Epoch: 0153 cost= 0.030125431 accuracy= 0.984\n",
      "Epoch: 0154 cost= 0.035179848 accuracy= 0.994\n",
      "Epoch: 0155 cost= 0.026352277 accuracy= 0.997\n",
      "Epoch: 0156 cost= 0.012452396 accuracy= 0.987\n",
      "Epoch: 0157 cost= 0.021266107 accuracy= 0.993\n",
      "Epoch: 0158 cost= 0.008164741 accuracy= 0.995\n",
      "Epoch: 0159 cost= 0.007804416 accuracy= 0.997\n",
      "Epoch: 0160 cost= 0.007584306 accuracy= 0.998\n",
      "Epoch: 0161 cost= 0.010307817 accuracy= 0.990\n",
      "Epoch: 0162 cost= 0.030713257 accuracy= 0.989\n",
      "Epoch: 0163 cost= 0.014574243 accuracy= 0.993\n",
      "Epoch: 0164 cost= 0.017747272 accuracy= 0.999\n",
      "Epoch: 0165 cost= 0.012474489 accuracy= 0.996\n",
      "Epoch: 0166 cost= 0.006900371 accuracy= 0.998\n",
      "Epoch: 0167 cost= 0.005464946 accuracy= 0.998\n",
      "Epoch: 0168 cost= 0.002878112 accuracy= 0.999\n",
      "Epoch: 0169 cost= 0.005725917 accuracy= 0.999\n",
      "Epoch: 0170 cost= 0.005913777 accuracy= 0.999\n",
      "Epoch: 0171 cost= 0.013303817 accuracy= 0.985\n",
      "Epoch: 0172 cost= 0.061713623 accuracy= 0.973\n",
      "Epoch: 0173 cost= 0.045314246 accuracy= 0.982\n",
      "Epoch: 0174 cost= 0.046789821 accuracy= 0.995\n",
      "Epoch: 0175 cost= 0.014720335 accuracy= 0.991\n",
      "Epoch: 0176 cost= 0.012578316 accuracy= 0.999\n",
      "Epoch: 0177 cost= 0.016381022 accuracy= 0.996\n",
      "Epoch: 0178 cost= 0.011749860 accuracy= 0.995\n",
      "Epoch: 0179 cost= 0.006768498 accuracy= 0.999\n",
      "Epoch: 0180 cost= 0.003650870 accuracy= 0.999\n",
      "Epoch: 0181 cost= 0.008791598 accuracy= 1.000\n",
      "Epoch: 0182 cost= 0.004347512 accuracy= 0.999\n",
      "Epoch: 0183 cost= 0.002475031 accuracy= 0.999\n",
      "Epoch: 0184 cost= 0.002932012 accuracy= 0.999\n",
      "Epoch: 0185 cost= 0.001907101 accuracy= 1.000\n",
      "Epoch: 0186 cost= 0.001909243 accuracy= 1.000\n",
      "Epoch: 0187 cost= 0.003077030 accuracy= 0.999\n",
      "Epoch: 0188 cost= 0.003190906 accuracy= 1.000\n",
      "Epoch: 0189 cost= 0.001348125 accuracy= 1.000\n",
      "Epoch: 0190 cost= 0.001591728 accuracy= 1.000\n",
      "Epoch: 0191 cost= 0.001216451 accuracy= 0.999\n",
      "Epoch: 0192 cost= 0.002319337 accuracy= 1.000\n",
      "Epoch: 0193 cost= 0.001678835 accuracy= 1.000\n",
      "Epoch: 0194 cost= 0.002330162 accuracy= 1.000\n",
      "Epoch: 0195 cost= 0.001158050 accuracy= 1.000\n",
      "Epoch: 0196 cost= 0.001207311 accuracy= 1.000\n",
      "Epoch: 0197 cost= 0.001084590 accuracy= 1.000\n",
      "Epoch: 0198 cost= 0.001428822 accuracy= 1.000\n",
      "Epoch: 0199 cost= 0.001191249 accuracy= 1.000\n",
      "Epoch: 0200 cost= 0.001041300 accuracy= 1.000\n",
      "Epoch: 0201 cost= 0.000733152 accuracy= 1.000\n",
      "Epoch: 0202 cost= 0.000761543 accuracy= 1.000\n",
      "Epoch: 0203 cost= 0.001561033 accuracy= 1.000\n",
      "Epoch: 0204 cost= 0.000887686 accuracy= 1.000\n",
      "Epoch: 0205 cost= 0.000910806 accuracy= 1.000\n",
      "Epoch: 0206 cost= 0.000967646 accuracy= 1.000\n",
      "Epoch: 0207 cost= 0.001198541 accuracy= 1.000\n",
      "Epoch: 0208 cost= 0.000665147 accuracy= 1.000\n",
      "Epoch: 0209 cost= 0.000551934 accuracy= 1.000\n",
      "Epoch: 0210 cost= 0.000425262 accuracy= 1.000\n",
      "Epoch: 0211 cost= 0.000570468 accuracy= 1.000\n",
      "Epoch: 0212 cost= 0.000499174 accuracy= 1.000\n",
      "Epoch: 0213 cost= 0.000425528 accuracy= 1.000\n",
      "Epoch: 0214 cost= 0.000551243 accuracy= 1.000\n",
      "Epoch: 0215 cost= 0.000419660 accuracy= 1.000\n",
      "Epoch: 0216 cost= 0.000416687 accuracy= 1.000\n",
      "Epoch: 0217 cost= 0.000464515 accuracy= 1.000\n",
      "Epoch: 0218 cost= 0.000435135 accuracy= 1.000\n",
      "Epoch: 0219 cost= 0.000714486 accuracy= 1.000\n",
      "Epoch: 0220 cost= 0.000674257 accuracy= 1.000\n",
      "Epoch: 0221 cost= 0.000512995 accuracy= 1.000\n",
      "Epoch: 0222 cost= 0.000396384 accuracy= 1.000\n",
      "Epoch: 0223 cost= 0.000313744 accuracy= 1.000\n",
      "Epoch: 0224 cost= 0.000219403 accuracy= 1.000\n",
      "Epoch: 0225 cost= 0.000567955 accuracy= 1.000\n",
      "Epoch: 0226 cost= 0.000678879 accuracy= 1.000\n",
      "Epoch: 0227 cost= 0.000585471 accuracy= 1.000\n",
      "Epoch: 0228 cost= 0.000616947 accuracy= 0.999\n",
      "Epoch: 0229 cost= 0.001576712 accuracy= 1.000\n",
      "Epoch: 0230 cost= 0.000927133 accuracy= 1.000\n",
      "Epoch: 0231 cost= 0.000577351 accuracy= 1.000\n",
      "Epoch: 0232 cost= 0.000558695 accuracy= 1.000\n",
      "Epoch: 0233 cost= 0.000526406 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0234 cost= 0.000342105 accuracy= 1.000\n",
      "Epoch: 0235 cost= 0.000421397 accuracy= 1.000\n",
      "Epoch: 0236 cost= 0.000390982 accuracy= 1.000\n",
      "Epoch: 0237 cost= 0.000312842 accuracy= 1.000\n",
      "Epoch: 0238 cost= 0.000394224 accuracy= 1.000\n",
      "Epoch: 0239 cost= 0.000317599 accuracy= 1.000\n",
      "Epoch: 0240 cost= 0.000324640 accuracy= 1.000\n",
      "Epoch: 0241 cost= 0.000296073 accuracy= 1.000\n",
      "Epoch: 0242 cost= 0.000378017 accuracy= 1.000\n",
      "Epoch: 0243 cost= 0.000371972 accuracy= 1.000\n",
      "Epoch: 0244 cost= 0.000275595 accuracy= 1.000\n",
      "Epoch: 0245 cost= 0.000354595 accuracy= 1.000\n",
      "Epoch: 0246 cost= 0.000306131 accuracy= 1.000\n",
      "Epoch: 0247 cost= 0.000272498 accuracy= 1.000\n",
      "Epoch: 0248 cost= 0.000274026 accuracy= 1.000\n",
      "Epoch: 0249 cost= 0.000273204 accuracy= 1.000\n",
      "Epoch: 0250 cost= 0.000263534 accuracy= 1.000\n",
      "Epoch: 0251 cost= 0.000243992 accuracy= 1.000\n",
      "Epoch: 0252 cost= 0.000214107 accuracy= 1.000\n",
      "Epoch: 0253 cost= 0.000225831 accuracy= 1.000\n",
      "Epoch: 0254 cost= 0.000201383 accuracy= 1.000\n",
      "Epoch: 0255 cost= 0.000231809 accuracy= 1.000\n",
      "Epoch: 0256 cost= 0.000364385 accuracy= 1.000\n",
      "Epoch: 0257 cost= 0.000202314 accuracy= 1.000\n",
      "Epoch: 0258 cost= 0.000265938 accuracy= 1.000\n",
      "Epoch: 0259 cost= 0.000201827 accuracy= 1.000\n",
      "Epoch: 0260 cost= 0.000279138 accuracy= 1.000\n",
      "Epoch: 0261 cost= 0.000215264 accuracy= 1.000\n",
      "Epoch: 0262 cost= 0.000207566 accuracy= 1.000\n",
      "Epoch: 0263 cost= 0.000313565 accuracy= 1.000\n",
      "Epoch: 0264 cost= 0.000269052 accuracy= 1.000\n",
      "Epoch: 0265 cost= 0.000187077 accuracy= 1.000\n",
      "Epoch: 0266 cost= 0.000196857 accuracy= 1.000\n",
      "Epoch: 0267 cost= 0.000207782 accuracy= 1.000\n",
      "Epoch: 0268 cost= 0.000192546 accuracy= 1.000\n",
      "Epoch: 0269 cost= 0.000201435 accuracy= 1.000\n",
      "Epoch: 0270 cost= 0.000208361 accuracy= 1.000\n",
      "Epoch: 0271 cost= 0.000128979 accuracy= 1.000\n",
      "Epoch: 0272 cost= 0.000256866 accuracy= 1.000\n",
      "Epoch: 0273 cost= 0.000208129 accuracy= 1.000\n",
      "Epoch: 0274 cost= 0.000202552 accuracy= 1.000\n",
      "Epoch: 0275 cost= 0.000276659 accuracy= 1.000\n",
      "Epoch: 0276 cost= 0.000193502 accuracy= 1.000\n",
      "Epoch: 0277 cost= 0.000155855 accuracy= 1.000\n",
      "Epoch: 0278 cost= 0.000170722 accuracy= 1.000\n",
      "Epoch: 0279 cost= 0.000206678 accuracy= 1.000\n",
      "Epoch: 0280 cost= 0.000235255 accuracy= 1.000\n",
      "Epoch: 0281 cost= 0.000139618 accuracy= 1.000\n",
      "Epoch: 0282 cost= 0.000134099 accuracy= 1.000\n",
      "Epoch: 0283 cost= 0.000175341 accuracy= 1.000\n",
      "Epoch: 0284 cost= 0.000197798 accuracy= 1.000\n",
      "Epoch: 0285 cost= 0.000185589 accuracy= 1.000\n",
      "Epoch: 0286 cost= 0.000191904 accuracy= 1.000\n",
      "Epoch: 0287 cost= 0.000134485 accuracy= 1.000\n",
      "Epoch: 0288 cost= 0.000151692 accuracy= 1.000\n",
      "Epoch: 0289 cost= 0.000185813 accuracy= 1.000\n",
      "Epoch: 0290 cost= 0.000136860 accuracy= 1.000\n",
      "Epoch: 0291 cost= 0.000164834 accuracy= 1.000\n",
      "Epoch: 0292 cost= 0.000127578 accuracy= 1.000\n",
      "Epoch: 0293 cost= 0.000175870 accuracy= 1.000\n",
      "Epoch: 0294 cost= 0.000111536 accuracy= 1.000\n",
      "Epoch: 0295 cost= 0.000197218 accuracy= 1.000\n",
      "Epoch: 0296 cost= 0.000266757 accuracy= 1.000\n",
      "Epoch: 0297 cost= 0.000151927 accuracy= 1.000\n",
      "Epoch: 0298 cost= 0.000182412 accuracy= 1.000\n",
      "Epoch: 0299 cost= 0.000132477 accuracy= 1.000\n",
      "Epoch: 0300 cost= 0.000098789 accuracy= 1.000\n",
      "Epoch: 0301 cost= 0.000147296 accuracy= 1.000\n",
      "Epoch: 0302 cost= 0.000129769 accuracy= 1.000\n",
      "Epoch: 0303 cost= 0.000107874 accuracy= 1.000\n",
      "Epoch: 0304 cost= 0.000132404 accuracy= 1.000\n",
      "Epoch: 0305 cost= 0.000144372 accuracy= 1.000\n",
      "Epoch: 0306 cost= 0.000121658 accuracy= 1.000\n",
      "Epoch: 0307 cost= 0.000124502 accuracy= 1.000\n",
      "Epoch: 0308 cost= 0.000123332 accuracy= 1.000\n",
      "Epoch: 0309 cost= 0.000111699 accuracy= 1.000\n",
      "Epoch: 0310 cost= 0.000128569 accuracy= 1.000\n",
      "Epoch: 0311 cost= 0.000138111 accuracy= 1.000\n",
      "Epoch: 0312 cost= 0.000084587 accuracy= 1.000\n",
      "Epoch: 0313 cost= 0.000128747 accuracy= 1.000\n",
      "Epoch: 0314 cost= 0.000083513 accuracy= 1.000\n",
      "Epoch: 0315 cost= 0.000142142 accuracy= 1.000\n",
      "Epoch: 0316 cost= 0.000168911 accuracy= 1.000\n",
      "Epoch: 0317 cost= 0.000104185 accuracy= 1.000\n",
      "Epoch: 0318 cost= 0.000106666 accuracy= 1.000\n",
      "Epoch: 0319 cost= 0.000077319 accuracy= 1.000\n",
      "Epoch: 0320 cost= 0.000107402 accuracy= 1.000\n",
      "Epoch: 0321 cost= 0.000078335 accuracy= 1.000\n",
      "Epoch: 0322 cost= 0.000111076 accuracy= 1.000\n",
      "Epoch: 0323 cost= 0.000077166 accuracy= 1.000\n",
      "Epoch: 0324 cost= 0.000120472 accuracy= 1.000\n",
      "Epoch: 0325 cost= 0.000093295 accuracy= 1.000\n",
      "Epoch: 0326 cost= 0.000144335 accuracy= 1.000\n",
      "Epoch: 0327 cost= 0.000103999 accuracy= 1.000\n",
      "Epoch: 0328 cost= 0.000072504 accuracy= 1.000\n",
      "Epoch: 0329 cost= 0.000110699 accuracy= 1.000\n",
      "Epoch: 0330 cost= 0.000091226 accuracy= 1.000\n",
      "Epoch: 0331 cost= 0.000110717 accuracy= 1.000\n",
      "Epoch: 0332 cost= 0.000094163 accuracy= 1.000\n",
      "Epoch: 0333 cost= 0.000098880 accuracy= 1.000\n",
      "Epoch: 0334 cost= 0.000065744 accuracy= 1.000\n",
      "Epoch: 0335 cost= 0.000094467 accuracy= 1.000\n",
      "Epoch: 0336 cost= 0.000090534 accuracy= 1.000\n",
      "Epoch: 0337 cost= 0.000069405 accuracy= 1.000\n",
      "Epoch: 0338 cost= 0.000077240 accuracy= 1.000\n",
      "Epoch: 0339 cost= 0.000089226 accuracy= 1.000\n",
      "Epoch: 0340 cost= 0.000065584 accuracy= 1.000\n",
      "Epoch: 0341 cost= 0.000062828 accuracy= 1.000\n",
      "Epoch: 0342 cost= 0.000094661 accuracy= 1.000\n",
      "Epoch: 0343 cost= 0.000068460 accuracy= 1.000\n",
      "Epoch: 0344 cost= 0.000099696 accuracy= 1.000\n",
      "Epoch: 0345 cost= 0.000079540 accuracy= 1.000\n",
      "Epoch: 0346 cost= 0.000087065 accuracy= 1.000\n",
      "Epoch: 0347 cost= 0.000075364 accuracy= 1.000\n",
      "Epoch: 0348 cost= 0.000065603 accuracy= 1.000\n",
      "Epoch: 0349 cost= 0.000069444 accuracy= 1.000\n",
      "Epoch: 0350 cost= 0.000096589 accuracy= 1.000\n",
      "Epoch: 0351 cost= 0.000080206 accuracy= 1.000\n",
      "Epoch: 0352 cost= 0.000069779 accuracy= 1.000\n",
      "Epoch: 0353 cost= 0.000059453 accuracy= 1.000\n",
      "Epoch: 0354 cost= 0.000072030 accuracy= 1.000\n",
      "Epoch: 0355 cost= 0.000064587 accuracy= 1.000\n",
      "Epoch: 0356 cost= 0.000070538 accuracy= 1.000\n",
      "Epoch: 0357 cost= 0.000070929 accuracy= 1.000\n",
      "Epoch: 0358 cost= 0.000075349 accuracy= 1.000\n",
      "Epoch: 0359 cost= 0.000074951 accuracy= 1.000\n",
      "Epoch: 0360 cost= 0.000069125 accuracy= 1.000\n",
      "Epoch: 0361 cost= 0.000063545 accuracy= 1.000\n",
      "Epoch: 0362 cost= 0.000084137 accuracy= 1.000\n",
      "Epoch: 0363 cost= 0.000046106 accuracy= 1.000\n",
      "Epoch: 0364 cost= 0.000058334 accuracy= 1.000\n",
      "Epoch: 0365 cost= 0.000052547 accuracy= 1.000\n",
      "Epoch: 0366 cost= 0.000050256 accuracy= 1.000\n",
      "Epoch: 0367 cost= 0.000080328 accuracy= 1.000\n",
      "Epoch: 0368 cost= 0.000068088 accuracy= 1.000\n",
      "Epoch: 0369 cost= 0.000063085 accuracy= 1.000\n",
      "Epoch: 0370 cost= 0.000077924 accuracy= 1.000\n",
      "Epoch: 0371 cost= 0.000071827 accuracy= 1.000\n",
      "Epoch: 0372 cost= 0.000065842 accuracy= 1.000\n",
      "Epoch: 0373 cost= 0.000057547 accuracy= 1.000\n",
      "Epoch: 0374 cost= 0.000053960 accuracy= 1.000\n",
      "Epoch: 0375 cost= 0.000060236 accuracy= 1.000\n",
      "Epoch: 0376 cost= 0.000036082 accuracy= 1.000\n",
      "Epoch: 0377 cost= 0.000035640 accuracy= 1.000\n",
      "Epoch: 0378 cost= 0.000065582 accuracy= 1.000\n",
      "Epoch: 0379 cost= 0.000056349 accuracy= 1.000\n",
      "Epoch: 0380 cost= 0.000038457 accuracy= 1.000\n",
      "Epoch: 0381 cost= 0.000040713 accuracy= 1.000\n",
      "Epoch: 0382 cost= 0.000051393 accuracy= 1.000\n",
      "Epoch: 0383 cost= 0.000049111 accuracy= 1.000\n",
      "Epoch: 0384 cost= 0.000052708 accuracy= 1.000\n",
      "Epoch: 0385 cost= 0.000051618 accuracy= 1.000\n",
      "Epoch: 0386 cost= 0.000049841 accuracy= 1.000\n",
      "Epoch: 0387 cost= 0.000040705 accuracy= 1.000\n",
      "Epoch: 0388 cost= 0.000048847 accuracy= 1.000\n",
      "Epoch: 0389 cost= 0.000054098 accuracy= 1.000\n",
      "Epoch: 0390 cost= 0.000047954 accuracy= 1.000\n",
      "Epoch: 0391 cost= 0.000041674 accuracy= 1.000\n",
      "Epoch: 0392 cost= 0.000061649 accuracy= 1.000\n",
      "Epoch: 0393 cost= 0.000043246 accuracy= 1.000\n",
      "Epoch: 0394 cost= 0.000056726 accuracy= 1.000\n",
      "Epoch: 0395 cost= 0.000055869 accuracy= 1.000\n",
      "Epoch: 0396 cost= 0.000056554 accuracy= 1.000\n",
      "Epoch: 0397 cost= 0.000047099 accuracy= 1.000\n",
      "Epoch: 0398 cost= 0.000038419 accuracy= 1.000\n",
      "Epoch: 0399 cost= 0.000046688 accuracy= 1.000\n",
      "Epoch: 0400 cost= 0.000051842 accuracy= 1.000\n",
      "Epoch: 0401 cost= 0.000047110 accuracy= 1.000\n",
      "Epoch: 0402 cost= 0.000030768 accuracy= 1.000\n",
      "Epoch: 0403 cost= 0.000043119 accuracy= 1.000\n",
      "Epoch: 0404 cost= 0.000030568 accuracy= 1.000\n",
      "Epoch: 0405 cost= 0.000047348 accuracy= 1.000\n",
      "Epoch: 0406 cost= 0.000043421 accuracy= 1.000\n",
      "Epoch: 0407 cost= 0.000044586 accuracy= 1.000\n",
      "Epoch: 0408 cost= 0.000044915 accuracy= 1.000\n",
      "Epoch: 0409 cost= 0.000041118 accuracy= 1.000\n",
      "Epoch: 0410 cost= 0.000034758 accuracy= 1.000\n",
      "Epoch: 0411 cost= 0.000040953 accuracy= 1.000\n",
      "Epoch: 0412 cost= 0.000033590 accuracy= 1.000\n",
      "Epoch: 0413 cost= 0.000030728 accuracy= 1.000\n",
      "Epoch: 0414 cost= 0.000037935 accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0415 cost= 0.000038964 accuracy= 1.000\n",
      "Epoch: 0416 cost= 0.000039487 accuracy= 1.000\n",
      "Epoch: 0417 cost= 0.000041936 accuracy= 1.000\n",
      "Epoch: 0418 cost= 0.000030721 accuracy= 1.000\n",
      "Epoch: 0419 cost= 0.000039175 accuracy= 1.000\n",
      "Epoch: 0420 cost= 0.000034530 accuracy= 1.000\n",
      "Epoch: 0421 cost= 0.000036961 accuracy= 1.000\n",
      "Epoch: 0422 cost= 0.000034304 accuracy= 1.000\n",
      "Epoch: 0423 cost= 0.000027135 accuracy= 1.000\n",
      "Epoch: 0424 cost= 0.000033278 accuracy= 1.000\n",
      "Epoch: 0425 cost= 0.000034572 accuracy= 1.000\n",
      "Epoch: 0426 cost= 0.000036739 accuracy= 1.000\n",
      "Epoch: 0427 cost= 0.000033563 accuracy= 1.000\n",
      "Epoch: 0428 cost= 0.000027117 accuracy= 1.000\n",
      "Epoch: 0429 cost= 0.000027606 accuracy= 1.000\n",
      "Epoch: 0430 cost= 0.000031432 accuracy= 1.000\n",
      "Epoch: 0431 cost= 0.000027846 accuracy= 1.000\n",
      "Epoch: 0432 cost= 0.000024434 accuracy= 1.000\n",
      "Epoch: 0433 cost= 0.000029210 accuracy= 1.000\n",
      "Epoch: 0434 cost= 0.000027195 accuracy= 1.000\n",
      "Epoch: 0435 cost= 0.000032410 accuracy= 1.000\n",
      "Epoch: 0436 cost= 0.000036590 accuracy= 1.000\n",
      "Epoch: 0437 cost= 0.000036207 accuracy= 1.000\n",
      "Epoch: 0438 cost= 0.000024929 accuracy= 1.000\n",
      "Epoch: 0439 cost= 0.000031589 accuracy= 1.000\n",
      "Epoch: 0440 cost= 0.000029776 accuracy= 1.000\n",
      "Epoch: 0441 cost= 0.000019627 accuracy= 1.000\n",
      "Epoch: 0442 cost= 0.000024850 accuracy= 1.000\n",
      "Epoch: 0443 cost= 0.000021201 accuracy= 1.000\n",
      "Epoch: 0444 cost= 0.000026364 accuracy= 1.000\n",
      "Epoch: 0445 cost= 0.000030974 accuracy= 1.000\n",
      "Epoch: 0446 cost= 0.000021847 accuracy= 1.000\n",
      "Epoch: 0447 cost= 0.000020526 accuracy= 1.000\n",
      "Epoch: 0448 cost= 0.000025466 accuracy= 1.000\n",
      "Epoch: 0449 cost= 0.000024029 accuracy= 1.000\n",
      "Epoch: 0450 cost= 0.000025288 accuracy= 1.000\n",
      "Epoch: 0451 cost= 0.000026341 accuracy= 1.000\n",
      "Epoch: 0452 cost= 0.000031503 accuracy= 1.000\n",
      "Epoch: 0453 cost= 0.000014392 accuracy= 1.000\n",
      "Epoch: 0454 cost= 0.000031297 accuracy= 1.000\n",
      "Epoch: 0455 cost= 0.000021914 accuracy= 1.000\n",
      "Epoch: 0456 cost= 0.000018217 accuracy= 1.000\n",
      "Epoch: 0457 cost= 0.000019145 accuracy= 1.000\n",
      "Epoch: 0458 cost= 0.000025112 accuracy= 1.000\n",
      "Epoch: 0459 cost= 0.000019046 accuracy= 1.000\n",
      "Epoch: 0460 cost= 0.000020597 accuracy= 1.000\n",
      "Epoch: 0461 cost= 0.000023984 accuracy= 1.000\n",
      "Epoch: 0462 cost= 0.000024871 accuracy= 1.000\n",
      "Epoch: 0463 cost= 0.000019383 accuracy= 1.000\n",
      "Epoch: 0464 cost= 0.000021523 accuracy= 1.000\n",
      "Epoch: 0465 cost= 0.000018145 accuracy= 1.000\n",
      "Epoch: 0466 cost= 0.000018276 accuracy= 1.000\n",
      "Epoch: 0467 cost= 0.000014999 accuracy= 1.000\n",
      "Epoch: 0468 cost= 0.000022590 accuracy= 1.000\n",
      "Epoch: 0469 cost= 0.000018911 accuracy= 1.000\n",
      "Epoch: 0470 cost= 0.000018734 accuracy= 1.000\n",
      "Epoch: 0471 cost= 0.000026385 accuracy= 1.000\n",
      "Epoch: 0472 cost= 0.000016972 accuracy= 1.000\n",
      "Epoch: 0473 cost= 0.000022983 accuracy= 1.000\n",
      "Epoch: 0474 cost= 0.000013468 accuracy= 1.000\n",
      "Epoch: 0475 cost= 0.000022463 accuracy= 1.000\n",
      "Epoch: 0476 cost= 0.000019887 accuracy= 1.000\n",
      "Epoch: 0477 cost= 0.000018038 accuracy= 1.000\n",
      "Epoch: 0478 cost= 0.000015633 accuracy= 1.000\n",
      "Epoch: 0479 cost= 0.000019097 accuracy= 1.000\n",
      "Epoch: 0480 cost= 0.000017587 accuracy= 1.000\n",
      "Epoch: 0481 cost= 0.000017536 accuracy= 1.000\n",
      "Epoch: 0482 cost= 0.000022256 accuracy= 1.000\n",
      "Epoch: 0483 cost= 0.000018905 accuracy= 1.000\n",
      "Epoch: 0484 cost= 0.000017147 accuracy= 1.000\n",
      "Epoch: 0485 cost= 0.000015724 accuracy= 1.000\n",
      "Epoch: 0486 cost= 0.000016003 accuracy= 1.000\n",
      "Epoch: 0487 cost= 0.000016555 accuracy= 1.000\n",
      "Epoch: 0488 cost= 0.000012800 accuracy= 1.000\n",
      "Epoch: 0489 cost= 0.000018494 accuracy= 1.000\n",
      "Epoch: 0490 cost= 0.000015562 accuracy= 1.000\n",
      "Epoch: 0491 cost= 0.000014603 accuracy= 1.000\n",
      "Epoch: 0492 cost= 0.000019008 accuracy= 1.000\n",
      "Epoch: 0493 cost= 0.000014292 accuracy= 1.000\n",
      "Epoch: 0494 cost= 0.000014961 accuracy= 1.000\n",
      "Epoch: 0495 cost= 0.000015427 accuracy= 1.000\n",
      "Epoch: 0496 cost= 0.000014851 accuracy= 1.000\n",
      "Epoch: 0497 cost= 0.000016182 accuracy= 1.000\n",
      "Epoch: 0498 cost= 0.000017733 accuracy= 1.000\n",
      "Epoch: 0499 cost= 0.000015497 accuracy= 1.000\n",
      "Epoch: 0500 cost= 0.000010325 accuracy= 1.000\n",
      "Epoch: 0501 cost= 0.000015935 accuracy= 1.000\n",
      "Epoch: 0502 cost= 0.000012352 accuracy= 1.000\n",
      "Epoch: 0503 cost= 0.000015356 accuracy= 1.000\n",
      "Epoch: 0504 cost= 0.000017939 accuracy= 1.000\n",
      "Epoch: 0505 cost= 0.000013170 accuracy= 1.000\n",
      "Epoch: 0506 cost= 0.000018191 accuracy= 1.000\n",
      "Epoch: 0507 cost= 0.000017458 accuracy= 1.000\n",
      "Epoch: 0508 cost= 0.000014525 accuracy= 1.000\n",
      "Epoch: 0509 cost= 0.000013382 accuracy= 1.000\n",
      "Epoch: 0510 cost= 0.000012656 accuracy= 1.000\n",
      "Epoch: 0511 cost= 0.000014422 accuracy= 1.000\n",
      "Epoch: 0512 cost= 0.000013048 accuracy= 1.000\n",
      "Epoch: 0513 cost= 0.000011126 accuracy= 1.000\n",
      "Epoch: 0514 cost= 0.000017672 accuracy= 1.000\n",
      "Epoch: 0515 cost= 0.000012054 accuracy= 1.000\n",
      "Epoch: 0516 cost= 0.000015294 accuracy= 1.000\n",
      "Epoch: 0517 cost= 0.000016480 accuracy= 1.000\n",
      "Epoch: 0518 cost= 0.000016704 accuracy= 1.000\n",
      "Epoch: 0519 cost= 0.000011096 accuracy= 1.000\n",
      "Epoch: 0520 cost= 0.000012855 accuracy= 1.000\n",
      "Epoch: 0521 cost= 0.000013709 accuracy= 1.000\n",
      "Epoch: 0522 cost= 0.000013303 accuracy= 1.000\n",
      "Epoch: 0523 cost= 0.000013857 accuracy= 1.000\n",
      "Epoch: 0524 cost= 0.000010418 accuracy= 1.000\n",
      "Epoch: 0525 cost= 0.000012501 accuracy= 1.000\n",
      "Epoch: 0526 cost= 0.000013948 accuracy= 1.000\n",
      "Epoch: 0527 cost= 0.000012105 accuracy= 1.000\n",
      "Epoch: 0528 cost= 0.000011882 accuracy= 1.000\n",
      "Epoch: 0529 cost= 0.000012038 accuracy= 1.000\n",
      "Epoch: 0530 cost= 0.000011053 accuracy= 1.000\n",
      "Epoch: 0531 cost= 0.000012397 accuracy= 1.000\n",
      "Epoch: 0532 cost= 0.000013738 accuracy= 1.000\n",
      "Epoch: 0533 cost= 0.000009649 accuracy= 1.000\n",
      "Epoch: 0534 cost= 0.000012592 accuracy= 1.000\n",
      "Epoch: 0535 cost= 0.000011584 accuracy= 1.000\n",
      "Epoch: 0536 cost= 0.000011255 accuracy= 1.000\n",
      "Epoch: 0537 cost= 0.000010627 accuracy= 1.000\n",
      "Epoch: 0538 cost= 0.000011901 accuracy= 1.000\n",
      "Epoch: 0539 cost= 0.000010544 accuracy= 1.000\n",
      "Epoch: 0540 cost= 0.000007934 accuracy= 1.000\n",
      "Epoch: 0541 cost= 0.000012896 accuracy= 1.000\n",
      "Epoch: 0542 cost= 0.000011135 accuracy= 1.000\n",
      "Epoch: 0543 cost= 0.000007089 accuracy= 1.000\n",
      "Epoch: 0544 cost= 0.000014119 accuracy= 1.000\n",
      "Epoch: 0545 cost= 0.000011560 accuracy= 1.000\n",
      "Epoch: 0546 cost= 0.000009463 accuracy= 1.000\n",
      "Epoch: 0547 cost= 0.000010271 accuracy= 1.000\n",
      "Epoch: 0548 cost= 0.000011960 accuracy= 1.000\n",
      "Epoch: 0549 cost= 0.000009305 accuracy= 1.000\n",
      "Epoch: 0550 cost= 0.000008842 accuracy= 1.000\n",
      "Epoch: 0551 cost= 0.000011111 accuracy= 1.000\n",
      "Epoch: 0552 cost= 0.000008609 accuracy= 1.000\n",
      "Epoch: 0553 cost= 0.000007588 accuracy= 1.000\n",
      "Epoch: 0554 cost= 0.000010926 accuracy= 1.000\n",
      "Epoch: 0555 cost= 0.000008226 accuracy= 1.000\n",
      "Epoch: 0556 cost= 0.000008223 accuracy= 1.000\n",
      "Epoch: 0557 cost= 0.000007332 accuracy= 1.000\n",
      "Epoch: 0558 cost= 0.000010970 accuracy= 1.000\n",
      "Epoch: 0559 cost= 0.000009673 accuracy= 1.000\n",
      "Epoch: 0560 cost= 0.000010376 accuracy= 1.000\n",
      "Epoch: 0561 cost= 0.000010077 accuracy= 1.000\n",
      "Epoch: 0562 cost= 0.000009700 accuracy= 1.000\n",
      "Epoch: 0563 cost= 0.000009280 accuracy= 1.000\n",
      "Epoch: 0564 cost= 0.000007505 accuracy= 1.000\n",
      "Epoch: 0565 cost= 0.000009346 accuracy= 1.000\n",
      "Epoch: 0566 cost= 0.000008839 accuracy= 1.000\n",
      "Epoch: 0567 cost= 0.000007670 accuracy= 1.000\n",
      "Epoch: 0568 cost= 0.000008334 accuracy= 1.000\n",
      "Epoch: 0569 cost= 0.000009496 accuracy= 1.000\n",
      "Epoch: 0570 cost= 0.000008421 accuracy= 1.000\n",
      "Epoch: 0571 cost= 0.000009870 accuracy= 1.000\n",
      "Epoch: 0572 cost= 0.000009920 accuracy= 1.000\n",
      "Epoch: 0573 cost= 0.000008897 accuracy= 1.000\n",
      "Epoch: 0574 cost= 0.000007708 accuracy= 1.000\n",
      "Epoch: 0575 cost= 0.000008451 accuracy= 1.000\n",
      "Epoch: 0576 cost= 0.000007124 accuracy= 1.000\n",
      "Epoch: 0577 cost= 0.000007589 accuracy= 1.000\n",
      "Epoch: 0578 cost= 0.000009002 accuracy= 1.000\n",
      "Epoch: 0579 cost= 0.000008596 accuracy= 1.000\n",
      "Epoch: 0580 cost= 0.000010857 accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_iris_tree2.ckpt\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn.py:276: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_iris_tree0.ckpt\n",
      "Model 0 restored\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_iris_tree1.ckpt\n",
      "Model 1 restored\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_iris_tree2.ckpt\n",
      "Model 2 restored\n",
      "(3, 540, 10)\n"
     ]
    }
   ],
   "source": [
    "from djinn import djinn\n",
    "\n",
    "print(\"djinn iris\")    \n",
    "modelname=\"class_djinn_iris\"   # name the model\n",
    "ntrees=3                 # number of trees = number of neural nets in ensemble\n",
    "maxdepth=4               # max depth of tree -- optimize this for each data set\n",
    "dropout_keep=1.0 \n",
    "\n",
    "#initialize the model\n",
    "model=djinn.DJINN_Classifier(ntrees,maxdepth,dropout_keep)\n",
    "\n",
    "x_train, y_train, x_test, y_test = out_cnn_train, y_train, out_cnn_test, y_test \n",
    "\n",
    "# find optimal settings: this function returns dict with hyper-parameters\n",
    "# each djinn function accepts random seeds for reproducible behavior\n",
    "# optimal=model.get_hyperparameters(x_train, y_train, random_state=1)\n",
    "# batchsize=optimal['batch_size']\n",
    "# learnrate=optimal['learn_rate']\n",
    "#epochs=optimal['epochs']\n",
    "epochs=580\n",
    "learnrate=0.004\n",
    "batchsize=63\n",
    "\n",
    "# train the model with hyperparameters determined above\n",
    "model.train(x_train,y_train,epochs=epochs,learn_rate=learnrate, batch_size=batchsize, \n",
    "              display_step=1, save_files=True, file_name=modelname, \n",
    "              save_model=True,model_name=modelname, random_state=1)\n",
    "\n",
    "# *note there is a function model.fit(x_train,y_train, ... ) that wraps \n",
    "# get_hyperparameters() and train(), so that you do not have to manually\n",
    "# pass hyperparameters to train(). However, get_hyperparameters() can\n",
    "# be expensive, so I recommend running it once per dataset and using those\n",
    "# hyperparameter values in train() to save computational time\n",
    "# make predictions\n",
    "m=model.predict(x_test) #returns the median prediction if more than one tree\n",
    "\n",
    "import sklearn\n",
    "#evaluate results\n",
    "acc=sklearn.metrics.accuracy_score(y_test,m.flatten())  \n",
    "#close model \n",
    "model.close_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + ( SVM, XGB, DTree, ExtraTrees, RandomFores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to Random Forest Classifier to predict its class\n",
    "predictions = rf.predict(out_cnn_test)\n",
    "accuracy_CNN_RF=accuracy_score(predictions , y_test)\n",
    "#print('CNN+RF : Accuracy:', accuracy_CNN_RF, '%.')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "Ext = ExtraTreesClassifier(n_estimators=100)\n",
    "Ext.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictions = Ext.predict(out_cnn_test)\n",
    "accuracy_CNN_Ext=accuracy_score(predictions , y_test)\n",
    "#print('CNN+Extrat : Accuracy:', accuracy_CNN_Ext, '%.')\n",
    "\n",
    "\n",
    "#Applying SVC (Support Vector Classification)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "svm.fit(out_cnn_train, y_train)\n",
    "#print('The accuracy of the SVM classifier on training data is {:.4f}'.format(svm.score(x_train, y_train)))\n",
    "\n",
    "\n",
    "#Applying XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf = xgb_clf.fit(out_cnn_train, y_train)\n",
    "#print('The accuracy of the XGBoost classifier on training data is {:.4f}'.format(xgb_clf.score(x_train, y_train)))\n",
    "\n",
    "\n",
    "#Applying Decision Tree\n",
    "from sklearn import tree\n",
    "#Create tree object\n",
    "decision_tree = tree.DecisionTreeClassifier()\n",
    "#Train DT based on scaled training set\n",
    "decision_tree.fit(out_cnn_train, y_train)\n",
    "#Print performance\n",
    "#print('The accuracy of the Decision Tree classifier on training data is {:.4f}'.format(decision_tree.score(x_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 10-fold cross-validation with the best KNN model\n",
    "# This will allow us to get a better results\n",
    "cx_train = np.concatenate((x_train, x_test), 0)\n",
    "cy_train = np.concatenate((y_train, y_test), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by RandomForest, ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier : from dataset originl\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train_, y_train_)\n",
    "predictions = rf.predict(x_test_)\n",
    "accuracy_RF=accuracy_score(predictions , y_test_)\n",
    "#print('RF : Accuracy:', accuracy_RF, '%.')\n",
    "\n",
    "# ExtraTreesClassifier : from dataset originl\n",
    "Extra = ExtraTreesClassifier(n_estimators=100)\n",
    "Extra.fit(x_train_, y_train_)\n",
    "predictions = Extra.predict(x_test_)\n",
    "accuracy_Extra=accuracy_score(predictions , y_test_)\n",
    "#print('Extra : Accuracy:', accuracy_Extra, '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RF          :: 0.9796 %.\n",
      "Accuracy Extrat      :: 0.9815 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN         :: 0.9741 %.\n",
      "Accuracy CNN+RF      :: 0.9759 %.\n",
      "Accuracy CNN+Extrat  :: 0.9833 %.\n",
      "Accuracy CNN+SVM     :: 0.2926 %.\n",
      "Accuracy CNN+XGBoost :: 0.9759 %.\n",
      "Accuracy CNN+DTree   :: 0.9426 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN+RF+MLP  :: 0.9778 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN+SVM using cv=10     :: 0.5415 %.\n",
      "Accuracy CNN+rf  using cv=10     :: 0.9866 %.\n",
      "Accuracy CNN+XGBoost using cv=10 :: 0.9811 %.\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy RF          ::',  \"{:.4f}\".format(accuracy_RF),'%.')\n",
    "print('Accuracy Extrat      ::',  \"{:.4f}\".format(accuracy_Extra),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN         ::',  \"{:.4f}\".format(accuracy_CNN), '%.')\n",
    "print('Accuracy CNN+RF      ::',  \"{:.4f}\".format(accuracy_CNN_RF), '%.')\n",
    "print('Accuracy CNN+Extrat  ::',  \"{:.4f}\".format(accuracy_CNN_Ext), '%.')\n",
    "print('Accuracy CNN+SVM     :: {:.4f}'.format(svm.score(x_test, y_test)),'%.')\n",
    "print('Accuracy CNN+XGBoost :: {:.4f}'.format(xgb_clf.score(x_test, y_test)),'%.')\n",
    "print('Accuracy CNN+DTree   :: {:.4f}'.format(decision_tree.score(x_test, y_test)),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN+RF+MLP  ::',  \"{:.4f}\".format(acc),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN+SVM using cv=10     :: {:.4f}' .format(cross_val_score(svm, cx_train, cy_train, cv=10, scoring='accuracy').mean()),'%.')\n",
    "print('Accuracy CNN+rf  using cv=10     :: {:.4f}' .format(cross_val_score(rf, cx_train, cy_train, cv=10, scoring='accuracy').mean() ),'%.')\n",
    "print('Accuracy CNN+XGBoost using cv=10 :: {:.4f}'.format(cross_val_score(xgb_clf, cx_train, cy_train, cv=10, scoring='accuracy').mean() ),'%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/38957/keras-conv1d-for-simple-data-target-prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
