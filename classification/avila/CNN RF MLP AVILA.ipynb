{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "try: from sklearn.model_selection import train_test_split\n",
    "except: from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the train & test and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('avila.txt', sep=\",\", header=None)\n",
    "df.columns = [\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266074</td>\n",
       "      <td>-0.165620</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.483299</td>\n",
       "      <td>0.172340</td>\n",
       "      <td>0.273364</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.251173</td>\n",
       "      <td>0.159345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.870736</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>1.436060</td>\n",
       "      <td>1.465940</td>\n",
       "      <td>0.636203</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.515587</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116585</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.068476</td>\n",
       "      <td>-0.783147</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.439463</td>\n",
       "      <td>-0.081827</td>\n",
       "      <td>-0.888236</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>0.582939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>-0.583590</td>\n",
       "      <td>-0.721442</td>\n",
       "      <td>-0.307984</td>\n",
       "      <td>0.710932</td>\n",
       "      <td>1.051693</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>-0.533994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229043</td>\n",
       "      <td>0.807926</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.148790</td>\n",
       "      <td>0.635431</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.032902</td>\n",
       "      <td>-0.086652</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col1      col2      col3      col4      col5      col6      col7  \\\n",
       "0  0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   \n",
       "1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   \n",
       "2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   \n",
       "3  0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   \n",
       "4  0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   \n",
       "\n",
       "       col8      col9     col10  target  \n",
       "0  0.929823  0.251173  0.159345       0  \n",
       "1  0.636203  0.282354  0.515587       0  \n",
       "2 -0.888236 -0.123005  0.582939       0  \n",
       "3  1.051693  0.594169 -0.533994       0  \n",
       "4  0.051062  0.032902 -0.086652       5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to values\n",
    "df = df.values\n",
    "Y = df[:,10]\n",
    "X = df[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data, split into training/testing groups\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.asarray(X), np.asarray(Y), test_size=0.3, shuffle= True)\n",
    "x_train_, x_test_, y_train_, y_test_ = x_train, x_test, y_train, y_test\n",
    "# The known number of output classes.\n",
    "num_classes = len(np.unique(y_train))\n",
    "# Input image dimensions\n",
    "input_shape = (X.shape[1],)\n",
    "\n",
    "# Convert class vectors to binary class matrices. This uses 1 hot encoding.\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], X.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0], X.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def CNN_net():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1],1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(X.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dense(X.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 14606 samples, validate on 6261 samples\n",
      "Epoch 1/200\n",
      "14606/14606 [==============================] - 1s 92us/step - loss: 1.7150 - accuracy: 0.4484 - val_loss: 1.4386 - val_accuracy: 0.5006\n",
      "Epoch 2/200\n",
      "14606/14606 [==============================] - 1s 83us/step - loss: 1.3653 - accuracy: 0.5287 - val_loss: 1.2692 - val_accuracy: 0.5657\n",
      "Epoch 3/200\n",
      "14606/14606 [==============================] - 1s 85us/step - loss: 1.2434 - accuracy: 0.5624 - val_loss: 1.2057 - val_accuracy: 0.5851\n",
      "Epoch 4/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 1.1875 - accuracy: 0.5808 - val_loss: 1.1427 - val_accuracy: 0.5972\n",
      "Epoch 5/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 1.1203 - accuracy: 0.5914 - val_loss: 1.1305 - val_accuracy: 0.5977\n",
      "Epoch 6/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 1.0765 - accuracy: 0.6039 - val_loss: 1.0559 - val_accuracy: 0.6202\n",
      "Epoch 7/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 1.0399 - accuracy: 0.6213 - val_loss: 1.0283 - val_accuracy: 0.6366\n",
      "Epoch 8/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 1.0025 - accuracy: 0.6354 - val_loss: 0.9926 - val_accuracy: 0.6464\n",
      "Epoch 9/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.9636 - accuracy: 0.6469 - val_loss: 0.9869 - val_accuracy: 0.6505\n",
      "Epoch 10/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.9326 - accuracy: 0.6571 - val_loss: 0.9361 - val_accuracy: 0.6676\n",
      "Epoch 11/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.9058 - accuracy: 0.6668 - val_loss: 0.9117 - val_accuracy: 0.6761\n",
      "Epoch 12/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.8809 - accuracy: 0.6776 - val_loss: 0.9006 - val_accuracy: 0.6764\n",
      "Epoch 13/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.8537 - accuracy: 0.6858 - val_loss: 0.8718 - val_accuracy: 0.6799\n",
      "Epoch 14/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.8357 - accuracy: 0.6880 - val_loss: 0.8662 - val_accuracy: 0.6870\n",
      "Epoch 15/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.8174 - accuracy: 0.6949 - val_loss: 0.8445 - val_accuracy: 0.7002\n",
      "Epoch 16/200\n",
      "14606/14606 [==============================] - 1s 98us/step - loss: 0.7973 - accuracy: 0.7018 - val_loss: 0.8299 - val_accuracy: 0.6905\n",
      "Epoch 17/200\n",
      "14606/14606 [==============================] - 1s 87us/step - loss: 0.7819 - accuracy: 0.7043 - val_loss: 0.8148 - val_accuracy: 0.7123\n",
      "Epoch 18/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.7701 - accuracy: 0.7124 - val_loss: 0.8172 - val_accuracy: 0.6972\n",
      "Epoch 19/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.7548 - accuracy: 0.7166 - val_loss: 0.8225 - val_accuracy: 0.6956\n",
      "Epoch 20/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.7435 - accuracy: 0.7225 - val_loss: 0.7713 - val_accuracy: 0.7191\n",
      "Epoch 21/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.7236 - accuracy: 0.7281 - val_loss: 0.7727 - val_accuracy: 0.7208\n",
      "Epoch 22/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.7133 - accuracy: 0.7323 - val_loss: 0.7763 - val_accuracy: 0.7221\n",
      "Epoch 23/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.7054 - accuracy: 0.7311 - val_loss: 0.7384 - val_accuracy: 0.7315\n",
      "Epoch 24/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.6924 - accuracy: 0.7397 - val_loss: 0.7404 - val_accuracy: 0.7267\n",
      "Epoch 25/200\n",
      "14606/14606 [==============================] - 1s 83us/step - loss: 0.6826 - accuracy: 0.7419 - val_loss: 0.7708 - val_accuracy: 0.7175\n",
      "Epoch 26/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.6717 - accuracy: 0.7495 - val_loss: 0.7053 - val_accuracy: 0.7379\n",
      "Epoch 27/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.6673 - accuracy: 0.7467 - val_loss: 0.7099 - val_accuracy: 0.7405\n",
      "Epoch 28/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.6543 - accuracy: 0.7506 - val_loss: 0.7040 - val_accuracy: 0.7389\n",
      "Epoch 29/200\n",
      "14606/14606 [==============================] - 1s 101us/step - loss: 0.6438 - accuracy: 0.7577 - val_loss: 0.6958 - val_accuracy: 0.7435\n",
      "Epoch 30/200\n",
      "14606/14606 [==============================] - 1s 95us/step - loss: 0.6378 - accuracy: 0.7520 - val_loss: 0.6792 - val_accuracy: 0.7480\n",
      "Epoch 31/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.6253 - accuracy: 0.7585 - val_loss: 0.6974 - val_accuracy: 0.7484\n",
      "Epoch 32/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.6186 - accuracy: 0.7632 - val_loss: 0.7092 - val_accuracy: 0.7411\n",
      "Epoch 33/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.6111 - accuracy: 0.7663 - val_loss: 0.6733 - val_accuracy: 0.7499\n",
      "Epoch 34/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.6086 - accuracy: 0.7664 - val_loss: 0.6542 - val_accuracy: 0.7569\n",
      "Epoch 35/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.5952 - accuracy: 0.7709 - val_loss: 0.6696 - val_accuracy: 0.7606\n",
      "Epoch 36/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.5916 - accuracy: 0.7735 - val_loss: 0.6473 - val_accuracy: 0.7638\n",
      "Epoch 37/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.5907 - accuracy: 0.7704 - val_loss: 0.6398 - val_accuracy: 0.7630\n",
      "Epoch 38/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.5987 - accuracy: 0.7737 - val_loss: 0.6748 - val_accuracy: 0.7528\n",
      "Epoch 39/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.5840 - accuracy: 0.7758 - val_loss: 0.6291 - val_accuracy: 0.7663\n",
      "Epoch 40/200\n",
      "14606/14606 [==============================] - 1s 83us/step - loss: 0.5712 - accuracy: 0.7800 - val_loss: 0.6610 - val_accuracy: 0.7547\n",
      "Epoch 41/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.5701 - accuracy: 0.7795 - val_loss: 0.6520 - val_accuracy: 0.7552\n",
      "Epoch 42/200\n",
      "14606/14606 [==============================] - 1s 93us/step - loss: 0.5606 - accuracy: 0.7834 - val_loss: 0.6501 - val_accuracy: 0.7603\n",
      "Epoch 43/200\n",
      "14606/14606 [==============================] - 1s 95us/step - loss: 0.5613 - accuracy: 0.7858 - val_loss: 0.6666 - val_accuracy: 0.7540\n",
      "Epoch 44/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.5553 - accuracy: 0.7882 - val_loss: 0.6202 - val_accuracy: 0.7726\n",
      "Epoch 45/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.5501 - accuracy: 0.7895 - val_loss: 0.6275 - val_accuracy: 0.7724\n",
      "Epoch 46/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.5450 - accuracy: 0.7888 - val_loss: 0.6196 - val_accuracy: 0.7706\n",
      "Epoch 47/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.5478 - accuracy: 0.7874 - val_loss: 0.6108 - val_accuracy: 0.7799\n",
      "Epoch 48/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.5414 - accuracy: 0.7916 - val_loss: 0.6476 - val_accuracy: 0.7678\n",
      "Epoch 49/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.5320 - accuracy: 0.7933 - val_loss: 0.6122 - val_accuracy: 0.7801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.5280 - accuracy: 0.7941 - val_loss: 0.6163 - val_accuracy: 0.7706\n",
      "Epoch 51/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.5235 - accuracy: 0.7991 - val_loss: 0.6048 - val_accuracy: 0.7748\n",
      "Epoch 52/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.5264 - accuracy: 0.7982 - val_loss: 0.6002 - val_accuracy: 0.7769\n",
      "Epoch 53/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.5319 - accuracy: 0.7958 - val_loss: 0.6081 - val_accuracy: 0.7766\n",
      "Epoch 54/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.5161 - accuracy: 0.7999 - val_loss: 0.5806 - val_accuracy: 0.7841\n",
      "Epoch 55/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.5128 - accuracy: 0.8042 - val_loss: 0.6134 - val_accuracy: 0.7783\n",
      "Epoch 56/200\n",
      "14606/14606 [==============================] - 1s 85us/step - loss: 0.5141 - accuracy: 0.8018 - val_loss: 0.6006 - val_accuracy: 0.7842\n",
      "Epoch 57/200\n",
      "14606/14606 [==============================] - 1s 88us/step - loss: 0.5090 - accuracy: 0.8028 - val_loss: 0.6232 - val_accuracy: 0.7697\n",
      "Epoch 58/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.5096 - accuracy: 0.8011 - val_loss: 0.5869 - val_accuracy: 0.7873\n",
      "Epoch 59/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.5042 - accuracy: 0.8061 - val_loss: 0.5923 - val_accuracy: 0.7820\n",
      "Epoch 60/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.4951 - accuracy: 0.8088 - val_loss: 0.5921 - val_accuracy: 0.7837\n",
      "Epoch 61/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.5028 - accuracy: 0.8040 - val_loss: 0.6290 - val_accuracy: 0.7743\n",
      "Epoch 62/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4899 - accuracy: 0.8119 - val_loss: 0.5969 - val_accuracy: 0.7857\n",
      "Epoch 63/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4899 - accuracy: 0.8127 - val_loss: 0.5691 - val_accuracy: 0.7933\n",
      "Epoch 64/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.4927 - accuracy: 0.8123 - val_loss: 0.6289 - val_accuracy: 0.7758\n",
      "Epoch 65/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4835 - accuracy: 0.8127 - val_loss: 0.5893 - val_accuracy: 0.7898\n",
      "Epoch 66/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.4789 - accuracy: 0.8163 - val_loss: 0.6267 - val_accuracy: 0.7670\n",
      "Epoch 67/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4754 - accuracy: 0.8168 - val_loss: 0.5859 - val_accuracy: 0.7885\n",
      "Epoch 68/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4758 - accuracy: 0.8167 - val_loss: 0.6236 - val_accuracy: 0.7710\n",
      "Epoch 69/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4699 - accuracy: 0.8185 - val_loss: 0.5747 - val_accuracy: 0.7924\n",
      "Epoch 70/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.4753 - accuracy: 0.8186 - val_loss: 0.5958 - val_accuracy: 0.7917\n",
      "Epoch 71/200\n",
      "14606/14606 [==============================] - 1s 90us/step - loss: 0.4649 - accuracy: 0.8214 - val_loss: 0.5965 - val_accuracy: 0.7949\n",
      "Epoch 72/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4652 - accuracy: 0.8178 - val_loss: 0.5769 - val_accuracy: 0.7914\n",
      "Epoch 73/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.4615 - accuracy: 0.8212 - val_loss: 0.5830 - val_accuracy: 0.7936\n",
      "Epoch 74/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4590 - accuracy: 0.8229 - val_loss: 0.5962 - val_accuracy: 0.7820\n",
      "Epoch 75/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4555 - accuracy: 0.8245 - val_loss: 0.5746 - val_accuracy: 0.7959\n",
      "Epoch 76/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.4581 - accuracy: 0.8229 - val_loss: 0.5781 - val_accuracy: 0.7959\n",
      "Epoch 77/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4560 - accuracy: 0.8235 - val_loss: 0.5511 - val_accuracy: 0.8098\n",
      "Epoch 78/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.4522 - accuracy: 0.8267 - val_loss: 0.5723 - val_accuracy: 0.7973\n",
      "Epoch 79/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4557 - accuracy: 0.8218 - val_loss: 0.5869 - val_accuracy: 0.7889\n",
      "Epoch 80/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.4406 - accuracy: 0.8311 - val_loss: 0.5566 - val_accuracy: 0.8067\n",
      "Epoch 81/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4439 - accuracy: 0.8287 - val_loss: 0.5699 - val_accuracy: 0.8019\n",
      "Epoch 82/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4374 - accuracy: 0.8314 - val_loss: 0.5525 - val_accuracy: 0.8130\n",
      "Epoch 83/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.4390 - accuracy: 0.8295 - val_loss: 0.5620 - val_accuracy: 0.7960\n",
      "Epoch 84/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.4357 - accuracy: 0.8316 - val_loss: 0.5852 - val_accuracy: 0.7952\n",
      "Epoch 85/200\n",
      "14606/14606 [==============================] - 1s 89us/step - loss: 0.4396 - accuracy: 0.8319 - val_loss: 0.5505 - val_accuracy: 0.8080\n",
      "Epoch 86/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.4873 - accuracy: 0.8196 - val_loss: 0.5769 - val_accuracy: 0.7999\n",
      "Epoch 87/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4306 - accuracy: 0.8332 - val_loss: 0.5538 - val_accuracy: 0.8019\n",
      "Epoch 88/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.4328 - accuracy: 0.8323 - val_loss: 0.5628 - val_accuracy: 0.8040\n",
      "Epoch 89/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4216 - accuracy: 0.8396 - val_loss: 0.5670 - val_accuracy: 0.8072\n",
      "Epoch 90/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4254 - accuracy: 0.8373 - val_loss: 0.5442 - val_accuracy: 0.8130\n",
      "Epoch 91/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4376 - accuracy: 0.8337 - val_loss: 0.5404 - val_accuracy: 0.8146\n",
      "Epoch 92/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4257 - accuracy: 0.8368 - val_loss: 0.5450 - val_accuracy: 0.8059\n",
      "Epoch 93/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.4234 - accuracy: 0.8340 - val_loss: 0.5314 - val_accuracy: 0.8154\n",
      "Epoch 94/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.4283 - accuracy: 0.8345 - val_loss: 0.5467 - val_accuracy: 0.8120\n",
      "Epoch 95/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4194 - accuracy: 0.8376 - val_loss: 0.5330 - val_accuracy: 0.8142\n",
      "Epoch 96/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4176 - accuracy: 0.8404 - val_loss: 0.5818 - val_accuracy: 0.8019\n",
      "Epoch 97/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.4237 - accuracy: 0.8377 - val_loss: 0.5786 - val_accuracy: 0.8066\n",
      "Epoch 98/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4297 - accuracy: 0.8358 - val_loss: 0.5609 - val_accuracy: 0.8077\n",
      "Epoch 99/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.4120 - accuracy: 0.8424 - val_loss: 0.5356 - val_accuracy: 0.8103\n",
      "Epoch 100/200\n",
      "14606/14606 [==============================] - 1s 89us/step - loss: 0.4063 - accuracy: 0.8429 - val_loss: 0.5609 - val_accuracy: 0.8018\n",
      "Epoch 101/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.4099 - accuracy: 0.8430 - val_loss: 0.5531 - val_accuracy: 0.8083\n",
      "Epoch 102/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.4093 - accuracy: 0.8448 - val_loss: 0.5349 - val_accuracy: 0.8134\n",
      "Epoch 103/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.4052 - accuracy: 0.8419 - val_loss: 0.5435 - val_accuracy: 0.8149\n",
      "Epoch 104/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4027 - accuracy: 0.8441 - val_loss: 0.5586 - val_accuracy: 0.8096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4084 - accuracy: 0.8438 - val_loss: 0.5462 - val_accuracy: 0.8174\n",
      "Epoch 106/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.4065 - accuracy: 0.8464 - val_loss: 0.5284 - val_accuracy: 0.8160\n",
      "Epoch 107/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4008 - accuracy: 0.8465 - val_loss: 0.5552 - val_accuracy: 0.8122\n",
      "Epoch 108/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3979 - accuracy: 0.8452 - val_loss: 0.5879 - val_accuracy: 0.8011\n",
      "Epoch 109/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4014 - accuracy: 0.8468 - val_loss: 0.6028 - val_accuracy: 0.7928\n",
      "Epoch 110/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.4034 - accuracy: 0.8443 - val_loss: 0.5512 - val_accuracy: 0.8152\n",
      "Epoch 111/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.3944 - accuracy: 0.8480 - val_loss: 0.5548 - val_accuracy: 0.8139\n",
      "Epoch 112/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.3991 - accuracy: 0.8449 - val_loss: 0.5393 - val_accuracy: 0.8187\n",
      "Epoch 113/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3924 - accuracy: 0.8449 - val_loss: 0.5811 - val_accuracy: 0.7986\n",
      "Epoch 114/200\n",
      "14606/14606 [==============================] - 1s 90us/step - loss: 0.3995 - accuracy: 0.8490 - val_loss: 0.5209 - val_accuracy: 0.8246\n",
      "Epoch 115/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.3865 - accuracy: 0.8494 - val_loss: 0.5537 - val_accuracy: 0.8099\n",
      "Epoch 116/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3875 - accuracy: 0.8526 - val_loss: 0.6307 - val_accuracy: 0.7861\n",
      "Epoch 117/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.4191 - accuracy: 0.8446 - val_loss: 0.5238 - val_accuracy: 0.8243\n",
      "Epoch 118/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3882 - accuracy: 0.8486 - val_loss: 0.5631 - val_accuracy: 0.8160\n",
      "Epoch 119/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.3830 - accuracy: 0.8536 - val_loss: 0.5640 - val_accuracy: 0.8133\n",
      "Epoch 120/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.3847 - accuracy: 0.8518 - val_loss: 0.5280 - val_accuracy: 0.8256\n",
      "Epoch 121/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.3788 - accuracy: 0.8561 - val_loss: 0.5280 - val_accuracy: 0.8270\n",
      "Epoch 122/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3772 - accuracy: 0.8543 - val_loss: 0.5866 - val_accuracy: 0.8063\n",
      "Epoch 123/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3855 - accuracy: 0.8524 - val_loss: 0.5740 - val_accuracy: 0.8091\n",
      "Epoch 124/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3767 - accuracy: 0.8562 - val_loss: 0.5647 - val_accuracy: 0.8082\n",
      "Epoch 125/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3774 - accuracy: 0.8568 - val_loss: 0.5614 - val_accuracy: 0.8176\n",
      "Epoch 126/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.3901 - accuracy: 0.8496 - val_loss: 0.5621 - val_accuracy: 0.8206\n",
      "Epoch 127/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3732 - accuracy: 0.8569 - val_loss: 0.5524 - val_accuracy: 0.8162\n",
      "Epoch 128/200\n",
      "14606/14606 [==============================] - 1s 84us/step - loss: 0.3739 - accuracy: 0.8602 - val_loss: 0.5204 - val_accuracy: 0.8221\n",
      "Epoch 129/200\n",
      "14606/14606 [==============================] - 1s 87us/step - loss: 0.4174 - accuracy: 0.8507 - val_loss: 0.5287 - val_accuracy: 0.8280\n",
      "Epoch 130/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3793 - accuracy: 0.8542 - val_loss: 0.5219 - val_accuracy: 0.8302\n",
      "Epoch 131/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3671 - accuracy: 0.8583 - val_loss: 0.5195 - val_accuracy: 0.8294\n",
      "Epoch 132/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3691 - accuracy: 0.8600 - val_loss: 0.5199 - val_accuracy: 0.8315\n",
      "Epoch 133/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.3632 - accuracy: 0.8621 - val_loss: 0.5635 - val_accuracy: 0.8197\n",
      "Epoch 134/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.3683 - accuracy: 0.8635 - val_loss: 0.5129 - val_accuracy: 0.8329\n",
      "Epoch 135/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.3715 - accuracy: 0.8583 - val_loss: 0.5964 - val_accuracy: 0.8119\n",
      "Epoch 136/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3658 - accuracy: 0.8602 - val_loss: 0.5161 - val_accuracy: 0.8336\n",
      "Epoch 137/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3638 - accuracy: 0.8617 - val_loss: 0.5440 - val_accuracy: 0.8241\n",
      "Epoch 138/200\n",
      "14606/14606 [==============================] - 1s 83us/step - loss: 0.3646 - accuracy: 0.8620 - val_loss: 0.5732 - val_accuracy: 0.8095\n",
      "Epoch 139/200\n",
      "14606/14606 [==============================] - 1s 81us/step - loss: 0.3711 - accuracy: 0.8603 - val_loss: 0.5280 - val_accuracy: 0.8297\n",
      "Epoch 140/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.3650 - accuracy: 0.8592 - val_loss: 0.5458 - val_accuracy: 0.8251\n",
      "Epoch 141/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3622 - accuracy: 0.8599 - val_loss: 0.5258 - val_accuracy: 0.8296\n",
      "Epoch 142/200\n",
      "14606/14606 [==============================] - 1s 83us/step - loss: 0.3564 - accuracy: 0.8642 - val_loss: 0.5204 - val_accuracy: 0.8315\n",
      "Epoch 143/200\n",
      "14606/14606 [==============================] - 1s 87us/step - loss: 0.3564 - accuracy: 0.8650 - val_loss: 0.5462 - val_accuracy: 0.8246\n",
      "Epoch 144/200\n",
      "14606/14606 [==============================] - 1s 71us/step - loss: 0.3685 - accuracy: 0.8591 - val_loss: 0.5430 - val_accuracy: 0.8264\n",
      "Epoch 145/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.3949 - accuracy: 0.8578 - val_loss: 0.5685 - val_accuracy: 0.8123\n",
      "Epoch 146/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3641 - accuracy: 0.8603 - val_loss: 0.5706 - val_accuracy: 0.8109\n",
      "Epoch 147/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3473 - accuracy: 0.8675 - val_loss: 0.5202 - val_accuracy: 0.8315\n",
      "Epoch 148/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3497 - accuracy: 0.8677 - val_loss: 0.5301 - val_accuracy: 0.8312\n",
      "Epoch 149/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.3400 - accuracy: 0.8720 - val_loss: 0.5618 - val_accuracy: 0.8256\n",
      "Epoch 150/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3595 - accuracy: 0.8635 - val_loss: 0.6012 - val_accuracy: 0.8005\n",
      "Epoch 151/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3506 - accuracy: 0.8679 - val_loss: 0.5502 - val_accuracy: 0.8211\n",
      "Epoch 152/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3565 - accuracy: 0.8646 - val_loss: 0.5339 - val_accuracy: 0.8331\n",
      "Epoch 153/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3522 - accuracy: 0.8635 - val_loss: 0.5294 - val_accuracy: 0.8315\n",
      "Epoch 154/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3445 - accuracy: 0.8668 - val_loss: 0.5456 - val_accuracy: 0.8202\n",
      "Epoch 155/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.3527 - accuracy: 0.8651 - val_loss: 0.5462 - val_accuracy: 0.8293\n",
      "Epoch 156/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3461 - accuracy: 0.8685 - val_loss: 0.5456 - val_accuracy: 0.8302\n",
      "Epoch 157/200\n",
      "14606/14606 [==============================] - 1s 89us/step - loss: 0.3794 - accuracy: 0.8634 - val_loss: 0.5704 - val_accuracy: 0.8256\n",
      "Epoch 158/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.3414 - accuracy: 0.8688 - val_loss: 0.5550 - val_accuracy: 0.8261\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3349 - accuracy: 0.8753 - val_loss: 0.5320 - val_accuracy: 0.8339\n",
      "Epoch 160/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.3443 - accuracy: 0.8690 - val_loss: 0.5165 - val_accuracy: 0.8347\n",
      "Epoch 161/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3416 - accuracy: 0.8714 - val_loss: 0.5404 - val_accuracy: 0.8234\n",
      "Epoch 162/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3387 - accuracy: 0.8705 - val_loss: 0.5382 - val_accuracy: 0.8342\n",
      "Epoch 163/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3393 - accuracy: 0.8716 - val_loss: 0.5283 - val_accuracy: 0.8305\n",
      "Epoch 164/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3437 - accuracy: 0.8679 - val_loss: 0.5233 - val_accuracy: 0.8396\n",
      "Epoch 165/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3388 - accuracy: 0.8709 - val_loss: 0.5721 - val_accuracy: 0.8224\n",
      "Epoch 166/200\n",
      "14606/14606 [==============================] - 1s 74us/step - loss: 0.3484 - accuracy: 0.8676 - val_loss: 0.5198 - val_accuracy: 0.8309\n",
      "Epoch 167/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.3352 - accuracy: 0.8729 - val_loss: 0.5221 - val_accuracy: 0.8345\n",
      "Epoch 168/200\n",
      "14606/14606 [==============================] - 1s 79us/step - loss: 0.3367 - accuracy: 0.8727 - val_loss: 0.5233 - val_accuracy: 0.8363\n",
      "Epoch 169/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.3408 - accuracy: 0.8722 - val_loss: 0.5179 - val_accuracy: 0.8385\n",
      "Epoch 170/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3343 - accuracy: 0.8738 - val_loss: 0.5435 - val_accuracy: 0.8317\n",
      "Epoch 171/200\n",
      "14606/14606 [==============================] - 1s 88us/step - loss: 0.3664 - accuracy: 0.8705 - val_loss: 0.5700 - val_accuracy: 0.8229\n",
      "Epoch 172/200\n",
      "14606/14606 [==============================] - 1s 86us/step - loss: 0.3438 - accuracy: 0.8696 - val_loss: 0.5145 - val_accuracy: 0.8443\n",
      "Epoch 173/200\n",
      "14606/14606 [==============================] - 1s 78us/step - loss: 0.3343 - accuracy: 0.8708 - val_loss: 0.5230 - val_accuracy: 0.8344\n",
      "Epoch 174/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.3278 - accuracy: 0.8744 - val_loss: 0.5048 - val_accuracy: 0.8409\n",
      "Epoch 175/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3252 - accuracy: 0.8758 - val_loss: 0.5114 - val_accuracy: 0.8396\n",
      "Epoch 176/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3400 - accuracy: 0.8707 - val_loss: 0.5716 - val_accuracy: 0.8235\n",
      "Epoch 177/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3411 - accuracy: 0.8703 - val_loss: 0.5311 - val_accuracy: 0.8326\n",
      "Epoch 178/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3292 - accuracy: 0.8735 - val_loss: 0.5292 - val_accuracy: 0.8379\n",
      "Epoch 179/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3224 - accuracy: 0.8781 - val_loss: 0.5087 - val_accuracy: 0.8425\n",
      "Epoch 180/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3218 - accuracy: 0.8770 - val_loss: 0.5041 - val_accuracy: 0.8484\n",
      "Epoch 181/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3296 - accuracy: 0.8756 - val_loss: 0.5148 - val_accuracy: 0.8481\n",
      "Epoch 182/200\n",
      "14606/14606 [==============================] - 1s 80us/step - loss: 0.3299 - accuracy: 0.8740 - val_loss: 0.5544 - val_accuracy: 0.8353\n",
      "Epoch 183/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3229 - accuracy: 0.8782 - val_loss: 0.5428 - val_accuracy: 0.8436\n",
      "Epoch 184/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3194 - accuracy: 0.8798 - val_loss: 0.5522 - val_accuracy: 0.8372\n",
      "Epoch 185/200\n",
      "14606/14606 [==============================] - 1s 82us/step - loss: 0.3210 - accuracy: 0.8794 - val_loss: 0.5271 - val_accuracy: 0.8414\n",
      "Epoch 186/200\n",
      "14606/14606 [==============================] - 1s 88us/step - loss: 0.3287 - accuracy: 0.8751 - val_loss: 0.5262 - val_accuracy: 0.8422\n",
      "Epoch 187/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3505 - accuracy: 0.8666 - val_loss: 0.5236 - val_accuracy: 0.8408\n",
      "Epoch 188/200\n",
      "14606/14606 [==============================] - 1s 72us/step - loss: 0.3202 - accuracy: 0.8798 - val_loss: 0.5738 - val_accuracy: 0.8304\n",
      "Epoch 189/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.3215 - accuracy: 0.8760 - val_loss: 0.5947 - val_accuracy: 0.8310\n",
      "Epoch 190/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3221 - accuracy: 0.8778 - val_loss: 0.5244 - val_accuracy: 0.8449\n",
      "Epoch 191/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3286 - accuracy: 0.8774 - val_loss: 0.5493 - val_accuracy: 0.8388\n",
      "Epoch 192/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.3213 - accuracy: 0.8772 - val_loss: 0.5513 - val_accuracy: 0.8235\n",
      "Epoch 193/200\n",
      "14606/14606 [==============================] - 1s 75us/step - loss: 0.3204 - accuracy: 0.8811 - val_loss: 0.5986 - val_accuracy: 0.8206\n",
      "Epoch 194/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3135 - accuracy: 0.8818 - val_loss: 0.5412 - val_accuracy: 0.8360\n",
      "Epoch 195/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3218 - accuracy: 0.8761 - val_loss: 0.5308 - val_accuracy: 0.8353\n",
      "Epoch 196/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3402 - accuracy: 0.8742 - val_loss: 0.5985 - val_accuracy: 0.8208\n",
      "Epoch 197/200\n",
      "14606/14606 [==============================] - 1s 76us/step - loss: 0.3263 - accuracy: 0.8783 - val_loss: 0.5290 - val_accuracy: 0.8441\n",
      "Epoch 198/200\n",
      "14606/14606 [==============================] - 1s 77us/step - loss: 0.3141 - accuracy: 0.8807 - val_loss: 0.5093 - val_accuracy: 0.8428\n",
      "Epoch 199/200\n",
      "14606/14606 [==============================] - 1s 73us/step - loss: 0.3187 - accuracy: 0.8761 - val_loss: 0.5241 - val_accuracy: 0.8393\n",
      "Epoch 200/200\n",
      "14606/14606 [==============================] - 1s 94us/step - loss: 0.3112 - accuracy: 0.8816 - val_loss: 0.5234 - val_accuracy: 0.8452\n"
     ]
    }
   ],
   "source": [
    "# Parametres\n",
    "verbose, epochs, batch_size = 1, 200, 32\n",
    "# initialize the model object\n",
    "clf_cnn = CNN_net()\n",
    "# fit network\n",
    "history = clf_cnn.fit(x_train, y_train_binary, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=verbose, validation_data=(x_test, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6261/6261 [==============================] - 0s 29us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5233950362625718, 0.845232367515564]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call predict to get predictions Report the accuracy\n",
    "clf_cnn.evaluate(x_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN : Accuracy: 0.8348506628334131\n"
     ]
    }
   ],
   "source": [
    "# call predict to get predictions\n",
    "y_pred = clf_cnn.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Report the accuracy\n",
    "accuracy_CNN = accuracy_score(y_test_binary, y_pred)\n",
    "print(\"CNN : Accuracy: \" + str(accuracy_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 8, 32)             128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2, 64)             6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                132       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 7,118\n",
      "Trainable params: 7,118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss','Validation loss'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training acc','Validation acc'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# from the first Fully-Connected layer \n",
    "layer_name = 'dense_1'\n",
    "intermediate_layer_model = Model(inputs=clf_cnn.input,\n",
    "                                 outputs=clf_cnn.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features of the train dataset to use it in future.\n",
    "out_cnn_train = intermediate_layer_model.predict(x_train)\n",
    "# Save the features of the test dataset to use it in future.\n",
    "out_cnn_test = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (from CNN) Shape: (14606, 10)\n",
      "Training Labels (from CNN) Shape: (14606,)\n",
      "Test Features (from CNN) Shape: (6261, 10)\n",
      "Test Labels (from CNN) Shape: (6261,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features (from CNN) Shape:', out_cnn_train.shape)\n",
    "print('Training Labels (from CNN) Shape:', y_train.shape)\n",
    "\n",
    "print('Test Features (from CNN) Shape:', out_cnn_test.shape)\n",
    "print('Test Labels (from CNN) Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + Random Forest + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djinn aloi\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:262: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:263: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:266: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:302: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:286: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:320: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:328: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:333: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:334: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:335: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch: 0001 cost= 2.824552877 accuracy= 0.051\n",
      "Epoch: 0002 cost= 2.371128321 accuracy= 0.187\n",
      "Epoch: 0003 cost= 1.957742793 accuracy= 0.412\n",
      "Epoch: 0004 cost= 1.869040302 accuracy= 0.412\n",
      "Epoch: 0005 cost= 1.859551021 accuracy= 0.412\n",
      "Epoch: 0006 cost= 1.827821936 accuracy= 0.412\n",
      "Epoch: 0007 cost= 1.841357810 accuracy= 0.412\n",
      "Epoch: 0008 cost= 1.851926310 accuracy= 0.412\n",
      "Epoch: 0009 cost= 1.839295081 accuracy= 0.412\n",
      "Epoch: 0010 cost= 1.836624912 accuracy= 0.412\n",
      "Epoch: 0011 cost= 1.824658445 accuracy= 0.412\n",
      "Epoch: 0012 cost= 1.824424641 accuracy= 0.412\n",
      "Epoch: 0013 cost= 1.832943525 accuracy= 0.412\n",
      "Epoch: 0014 cost= 1.820630363 accuracy= 0.412\n",
      "Epoch: 0015 cost= 1.823020152 accuracy= 0.412\n",
      "Epoch: 0016 cost= 1.817183665 accuracy= 0.412\n",
      "Epoch: 0017 cost= 1.826295001 accuracy= 0.412\n",
      "Epoch: 0018 cost= 1.823724951 accuracy= 0.412\n",
      "Epoch: 0019 cost= 1.831011500 accuracy= 0.412\n",
      "Epoch: 0020 cost= 1.824643237 accuracy= 0.412\n",
      "Epoch: 0021 cost= 1.822291051 accuracy= 0.412\n",
      "Epoch: 0022 cost= 1.839259216 accuracy= 0.412\n",
      "Epoch: 0023 cost= 1.831335613 accuracy= 0.412\n",
      "Epoch: 0024 cost= 1.840742895 accuracy= 0.412\n",
      "Epoch: 0025 cost= 1.819610153 accuracy= 0.412\n",
      "Epoch: 0026 cost= 1.814733267 accuracy= 0.412\n",
      "Epoch: 0027 cost= 1.810684425 accuracy= 0.412\n",
      "Epoch: 0028 cost= 1.813746061 accuracy= 0.412\n",
      "Epoch: 0029 cost= 1.801077315 accuracy= 0.412\n",
      "Epoch: 0030 cost= 1.806883761 accuracy= 0.412\n",
      "Epoch: 0031 cost= 1.795185872 accuracy= 0.412\n",
      "Epoch: 0032 cost= 1.796559640 accuracy= 0.412\n",
      "Epoch: 0033 cost= 1.767473902 accuracy= 0.412\n",
      "Epoch: 0034 cost= 1.770951595 accuracy= 0.412\n",
      "Epoch: 0035 cost= 1.748642615 accuracy= 0.412\n",
      "Epoch: 0036 cost= 1.726715769 accuracy= 0.412\n",
      "Epoch: 0037 cost= 1.726657493 accuracy= 0.412\n",
      "Epoch: 0038 cost= 1.728307349 accuracy= 0.412\n",
      "Epoch: 0039 cost= 1.689186096 accuracy= 0.411\n",
      "Epoch: 0040 cost= 1.660739592 accuracy= 0.422\n",
      "Epoch: 0041 cost= 1.626027499 accuracy= 0.433\n",
      "Epoch: 0042 cost= 1.618959018 accuracy= 0.427\n",
      "Epoch: 0043 cost= 1.634513634 accuracy= 0.474\n",
      "Epoch: 0044 cost= 1.595303501 accuracy= 0.441\n",
      "Epoch: 0045 cost= 1.565199018 accuracy= 0.464\n",
      "Epoch: 0046 cost= 1.537682090 accuracy= 0.463\n",
      "Epoch: 0047 cost= 1.533362406 accuracy= 0.466\n",
      "Epoch: 0048 cost= 1.540798868 accuracy= 0.467\n",
      "Epoch: 0049 cost= 1.525278960 accuracy= 0.475\n",
      "Epoch: 0050 cost= 1.527939439 accuracy= 0.469\n",
      "Epoch: 0051 cost= 1.519730755 accuracy= 0.475\n",
      "Epoch: 0052 cost= 1.525593877 accuracy= 0.472\n",
      "Epoch: 0053 cost= 1.534610697 accuracy= 0.463\n",
      "Epoch: 0054 cost= 1.519043275 accuracy= 0.469\n",
      "Epoch: 0055 cost= 1.499282939 accuracy= 0.479\n",
      "Epoch: 0056 cost= 1.510479348 accuracy= 0.480\n",
      "Epoch: 0057 cost= 1.502021296 accuracy= 0.475\n",
      "Epoch: 0058 cost= 1.512072682 accuracy= 0.472\n",
      "Epoch: 0059 cost= 1.514038461 accuracy= 0.466\n",
      "Epoch: 0060 cost= 1.539142132 accuracy= 0.467\n",
      "Epoch: 0061 cost= 1.517170293 accuracy= 0.474\n",
      "Epoch: 0062 cost= 1.497977734 accuracy= 0.470\n",
      "Epoch: 0063 cost= 1.508551989 accuracy= 0.476\n",
      "Epoch: 0064 cost= 1.504365666 accuracy= 0.482\n",
      "Epoch: 0065 cost= 1.490614244 accuracy= 0.477\n",
      "Epoch: 0066 cost= 1.495073557 accuracy= 0.476\n",
      "Epoch: 0067 cost= 1.511424746 accuracy= 0.481\n",
      "Epoch: 0068 cost= 1.492137330 accuracy= 0.479\n",
      "Epoch: 0069 cost= 1.506112746 accuracy= 0.478\n",
      "Epoch: 0070 cost= 1.488397343 accuracy= 0.479\n",
      "Epoch: 0071 cost= 1.482945102 accuracy= 0.482\n",
      "Epoch: 0072 cost= 1.485283102 accuracy= 0.482\n",
      "Epoch: 0073 cost= 1.490015796 accuracy= 0.480\n",
      "Epoch: 0074 cost= 1.475801332 accuracy= 0.482\n",
      "Epoch: 0075 cost= 1.493261933 accuracy= 0.482\n",
      "Epoch: 0076 cost= 1.483881286 accuracy= 0.480\n",
      "Epoch: 0077 cost= 1.468013457 accuracy= 0.483\n",
      "Epoch: 0078 cost= 1.472504139 accuracy= 0.480\n",
      "Epoch: 0079 cost= 1.484900492 accuracy= 0.480\n",
      "Epoch: 0080 cost= 1.475230455 accuracy= 0.483\n",
      "Epoch: 0081 cost= 1.492862906 accuracy= 0.480\n",
      "Epoch: 0082 cost= 1.487505300 accuracy= 0.479\n",
      "Epoch: 0083 cost= 1.496677739 accuracy= 0.482\n",
      "Epoch: 0084 cost= 1.502625244 accuracy= 0.479\n",
      "Epoch: 0085 cost= 1.488583071 accuracy= 0.478\n",
      "Epoch: 0086 cost= 1.492053679 accuracy= 0.482\n",
      "Epoch: 0087 cost= 1.491083213 accuracy= 0.484\n",
      "Epoch: 0088 cost= 1.475892016 accuracy= 0.482\n",
      "Epoch: 0089 cost= 1.498702884 accuracy= 0.483\n",
      "Epoch: 0090 cost= 1.476731368 accuracy= 0.486\n",
      "Epoch: 0091 cost= 1.467248729 accuracy= 0.486\n",
      "Epoch: 0092 cost= 1.477861234 accuracy= 0.485\n",
      "Epoch: 0093 cost= 1.474096264 accuracy= 0.486\n",
      "Epoch: 0094 cost= 1.477650523 accuracy= 0.479\n",
      "Epoch: 0095 cost= 1.447882874 accuracy= 0.480\n",
      "Epoch: 0096 cost= 1.483407804 accuracy= 0.482\n",
      "Epoch: 0097 cost= 1.498444489 accuracy= 0.482\n",
      "Epoch: 0098 cost= 1.470563310 accuracy= 0.483\n",
      "Epoch: 0099 cost= 1.463517649 accuracy= 0.484\n",
      "Epoch: 0100 cost= 1.479054502 accuracy= 0.485\n",
      "Epoch: 0101 cost= 1.477853196 accuracy= 0.484\n",
      "Epoch: 0102 cost= 1.469753146 accuracy= 0.486\n",
      "Epoch: 0103 cost= 1.451923966 accuracy= 0.481\n",
      "Epoch: 0104 cost= 1.466364826 accuracy= 0.471\n",
      "Epoch: 0105 cost= 1.496669752 accuracy= 0.476\n",
      "Epoch: 0106 cost= 1.499923962 accuracy= 0.464\n",
      "Epoch: 0107 cost= 1.487284030 accuracy= 0.483\n",
      "Epoch: 0108 cost= 1.496147479 accuracy= 0.483\n",
      "Epoch: 0109 cost= 1.480661750 accuracy= 0.473\n",
      "Epoch: 0110 cost= 1.475448251 accuracy= 0.483\n",
      "Epoch: 0111 cost= 1.469055516 accuracy= 0.486\n",
      "Epoch: 0112 cost= 1.475507634 accuracy= 0.482\n",
      "Epoch: 0113 cost= 1.460487417 accuracy= 0.486\n",
      "Epoch: 0114 cost= 1.462522694 accuracy= 0.483\n",
      "Epoch: 0115 cost= 1.483287760 accuracy= 0.480\n",
      "Epoch: 0116 cost= 1.480292474 accuracy= 0.481\n",
      "Epoch: 0117 cost= 1.469424452 accuracy= 0.479\n",
      "Epoch: 0118 cost= 1.469960025 accuracy= 0.484\n",
      "Epoch: 0119 cost= 1.461423789 accuracy= 0.484\n",
      "Epoch: 0120 cost= 1.445939405 accuracy= 0.488\n",
      "Epoch: 0121 cost= 1.446677089 accuracy= 0.484\n",
      "Epoch: 0122 cost= 1.458038160 accuracy= 0.488\n",
      "Epoch: 0123 cost= 1.454409633 accuracy= 0.487\n",
      "Epoch: 0124 cost= 1.466836163 accuracy= 0.486\n",
      "Epoch: 0125 cost= 1.463005526 accuracy= 0.484\n",
      "Epoch: 0126 cost= 1.445233975 accuracy= 0.482\n",
      "Epoch: 0127 cost= 1.465730565 accuracy= 0.484\n",
      "Epoch: 0128 cost= 1.465088861 accuracy= 0.486\n",
      "Epoch: 0129 cost= 1.461374913 accuracy= 0.487\n",
      "Epoch: 0130 cost= 1.456025158 accuracy= 0.487\n",
      "Epoch: 0131 cost= 1.447382263 accuracy= 0.484\n",
      "Epoch: 0132 cost= 1.459713868 accuracy= 0.487\n",
      "Epoch: 0133 cost= 1.437940546 accuracy= 0.483\n",
      "Epoch: 0134 cost= 1.448853799 accuracy= 0.488\n",
      "Epoch: 0135 cost= 1.442696299 accuracy= 0.488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0136 cost= 1.453020573 accuracy= 0.487\n",
      "Epoch: 0137 cost= 1.458654676 accuracy= 0.489\n",
      "Epoch: 0138 cost= 1.481338416 accuracy= 0.478\n",
      "Epoch: 0139 cost= 1.453031387 accuracy= 0.483\n",
      "Epoch: 0140 cost= 1.450374178 accuracy= 0.484\n",
      "Epoch: 0141 cost= 1.446518626 accuracy= 0.487\n",
      "Epoch: 0142 cost= 1.446280752 accuracy= 0.485\n",
      "Epoch: 0143 cost= 1.444492425 accuracy= 0.486\n",
      "Epoch: 0144 cost= 1.447947655 accuracy= 0.488\n",
      "Epoch: 0145 cost= 1.458519544 accuracy= 0.486\n",
      "Epoch: 0146 cost= 1.448281458 accuracy= 0.490\n",
      "Epoch: 0147 cost= 1.444407208 accuracy= 0.489\n",
      "Epoch: 0148 cost= 1.470206976 accuracy= 0.482\n",
      "Epoch: 0149 cost= 1.472511002 accuracy= 0.477\n",
      "Epoch: 0150 cost= 1.470433474 accuracy= 0.479\n",
      "Epoch: 0151 cost= 1.449858665 accuracy= 0.483\n",
      "Epoch: 0152 cost= 1.443818842 accuracy= 0.488\n",
      "Epoch: 0153 cost= 1.449587839 accuracy= 0.489\n",
      "Epoch: 0154 cost= 1.466443675 accuracy= 0.484\n",
      "Epoch: 0155 cost= 1.463084187 accuracy= 0.481\n",
      "Epoch: 0156 cost= 1.471328565 accuracy= 0.480\n",
      "Epoch: 0157 cost= 1.426883817 accuracy= 0.485\n",
      "Epoch: 0158 cost= 1.461467726 accuracy= 0.478\n",
      "Epoch: 0159 cost= 1.478634681 accuracy= 0.480\n",
      "Epoch: 0160 cost= 1.462263533 accuracy= 0.480\n",
      "Epoch: 0161 cost= 1.456058162 accuracy= 0.487\n",
      "Epoch: 0162 cost= 1.433127301 accuracy= 0.487\n",
      "Epoch: 0163 cost= 1.445204020 accuracy= 0.490\n",
      "Epoch: 0164 cost= 1.458866869 accuracy= 0.488\n",
      "Epoch: 0165 cost= 1.439555526 accuracy= 0.487\n",
      "Epoch: 0166 cost= 1.457073212 accuracy= 0.485\n",
      "Epoch: 0167 cost= 1.434312769 accuracy= 0.486\n",
      "Epoch: 0168 cost= 1.467998300 accuracy= 0.482\n",
      "Epoch: 0169 cost= 1.457051550 accuracy= 0.482\n",
      "Epoch: 0170 cost= 1.452587570 accuracy= 0.477\n",
      "Epoch: 0171 cost= 1.458954624 accuracy= 0.480\n",
      "Epoch: 0172 cost= 1.437906401 accuracy= 0.486\n",
      "Epoch: 0173 cost= 1.423936912 accuracy= 0.485\n",
      "Epoch: 0174 cost= 1.461655361 accuracy= 0.482\n",
      "Epoch: 0175 cost= 1.450606040 accuracy= 0.487\n",
      "Epoch: 0176 cost= 1.447788886 accuracy= 0.487\n",
      "Epoch: 0177 cost= 1.449151533 accuracy= 0.485\n",
      "Epoch: 0178 cost= 1.439913051 accuracy= 0.483\n",
      "Epoch: 0179 cost= 1.444447381 accuracy= 0.485\n",
      "Epoch: 0180 cost= 1.424951281 accuracy= 0.485\n",
      "Epoch: 0181 cost= 1.457024693 accuracy= 0.489\n",
      "Epoch: 0182 cost= 1.435249703 accuracy= 0.492\n",
      "Epoch: 0183 cost= 1.442289335 accuracy= 0.487\n",
      "Epoch: 0184 cost= 1.444045799 accuracy= 0.483\n",
      "Epoch: 0185 cost= 1.448425463 accuracy= 0.482\n",
      "Epoch: 0186 cost= 1.448562792 accuracy= 0.484\n",
      "Epoch: 0187 cost= 1.430883595 accuracy= 0.488\n",
      "Epoch: 0188 cost= 1.428740535 accuracy= 0.491\n",
      "Epoch: 0189 cost= 1.432030201 accuracy= 0.490\n",
      "Epoch: 0190 cost= 1.422084263 accuracy= 0.493\n",
      "Epoch: 0191 cost= 1.415258833 accuracy= 0.490\n",
      "Epoch: 0192 cost= 1.445760216 accuracy= 0.492\n",
      "Epoch: 0193 cost= 1.447397777 accuracy= 0.492\n",
      "Epoch: 0194 cost= 1.425823740 accuracy= 0.490\n",
      "Epoch: 0195 cost= 1.426258036 accuracy= 0.492\n",
      "Epoch: 0196 cost= 1.423587867 accuracy= 0.493\n",
      "Epoch: 0197 cost= 1.445444924 accuracy= 0.492\n",
      "Epoch: 0198 cost= 1.430792894 accuracy= 0.489\n",
      "Epoch: 0199 cost= 1.424922909 accuracy= 0.483\n",
      "Epoch: 0200 cost= 1.435894932 accuracy= 0.486\n",
      "Epoch: 0201 cost= 1.439738274 accuracy= 0.480\n",
      "Epoch: 0202 cost= 1.448224987 accuracy= 0.483\n",
      "Epoch: 0203 cost= 1.421857732 accuracy= 0.485\n",
      "Epoch: 0204 cost= 1.431019800 accuracy= 0.494\n",
      "Epoch: 0205 cost= 1.416729638 accuracy= 0.491\n",
      "Epoch: 0206 cost= 1.424703291 accuracy= 0.490\n",
      "Epoch: 0207 cost= 1.404795136 accuracy= 0.491\n",
      "Epoch: 0208 cost= 1.423132930 accuracy= 0.493\n",
      "Epoch: 0209 cost= 1.415323513 accuracy= 0.492\n",
      "Epoch: 0210 cost= 1.395773138 accuracy= 0.488\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_aloi_tree0.ckpt\n",
      "Epoch: 0001 cost= 2.866611685 accuracy= 0.051\n",
      "Epoch: 0002 cost= 2.427076817 accuracy= 0.187\n",
      "Epoch: 0003 cost= 1.999046360 accuracy= 0.412\n",
      "Epoch: 0004 cost= 1.931252616 accuracy= 0.412\n",
      "Epoch: 0005 cost= 1.851330621 accuracy= 0.412\n",
      "Epoch: 0006 cost= 1.848220212 accuracy= 0.412\n",
      "Epoch: 0007 cost= 1.839805876 accuracy= 0.412\n",
      "Epoch: 0008 cost= 1.838940110 accuracy= 0.412\n",
      "Epoch: 0009 cost= 1.834198407 accuracy= 0.412\n",
      "Epoch: 0010 cost= 1.840496438 accuracy= 0.412\n",
      "Epoch: 0011 cost= 1.835156526 accuracy= 0.412\n",
      "Epoch: 0012 cost= 1.822804536 accuracy= 0.412\n",
      "Epoch: 0013 cost= 1.831007055 accuracy= 0.412\n",
      "Epoch: 0014 cost= 1.836017404 accuracy= 0.412\n",
      "Epoch: 0015 cost= 1.830077086 accuracy= 0.412\n",
      "Epoch: 0016 cost= 1.850051846 accuracy= 0.412\n",
      "Epoch: 0017 cost= 1.841885294 accuracy= 0.412\n",
      "Epoch: 0018 cost= 1.836420076 accuracy= 0.412\n",
      "Epoch: 0019 cost= 1.846629415 accuracy= 0.412\n",
      "Epoch: 0020 cost= 1.838003193 accuracy= 0.412\n",
      "Epoch: 0021 cost= 1.824048860 accuracy= 0.412\n",
      "Epoch: 0022 cost= 1.837435705 accuracy= 0.412\n",
      "Epoch: 0023 cost= 1.825047272 accuracy= 0.412\n",
      "Epoch: 0024 cost= 1.831918103 accuracy= 0.412\n",
      "Epoch: 0025 cost= 1.829960295 accuracy= 0.412\n",
      "Epoch: 0026 cost= 1.840863194 accuracy= 0.412\n",
      "Epoch: 0027 cost= 1.836718747 accuracy= 0.412\n",
      "Epoch: 0028 cost= 1.837283339 accuracy= 0.412\n",
      "Epoch: 0029 cost= 1.834323951 accuracy= 0.412\n",
      "Epoch: 0030 cost= 1.822047251 accuracy= 0.412\n",
      "Epoch: 0031 cost= 1.840775558 accuracy= 0.412\n",
      "Epoch: 0032 cost= 1.819457855 accuracy= 0.412\n",
      "Epoch: 0033 cost= 1.839596340 accuracy= 0.412\n",
      "Epoch: 0034 cost= 1.811474987 accuracy= 0.412\n",
      "Epoch: 0035 cost= 1.821738924 accuracy= 0.412\n",
      "Epoch: 0036 cost= 1.817976628 accuracy= 0.412\n",
      "Epoch: 0037 cost= 1.809102229 accuracy= 0.412\n",
      "Epoch: 0038 cost= 1.802255579 accuracy= 0.412\n",
      "Epoch: 0039 cost= 1.818822128 accuracy= 0.412\n",
      "Epoch: 0040 cost= 1.793861066 accuracy= 0.412\n",
      "Epoch: 0041 cost= 1.796008723 accuracy= 0.412\n",
      "Epoch: 0042 cost= 1.785743100 accuracy= 0.412\n",
      "Epoch: 0043 cost= 1.777579478 accuracy= 0.412\n",
      "Epoch: 0044 cost= 1.770185062 accuracy= 0.412\n",
      "Epoch: 0045 cost= 1.769389238 accuracy= 0.412\n",
      "Epoch: 0046 cost= 1.744315028 accuracy= 0.412\n",
      "Epoch: 0047 cost= 1.755529353 accuracy= 0.412\n",
      "Epoch: 0048 cost= 1.724773884 accuracy= 0.412\n",
      "Epoch: 0049 cost= 1.707064952 accuracy= 0.412\n",
      "Epoch: 0050 cost= 1.678030099 accuracy= 0.412\n",
      "Epoch: 0051 cost= 1.672190751 accuracy= 0.414\n",
      "Epoch: 0052 cost= 1.655065979 accuracy= 0.422\n",
      "Epoch: 0053 cost= 1.663975545 accuracy= 0.435\n",
      "Epoch: 0054 cost= 1.632561173 accuracy= 0.439\n",
      "Epoch: 0055 cost= 1.609997170 accuracy= 0.432\n",
      "Epoch: 0056 cost= 1.604233333 accuracy= 0.455\n",
      "Epoch: 0057 cost= 1.571290714 accuracy= 0.457\n",
      "Epoch: 0058 cost= 1.561441030 accuracy= 0.470\n",
      "Epoch: 0059 cost= 1.574633547 accuracy= 0.464\n",
      "Epoch: 0060 cost= 1.565469078 accuracy= 0.467\n",
      "Epoch: 0061 cost= 1.549933910 accuracy= 0.464\n",
      "Epoch: 0062 cost= 1.548478416 accuracy= 0.467\n",
      "Epoch: 0063 cost= 1.563360248 accuracy= 0.466\n",
      "Epoch: 0064 cost= 1.547283939 accuracy= 0.475\n",
      "Epoch: 0065 cost= 1.527469754 accuracy= 0.466\n",
      "Epoch: 0066 cost= 1.530617152 accuracy= 0.472\n",
      "Epoch: 0067 cost= 1.524701135 accuracy= 0.469\n",
      "Epoch: 0068 cost= 1.519409418 accuracy= 0.477\n",
      "Epoch: 0069 cost= 1.512752805 accuracy= 0.474\n",
      "Epoch: 0070 cost= 1.526780588 accuracy= 0.478\n",
      "Epoch: 0071 cost= 1.517374141 accuracy= 0.477\n",
      "Epoch: 0072 cost= 1.514851655 accuracy= 0.479\n",
      "Epoch: 0073 cost= 1.511829938 accuracy= 0.480\n",
      "Epoch: 0074 cost= 1.522220646 accuracy= 0.481\n",
      "Epoch: 0075 cost= 1.506224411 accuracy= 0.479\n",
      "Epoch: 0076 cost= 1.520991496 accuracy= 0.479\n",
      "Epoch: 0077 cost= 1.502698541 accuracy= 0.480\n",
      "Epoch: 0078 cost= 1.512468900 accuracy= 0.471\n",
      "Epoch: 0079 cost= 1.500945006 accuracy= 0.479\n",
      "Epoch: 0080 cost= 1.520303879 accuracy= 0.479\n",
      "Epoch: 0081 cost= 1.523733803 accuracy= 0.477\n",
      "Epoch: 0082 cost= 1.512018221 accuracy= 0.476\n",
      "Epoch: 0083 cost= 1.503609572 accuracy= 0.481\n",
      "Epoch: 0084 cost= 1.495714630 accuracy= 0.478\n",
      "Epoch: 0085 cost= 1.502607720 accuracy= 0.480\n",
      "Epoch: 0086 cost= 1.488137296 accuracy= 0.478\n",
      "Epoch: 0087 cost= 1.506653956 accuracy= 0.473\n",
      "Epoch: 0088 cost= 1.506228328 accuracy= 0.478\n",
      "Epoch: 0089 cost= 1.521091069 accuracy= 0.483\n",
      "Epoch: 0090 cost= 1.482642974 accuracy= 0.481\n",
      "Epoch: 0091 cost= 1.506565758 accuracy= 0.482\n",
      "Epoch: 0092 cost= 1.492574402 accuracy= 0.477\n",
      "Epoch: 0093 cost= 1.503493479 accuracy= 0.474\n",
      "Epoch: 0094 cost= 1.502575534 accuracy= 0.481\n",
      "Epoch: 0095 cost= 1.511892898 accuracy= 0.477\n",
      "Epoch: 0096 cost= 1.482003348 accuracy= 0.483\n",
      "Epoch: 0097 cost= 1.502742086 accuracy= 0.479\n",
      "Epoch: 0098 cost= 1.488585711 accuracy= 0.479\n",
      "Epoch: 0099 cost= 1.505957007 accuracy= 0.484\n",
      "Epoch: 0100 cost= 1.475664990 accuracy= 0.483\n",
      "Epoch: 0101 cost= 1.486262134 accuracy= 0.484\n",
      "Epoch: 0102 cost= 1.466333083 accuracy= 0.485\n",
      "Epoch: 0103 cost= 1.470306192 accuracy= 0.484\n",
      "Epoch: 0104 cost= 1.478992837 accuracy= 0.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0105 cost= 1.479519657 accuracy= 0.482\n",
      "Epoch: 0106 cost= 1.481776953 accuracy= 0.484\n",
      "Epoch: 0107 cost= 1.465956977 accuracy= 0.484\n",
      "Epoch: 0108 cost= 1.474354591 accuracy= 0.483\n",
      "Epoch: 0109 cost= 1.482027973 accuracy= 0.484\n",
      "Epoch: 0110 cost= 1.477621521 accuracy= 0.485\n",
      "Epoch: 0111 cost= 1.472430195 accuracy= 0.487\n",
      "Epoch: 0112 cost= 1.467349138 accuracy= 0.486\n",
      "Epoch: 0113 cost= 1.467487318 accuracy= 0.481\n",
      "Epoch: 0114 cost= 1.485091891 accuracy= 0.484\n",
      "Epoch: 0115 cost= 1.466468743 accuracy= 0.485\n",
      "Epoch: 0116 cost= 1.474752733 accuracy= 0.485\n",
      "Epoch: 0117 cost= 1.468903422 accuracy= 0.485\n",
      "Epoch: 0118 cost= 1.465414524 accuracy= 0.486\n",
      "Epoch: 0119 cost= 1.472287604 accuracy= 0.487\n",
      "Epoch: 0120 cost= 1.454474432 accuracy= 0.487\n",
      "Epoch: 0121 cost= 1.470588991 accuracy= 0.486\n",
      "Epoch: 0122 cost= 1.463894895 accuracy= 0.482\n",
      "Epoch: 0123 cost= 1.477526750 accuracy= 0.471\n",
      "Epoch: 0124 cost= 1.487645030 accuracy= 0.473\n",
      "Epoch: 0125 cost= 1.485202721 accuracy= 0.482\n",
      "Epoch: 0126 cost= 1.479352883 accuracy= 0.481\n",
      "Epoch: 0127 cost= 1.470295514 accuracy= 0.478\n",
      "Epoch: 0128 cost= 1.464838334 accuracy= 0.484\n",
      "Epoch: 0129 cost= 1.463507942 accuracy= 0.487\n",
      "Epoch: 0130 cost= 1.464919482 accuracy= 0.486\n",
      "Epoch: 0131 cost= 1.455535701 accuracy= 0.485\n",
      "Epoch: 0132 cost= 1.468225735 accuracy= 0.482\n",
      "Epoch: 0133 cost= 1.448837144 accuracy= 0.486\n",
      "Epoch: 0134 cost= 1.479963575 accuracy= 0.483\n",
      "Epoch: 0135 cost= 1.463690792 accuracy= 0.486\n",
      "Epoch: 0136 cost= 1.472190584 accuracy= 0.486\n",
      "Epoch: 0137 cost= 1.488095369 accuracy= 0.482\n",
      "Epoch: 0138 cost= 1.460282428 accuracy= 0.480\n",
      "Epoch: 0139 cost= 1.464898058 accuracy= 0.481\n",
      "Epoch: 0140 cost= 1.455329095 accuracy= 0.487\n",
      "Epoch: 0141 cost= 1.458666444 accuracy= 0.485\n",
      "Epoch: 0142 cost= 1.452232838 accuracy= 0.485\n",
      "Epoch: 0143 cost= 1.450014949 accuracy= 0.478\n",
      "Epoch: 0144 cost= 1.432779619 accuracy= 0.486\n",
      "Epoch: 0145 cost= 1.451746668 accuracy= 0.487\n",
      "Epoch: 0146 cost= 1.447665572 accuracy= 0.480\n",
      "Epoch: 0147 cost= 1.458959954 accuracy= 0.485\n",
      "Epoch: 0148 cost= 1.456145661 accuracy= 0.486\n",
      "Epoch: 0149 cost= 1.429483652 accuracy= 0.485\n",
      "Epoch: 0150 cost= 1.443366442 accuracy= 0.485\n",
      "Epoch: 0151 cost= 1.444911804 accuracy= 0.482\n",
      "Epoch: 0152 cost= 1.445763929 accuracy= 0.488\n",
      "Epoch: 0153 cost= 1.447553635 accuracy= 0.487\n",
      "Epoch: 0154 cost= 1.467260293 accuracy= 0.481\n",
      "Epoch: 0155 cost= 1.475812163 accuracy= 0.486\n",
      "Epoch: 0156 cost= 1.432676945 accuracy= 0.486\n",
      "Epoch: 0157 cost= 1.451562711 accuracy= 0.485\n",
      "Epoch: 0158 cost= 1.467642154 accuracy= 0.481\n",
      "Epoch: 0159 cost= 1.460271716 accuracy= 0.486\n",
      "Epoch: 0160 cost= 1.449563605 accuracy= 0.484\n",
      "Epoch: 0161 cost= 1.447709833 accuracy= 0.478\n",
      "Epoch: 0162 cost= 1.461747425 accuracy= 0.487\n",
      "Epoch: 0163 cost= 1.443893926 accuracy= 0.487\n",
      "Epoch: 0164 cost= 1.446244836 accuracy= 0.486\n",
      "Epoch: 0165 cost= 1.461098398 accuracy= 0.486\n",
      "Epoch: 0166 cost= 1.444565432 accuracy= 0.486\n",
      "Epoch: 0167 cost= 1.449166008 accuracy= 0.487\n",
      "Epoch: 0168 cost= 1.456509965 accuracy= 0.487\n",
      "Epoch: 0169 cost= 1.461990799 accuracy= 0.486\n",
      "Epoch: 0170 cost= 1.459589737 accuracy= 0.486\n",
      "Epoch: 0171 cost= 1.453198177 accuracy= 0.486\n",
      "Epoch: 0172 cost= 1.463250841 accuracy= 0.485\n",
      "Epoch: 0173 cost= 1.437608923 accuracy= 0.485\n",
      "Epoch: 0174 cost= 1.444786549 accuracy= 0.487\n",
      "Epoch: 0175 cost= 1.435314553 accuracy= 0.486\n",
      "Epoch: 0176 cost= 1.433293172 accuracy= 0.488\n",
      "Epoch: 0177 cost= 1.438451546 accuracy= 0.483\n",
      "Epoch: 0178 cost= 1.446964485 accuracy= 0.487\n",
      "Epoch: 0179 cost= 1.450426459 accuracy= 0.487\n",
      "Epoch: 0180 cost= 1.448371410 accuracy= 0.486\n",
      "Epoch: 0181 cost= 1.451572214 accuracy= 0.486\n",
      "Epoch: 0182 cost= 1.452835304 accuracy= 0.487\n",
      "Epoch: 0183 cost= 1.452582819 accuracy= 0.485\n",
      "Epoch: 0184 cost= 1.450974090 accuracy= 0.484\n",
      "Epoch: 0185 cost= 1.461762122 accuracy= 0.483\n",
      "Epoch: 0186 cost= 1.441748125 accuracy= 0.484\n",
      "Epoch: 0187 cost= 1.452076367 accuracy= 0.486\n",
      "Epoch: 0188 cost= 1.444817543 accuracy= 0.483\n",
      "Epoch: 0189 cost= 1.443759390 accuracy= 0.485\n",
      "Epoch: 0190 cost= 1.437488760 accuracy= 0.488\n",
      "Epoch: 0191 cost= 1.446021523 accuracy= 0.486\n",
      "Epoch: 0192 cost= 1.435352530 accuracy= 0.485\n",
      "Epoch: 0193 cost= 1.428245068 accuracy= 0.487\n",
      "Epoch: 0194 cost= 1.440434047 accuracy= 0.484\n",
      "Epoch: 0195 cost= 1.433673842 accuracy= 0.487\n",
      "Epoch: 0196 cost= 1.448720523 accuracy= 0.487\n",
      "Epoch: 0197 cost= 1.474254813 accuracy= 0.484\n",
      "Epoch: 0198 cost= 1.456384540 accuracy= 0.485\n",
      "Epoch: 0199 cost= 1.436264072 accuracy= 0.487\n",
      "Epoch: 0200 cost= 1.439694132 accuracy= 0.487\n",
      "Epoch: 0201 cost= 1.428483043 accuracy= 0.487\n",
      "Epoch: 0202 cost= 1.423870393 accuracy= 0.488\n",
      "Epoch: 0203 cost= 1.441558957 accuracy= 0.487\n",
      "Epoch: 0204 cost= 1.430663722 accuracy= 0.486\n",
      "Epoch: 0205 cost= 1.428242241 accuracy= 0.486\n",
      "Epoch: 0206 cost= 1.414062040 accuracy= 0.486\n",
      "Epoch: 0207 cost= 1.427241768 accuracy= 0.487\n",
      "Epoch: 0208 cost= 1.424574120 accuracy= 0.487\n",
      "Epoch: 0209 cost= 1.420241679 accuracy= 0.487\n",
      "Epoch: 0210 cost= 1.430079290 accuracy= 0.488\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_aloi_tree1.ckpt\n",
      "Epoch: 0001 cost= 2.787271329 accuracy= 0.051\n",
      "Epoch: 0002 cost= 2.409470252 accuracy= 0.187\n",
      "Epoch: 0003 cost= 1.969350219 accuracy= 0.412\n",
      "Epoch: 0004 cost= 1.910522750 accuracy= 0.412\n",
      "Epoch: 0005 cost= 1.872160946 accuracy= 0.412\n",
      "Epoch: 0006 cost= 1.838980930 accuracy= 0.412\n",
      "Epoch: 0007 cost= 1.847066828 accuracy= 0.412\n",
      "Epoch: 0008 cost= 1.832721625 accuracy= 0.412\n",
      "Epoch: 0009 cost= 1.841597983 accuracy= 0.412\n",
      "Epoch: 0010 cost= 1.830401404 accuracy= 0.412\n",
      "Epoch: 0011 cost= 1.838242633 accuracy= 0.412\n",
      "Epoch: 0012 cost= 1.835127064 accuracy= 0.412\n",
      "Epoch: 0013 cost= 1.842142412 accuracy= 0.412\n",
      "Epoch: 0014 cost= 1.846226454 accuracy= 0.412\n",
      "Epoch: 0015 cost= 1.822744438 accuracy= 0.412\n",
      "Epoch: 0016 cost= 1.824581776 accuracy= 0.412\n",
      "Epoch: 0017 cost= 1.819228956 accuracy= 0.412\n",
      "Epoch: 0018 cost= 1.810982108 accuracy= 0.412\n",
      "Epoch: 0019 cost= 1.829222679 accuracy= 0.412\n",
      "Epoch: 0020 cost= 1.824811305 accuracy= 0.412\n",
      "Epoch: 0021 cost= 1.836168970 accuracy= 0.412\n",
      "Epoch: 0022 cost= 1.843317458 accuracy= 0.412\n",
      "Epoch: 0023 cost= 1.831300378 accuracy= 0.412\n",
      "Epoch: 0024 cost= 1.821306212 accuracy= 0.412\n",
      "Epoch: 0025 cost= 1.838598626 accuracy= 0.412\n",
      "Epoch: 0026 cost= 1.843230282 accuracy= 0.412\n",
      "Epoch: 0027 cost= 1.820703132 accuracy= 0.412\n",
      "Epoch: 0028 cost= 1.847093122 accuracy= 0.412\n",
      "Epoch: 0029 cost= 1.845133219 accuracy= 0.412\n",
      "Epoch: 0030 cost= 1.848024913 accuracy= 0.412\n",
      "Epoch: 0031 cost= 1.825369869 accuracy= 0.412\n",
      "Epoch: 0032 cost= 1.827238389 accuracy= 0.412\n",
      "Epoch: 0033 cost= 1.830934865 accuracy= 0.412\n",
      "Epoch: 0034 cost= 1.846303412 accuracy= 0.412\n",
      "Epoch: 0035 cost= 1.824316042 accuracy= 0.412\n",
      "Epoch: 0036 cost= 1.832572937 accuracy= 0.412\n",
      "Epoch: 0037 cost= 1.833422746 accuracy= 0.412\n",
      "Epoch: 0038 cost= 1.816270505 accuracy= 0.412\n",
      "Epoch: 0039 cost= 1.847787329 accuracy= 0.412\n",
      "Epoch: 0040 cost= 1.831273488 accuracy= 0.412\n",
      "Epoch: 0041 cost= 1.846240725 accuracy= 0.412\n",
      "Epoch: 0042 cost= 1.835346750 accuracy= 0.412\n",
      "Epoch: 0043 cost= 1.834015557 accuracy= 0.412\n",
      "Epoch: 0044 cost= 1.820364680 accuracy= 0.412\n",
      "Epoch: 0045 cost= 1.820045556 accuracy= 0.412\n",
      "Epoch: 0046 cost= 1.824224523 accuracy= 0.412\n",
      "Epoch: 0047 cost= 1.824153900 accuracy= 0.412\n",
      "Epoch: 0048 cost= 1.823024000 accuracy= 0.412\n",
      "Epoch: 0049 cost= 1.817388586 accuracy= 0.412\n",
      "Epoch: 0050 cost= 1.823037062 accuracy= 0.412\n",
      "Epoch: 0051 cost= 1.828200306 accuracy= 0.412\n",
      "Epoch: 0052 cost= 1.813835076 accuracy= 0.412\n",
      "Epoch: 0053 cost= 1.819648368 accuracy= 0.412\n",
      "Epoch: 0054 cost= 1.825098855 accuracy= 0.412\n",
      "Epoch: 0055 cost= 1.795521191 accuracy= 0.412\n",
      "Epoch: 0056 cost= 1.819993785 accuracy= 0.412\n",
      "Epoch: 0057 cost= 1.807956134 accuracy= 0.412\n",
      "Epoch: 0058 cost= 1.782613431 accuracy= 0.412\n",
      "Epoch: 0059 cost= 1.775424583 accuracy= 0.412\n",
      "Epoch: 0060 cost= 1.794657741 accuracy= 0.412\n",
      "Epoch: 0061 cost= 1.768428138 accuracy= 0.412\n",
      "Epoch: 0062 cost= 1.743173020 accuracy= 0.412\n",
      "Epoch: 0063 cost= 1.756194574 accuracy= 0.412\n",
      "Epoch: 0064 cost= 1.759151970 accuracy= 0.412\n",
      "Epoch: 0065 cost= 1.723926153 accuracy= 0.412\n",
      "Epoch: 0066 cost= 1.713399683 accuracy= 0.412\n",
      "Epoch: 0067 cost= 1.677570820 accuracy= 0.413\n",
      "Epoch: 0068 cost= 1.658712557 accuracy= 0.424\n",
      "Epoch: 0069 cost= 1.652825219 accuracy= 0.456\n",
      "Epoch: 0070 cost= 1.617269686 accuracy= 0.459\n",
      "Epoch: 0071 cost= 1.611604588 accuracy= 0.459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0072 cost= 1.579346214 accuracy= 0.449\n",
      "Epoch: 0073 cost= 1.573608586 accuracy= 0.459\n",
      "Epoch: 0074 cost= 1.572520750 accuracy= 0.469\n",
      "Epoch: 0075 cost= 1.579094785 accuracy= 0.467\n",
      "Epoch: 0076 cost= 1.561293840 accuracy= 0.462\n",
      "Epoch: 0077 cost= 1.563018220 accuracy= 0.451\n",
      "Epoch: 0078 cost= 1.546925800 accuracy= 0.463\n",
      "Epoch: 0079 cost= 1.546865531 accuracy= 0.473\n",
      "Epoch: 0080 cost= 1.515475563 accuracy= 0.473\n",
      "Epoch: 0081 cost= 1.529055987 accuracy= 0.472\n",
      "Epoch: 0082 cost= 1.521417175 accuracy= 0.476\n",
      "Epoch: 0083 cost= 1.516602533 accuracy= 0.474\n",
      "Epoch: 0084 cost= 1.530458740 accuracy= 0.474\n",
      "Epoch: 0085 cost= 1.524054698 accuracy= 0.475\n",
      "Epoch: 0086 cost= 1.503591384 accuracy= 0.475\n",
      "Epoch: 0087 cost= 1.488098979 accuracy= 0.475\n",
      "Epoch: 0088 cost= 1.507535219 accuracy= 0.474\n",
      "Epoch: 0089 cost= 1.505548630 accuracy= 0.474\n",
      "Epoch: 0090 cost= 1.510244778 accuracy= 0.476\n",
      "Epoch: 0091 cost= 1.490437405 accuracy= 0.470\n",
      "Epoch: 0092 cost= 1.509289435 accuracy= 0.470\n",
      "Epoch: 0093 cost= 1.507398895 accuracy= 0.471\n",
      "Epoch: 0094 cost= 1.522911191 accuracy= 0.480\n",
      "Epoch: 0095 cost= 1.504270758 accuracy= 0.478\n",
      "Epoch: 0096 cost= 1.486729128 accuracy= 0.480\n",
      "Epoch: 0097 cost= 1.514441490 accuracy= 0.476\n",
      "Epoch: 0098 cost= 1.510817834 accuracy= 0.479\n",
      "Epoch: 0099 cost= 1.502052324 accuracy= 0.480\n",
      "Epoch: 0100 cost= 1.491753033 accuracy= 0.481\n",
      "Epoch: 0101 cost= 1.480673552 accuracy= 0.480\n",
      "Epoch: 0102 cost= 1.497228793 accuracy= 0.480\n",
      "Epoch: 0103 cost= 1.488178968 accuracy= 0.479\n",
      "Epoch: 0104 cost= 1.479246378 accuracy= 0.480\n",
      "Epoch: 0105 cost= 1.488615717 accuracy= 0.481\n",
      "Epoch: 0106 cost= 1.493125694 accuracy= 0.483\n",
      "Epoch: 0107 cost= 1.481433596 accuracy= 0.481\n",
      "Epoch: 0108 cost= 1.502058438 accuracy= 0.484\n",
      "Epoch: 0109 cost= 1.481397612 accuracy= 0.482\n",
      "Epoch: 0110 cost= 1.499582308 accuracy= 0.482\n",
      "Epoch: 0111 cost= 1.462113976 accuracy= 0.480\n",
      "Epoch: 0112 cost= 1.481126087 accuracy= 0.483\n",
      "Epoch: 0113 cost= 1.494516168 accuracy= 0.481\n",
      "Epoch: 0114 cost= 1.500285932 accuracy= 0.482\n",
      "Epoch: 0115 cost= 1.493069904 accuracy= 0.481\n",
      "Epoch: 0116 cost= 1.468372004 accuracy= 0.483\n",
      "Epoch: 0117 cost= 1.470744899 accuracy= 0.482\n",
      "Epoch: 0118 cost= 1.481464182 accuracy= 0.482\n",
      "Epoch: 0119 cost= 1.473488518 accuracy= 0.484\n",
      "Epoch: 0120 cost= 1.473928417 accuracy= 0.485\n",
      "Epoch: 0121 cost= 1.478280749 accuracy= 0.484\n",
      "Epoch: 0122 cost= 1.481157286 accuracy= 0.485\n",
      "Epoch: 0123 cost= 1.482435891 accuracy= 0.483\n",
      "Epoch: 0124 cost= 1.478832177 accuracy= 0.485\n",
      "Epoch: 0125 cost= 1.462866391 accuracy= 0.483\n",
      "Epoch: 0126 cost= 1.486440182 accuracy= 0.484\n",
      "Epoch: 0127 cost= 1.473757250 accuracy= 0.486\n",
      "Epoch: 0128 cost= 1.482252700 accuracy= 0.483\n",
      "Epoch: 0129 cost= 1.463545561 accuracy= 0.482\n",
      "Epoch: 0130 cost= 1.469903690 accuracy= 0.486\n",
      "Epoch: 0131 cost= 1.471016152 accuracy= 0.486\n",
      "Epoch: 0132 cost= 1.454824993 accuracy= 0.480\n",
      "Epoch: 0133 cost= 1.479911293 accuracy= 0.484\n",
      "Epoch: 0134 cost= 1.468508840 accuracy= 0.485\n",
      "Epoch: 0135 cost= 1.477858646 accuracy= 0.483\n",
      "Epoch: 0136 cost= 1.453970024 accuracy= 0.484\n",
      "Epoch: 0137 cost= 1.488207715 accuracy= 0.484\n",
      "Epoch: 0138 cost= 1.467037014 accuracy= 0.484\n",
      "Epoch: 0139 cost= 1.461094924 accuracy= 0.485\n",
      "Epoch: 0140 cost= 1.488433617 accuracy= 0.482\n",
      "Epoch: 0141 cost= 1.498853547 accuracy= 0.486\n",
      "Epoch: 0142 cost= 1.468718648 accuracy= 0.483\n",
      "Epoch: 0143 cost= 1.465591482 accuracy= 0.486\n",
      "Epoch: 0144 cost= 1.446892159 accuracy= 0.482\n",
      "Epoch: 0145 cost= 1.454953773 accuracy= 0.487\n",
      "Epoch: 0146 cost= 1.459506239 accuracy= 0.487\n",
      "Epoch: 0147 cost= 1.458313891 accuracy= 0.484\n",
      "Epoch: 0148 cost= 1.459877287 accuracy= 0.486\n",
      "Epoch: 0149 cost= 1.444874934 accuracy= 0.487\n",
      "Epoch: 0150 cost= 1.489025950 accuracy= 0.471\n",
      "Epoch: 0151 cost= 1.514999577 accuracy= 0.470\n",
      "Epoch: 0152 cost= 1.533995186 accuracy= 0.486\n",
      "Epoch: 0153 cost= 1.500544446 accuracy= 0.478\n",
      "Epoch: 0154 cost= 1.480239476 accuracy= 0.484\n",
      "Epoch: 0155 cost= 1.495026180 accuracy= 0.475\n",
      "Epoch: 0156 cost= 1.471350959 accuracy= 0.481\n",
      "Epoch: 0157 cost= 1.446446725 accuracy= 0.477\n",
      "Epoch: 0158 cost= 1.475747347 accuracy= 0.485\n",
      "Epoch: 0159 cost= 1.447621652 accuracy= 0.486\n",
      "Epoch: 0160 cost= 1.451395597 accuracy= 0.487\n",
      "Epoch: 0161 cost= 1.448115332 accuracy= 0.486\n",
      "Epoch: 0162 cost= 1.465785027 accuracy= 0.487\n",
      "Epoch: 0163 cost= 1.461423363 accuracy= 0.487\n",
      "Epoch: 0164 cost= 1.460740158 accuracy= 0.480\n",
      "Epoch: 0165 cost= 1.439432740 accuracy= 0.488\n",
      "Epoch: 0166 cost= 1.453695791 accuracy= 0.486\n",
      "Epoch: 0167 cost= 1.449105382 accuracy= 0.488\n",
      "Epoch: 0168 cost= 1.442613517 accuracy= 0.486\n",
      "Epoch: 0169 cost= 1.455206054 accuracy= 0.487\n",
      "Epoch: 0170 cost= 1.463713186 accuracy= 0.488\n",
      "Epoch: 0171 cost= 1.450380581 accuracy= 0.484\n",
      "Epoch: 0172 cost= 1.439380850 accuracy= 0.479\n",
      "Epoch: 0173 cost= 1.443973490 accuracy= 0.487\n",
      "Epoch: 0174 cost= 1.446739861 accuracy= 0.488\n",
      "Epoch: 0175 cost= 1.453624214 accuracy= 0.486\n",
      "Epoch: 0176 cost= 1.428255933 accuracy= 0.485\n",
      "Epoch: 0177 cost= 1.456880689 accuracy= 0.487\n",
      "Epoch: 0178 cost= 1.446601084 accuracy= 0.488\n",
      "Epoch: 0179 cost= 1.456056867 accuracy= 0.488\n",
      "Epoch: 0180 cost= 1.444891776 accuracy= 0.488\n",
      "Epoch: 0181 cost= 1.477651971 accuracy= 0.488\n",
      "Epoch: 0182 cost= 1.447244559 accuracy= 0.486\n",
      "Epoch: 0183 cost= 1.454661557 accuracy= 0.480\n",
      "Epoch: 0184 cost= 1.451779553 accuracy= 0.488\n",
      "Epoch: 0185 cost= 1.431484614 accuracy= 0.487\n",
      "Epoch: 0186 cost= 1.432144318 accuracy= 0.487\n",
      "Epoch: 0187 cost= 1.456192051 accuracy= 0.489\n",
      "Epoch: 0188 cost= 1.428599494 accuracy= 0.488\n",
      "Epoch: 0189 cost= 1.441467796 accuracy= 0.487\n",
      "Epoch: 0190 cost= 1.433208908 accuracy= 0.488\n",
      "Epoch: 0191 cost= 1.438279748 accuracy= 0.488\n",
      "Epoch: 0192 cost= 1.443311913 accuracy= 0.488\n",
      "Epoch: 0193 cost= 1.436990482 accuracy= 0.489\n",
      "Epoch: 0194 cost= 1.432734830 accuracy= 0.485\n",
      "Epoch: 0195 cost= 1.426754713 accuracy= 0.482\n",
      "Epoch: 0196 cost= 1.464838965 accuracy= 0.484\n",
      "Epoch: 0197 cost= 1.443539602 accuracy= 0.486\n",
      "Epoch: 0198 cost= 1.439279403 accuracy= 0.485\n",
      "Epoch: 0199 cost= 1.450728961 accuracy= 0.488\n",
      "Epoch: 0200 cost= 1.426164814 accuracy= 0.486\n",
      "Epoch: 0201 cost= 1.432783144 accuracy= 0.488\n",
      "Epoch: 0202 cost= 1.439968331 accuracy= 0.488\n",
      "Epoch: 0203 cost= 1.432556510 accuracy= 0.486\n",
      "Epoch: 0204 cost= 1.419921790 accuracy= 0.487\n",
      "Epoch: 0205 cost= 1.462744662 accuracy= 0.477\n",
      "Epoch: 0206 cost= 1.433588794 accuracy= 0.488\n",
      "Epoch: 0207 cost= 1.453511442 accuracy= 0.482\n",
      "Epoch: 0208 cost= 1.461698413 accuracy= 0.485\n",
      "Epoch: 0209 cost= 1.435852647 accuracy= 0.488\n",
      "Epoch: 0210 cost= 1.434157116 accuracy= 0.488\n",
      "Optimization Finished!\n",
      "Model saved in: ./class_djinn_aloi_tree2.ckpt\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn.py:276: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_aloi_tree0.ckpt\n",
      "Model 0 restored\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_aloi_tree1.ckpt\n",
      "Model 1 restored\n",
      "INFO:tensorflow:Restoring parameters from ./class_djinn_aloi_tree2.ckpt\n",
      "Model 2 restored\n",
      "(3, 6261, 12)\n"
     ]
    }
   ],
   "source": [
    "from djinn import djinn\n",
    "\n",
    "print(\"djinn aloi\")    \n",
    "modelname=\"class_djinn_aloi\"   # name the model\n",
    "ntrees=3                 # number of trees = number of neural nets in ensemble\n",
    "maxdepth=4               # max depth of tree -- optimize this for each data set\n",
    "dropout_keep=1.0 \n",
    "\n",
    "#initialize the model\n",
    "model=djinn.DJINN_Classifier(ntrees,maxdepth,dropout_keep)\n",
    "\n",
    "x_train, y_train, x_test, y_test = out_cnn_train, y_train, out_cnn_test, y_test \n",
    "# find optimal settings: this function returns dict with hyper-parameters\n",
    "# each djinn function accepts random seeds for reproducible behavior\n",
    "# optimal=model.get_hyperparameters(x_train, y_train, random_state=1)\n",
    "# batchsize=optimal['batch_size']\n",
    "# learnrate=optimal['learn_rate']\n",
    "# epochs=optimal['epochs']\n",
    "\n",
    "batchsize=1750\n",
    "learnrate=0.00643986000283503\n",
    "epochs=210\n",
    "\n",
    "# train the model with hyperparameters determined above\n",
    "model.train(x_train,y_train,epochs=epochs,learn_rate=learnrate, batch_size=batchsize, \n",
    "              display_step=1, save_files=True, file_name=modelname, \n",
    "              save_model=True,model_name=modelname, random_state=1)\n",
    "\n",
    "# *note there is a function model.fit(x_train,y_train, ... ) that wraps \n",
    "# get_hyperparameters() and train(), so that you do not have to manually\n",
    "# pass hyperparameters to train(). However, get_hyperparameters() can\n",
    "# be expensive, so I recommend running it once per dataset and using those\n",
    "# hyperparameter values in train() to save computational time\n",
    "# make predictions\n",
    "m=model.predict(x_test) #returns the median prediction if more than one tree\n",
    "\n",
    "import sklearn\n",
    "#evaluate results\n",
    "acc=sklearn.metrics.accuracy_score(y_test,m.flatten())  \n",
    "#close model \n",
    "model.close_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + ( SVM, XGB, DTree, ExtraTrees, RandomFores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to Random Forest Classifier to predict its class\n",
    "predictions = rf.predict(out_cnn_test)\n",
    "accuracy_CNN_RF=accuracy_score(predictions , y_test)\n",
    "#print('CNN+RF : Accuracy:', accuracy_CNN_RF, '%.')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "Ext = ExtraTreesClassifier(n_estimators=100)\n",
    "Ext.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictions = Ext.predict(out_cnn_test)\n",
    "accuracy_CNN_Ext=accuracy_score(predictions , y_test)\n",
    "#print('CNN+Extrat : Accuracy:', accuracy_CNN_Ext, '%.')\n",
    "\n",
    "\n",
    "#Applying SVC (Support Vector Classification)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "svm.fit(out_cnn_train, y_train)\n",
    "#print('The accuracy of the SVM classifier on training data is {:.4f}'.format(svm.score(x_train, y_train)))\n",
    "\n",
    "\n",
    "#Applying XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf = xgb_clf.fit(out_cnn_train, y_train)\n",
    "#print('The accuracy of the XGBoost classifier on training data is {:.4f}'.format(xgb_clf.score(x_train, y_train)))\n",
    "\n",
    "\n",
    "#Applying Decision Tree\n",
    "from sklearn import tree\n",
    "#Create tree object\n",
    "decision_tree = tree.DecisionTreeClassifier()\n",
    "#Train DT based on scaled training set\n",
    "decision_tree.fit(out_cnn_train, y_train)\n",
    "#Print performance\n",
    "#print('The accuracy of the Decision Tree classifier on training data is {:.4f}'.format(decision_tree.score(x_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 10-fold cross-validation with the best KNN model\n",
    "# This will allow us to get a better results\n",
    "cx_train = np.concatenate((x_train, x_test), 0)\n",
    "cy_train = np.concatenate((y_train, y_test), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by RandomForest, ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier : from dataset originl\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train_, y_train_)\n",
    "predictions = rf.predict(x_test_)\n",
    "accuracy_RF=accuracy_score(predictions , y_test_)\n",
    "#print('RF : Accuracy:', accuracy_RF, '%.')\n",
    "\n",
    "# ExtraTreesClassifier : from dataset originl\n",
    "Extra = ExtraTreesClassifier(n_estimators=100)\n",
    "Extra.fit(x_train_, y_train_)\n",
    "predictions = Extra.predict(x_test_)\n",
    "accuracy_Extra=accuracy_score(predictions , y_test_)\n",
    "#print('Extra : Accuracy:', accuracy_Extra, '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RF          :: 0.9933 %.\n",
      "Accuracy Extrat      :: 0.9912 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN         :: 0.8349 %.\n",
      "Accuracy CNN+RF      :: 0.8786 %.\n",
      "Accuracy CNN+Extrat  :: 0.8930 %.\n",
      "Accuracy CNN+SVM     :: 0.8238 %.\n",
      "Accuracy CNN+XGBoost :: 0.8756 %.\n",
      "Accuracy CNN+DTree   :: 0.7855 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN+RF+MLP  :: 0.4878 %.\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Accuracy CNN+SVM using cv=10     :: 0.8487 %.\n",
      "Accuracy CNN+rf  using cv=10     :: 0.8986 %.\n",
      "Accuracy CNN+XGBoost using cv=10 :: 0.9032 %.\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy RF          ::',  \"{:.4f}\".format(accuracy_RF),'%.')\n",
    "print('Accuracy Extrat      ::',  \"{:.4f}\".format(accuracy_Extra),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN         ::',  \"{:.4f}\".format(accuracy_CNN), '%.')\n",
    "print('Accuracy CNN+RF      ::',  \"{:.4f}\".format(accuracy_CNN_RF), '%.')\n",
    "print('Accuracy CNN+Extrat  ::',  \"{:.4f}\".format(accuracy_CNN_Ext), '%.')\n",
    "print('Accuracy CNN+SVM     :: {:.4f}'.format(svm.score(x_test, y_test)),'%.')\n",
    "print('Accuracy CNN+XGBoost :: {:.4f}'.format(xgb_clf.score(x_test, y_test)),'%.')\n",
    "print('Accuracy CNN+DTree   :: {:.4f}'.format(decision_tree.score(x_test, y_test)),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN+RF+MLP  ::',  \"{:.4f}\".format(acc),'%.')\n",
    "print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print('Accuracy CNN+SVM using cv=10     :: {:.4f}' .format(cross_val_score(svm, cx_train, cy_train, cv=10, scoring='accuracy').mean()),'%.')\n",
    "print('Accuracy CNN+rf  using cv=10     :: {:.4f}' .format(cross_val_score(rf, cx_train, cy_train, cv=10, scoring='accuracy').mean() ),'%.')\n",
    "print('Accuracy CNN+XGBoost using cv=10 :: {:.4f}'.format(cross_val_score(xgb_clf, cx_train, cy_train, cv=10, scoring='accuracy').mean() ),'%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/38957/keras-conv1d-for-simple-data-target-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
