{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# cnn model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import sklearn\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "try: from sklearn.model_selection import train_test_split\n",
    "except: from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test,m):\n",
    "    #evaluate results\n",
    "    mse=sklearn.metrics.mean_squared_error(y_test,m)\n",
    "    mabs=sklearn.metrics.mean_absolute_error(y_test,m)\n",
    "    exvar=sklearn.metrics.explained_variance_score(y_test,m)   \n",
    "    print('Mean Squa Error :',mse)\n",
    "    print('Mean Abso Error :',mabs)\n",
    "    print('Expl. Variance  :',exvar,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the train & test and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.260</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.453</td>\n",
       "      <td>3.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.189</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.125</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.027</td>\n",
       "      <td>0.331</td>\n",
       "      <td>1.472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.807</td>\n",
       "      <td>3.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.094</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.886</td>\n",
       "      <td>5.390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2  3  4      5      6\n",
       "0  3.260  0.829  1.676  0  1  1.453  3.770\n",
       "1  2.189  0.580  0.863  0  0  1.348  3.115\n",
       "2  2.125  0.638  0.831  0  0  1.348  3.531\n",
       "3  3.027  0.331  1.472  1  0  1.807  3.510\n",
       "4  2.094  0.827  0.860  0  0  1.886  5.390"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "# load data\n",
    "df=pd.read_csv('qsar_fish_toxicity.csv', sep=';', header = None)\n",
    "\n",
    "# drop nan \n",
    "df = df.dropna()\n",
    "# the head of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(908, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(908, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "# df to values\n",
    "df = df.values\n",
    "Y = df[:,6]\n",
    "X = df[:,0:6]\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(X.shape)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.2, random_state=1) \n",
    "x_train_, x_test_, y_train_, y_test_ = x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "\n",
    "    mean = np.mean(train, axis=0)\n",
    "    std = np.std(train, axis=0)+0.000001\n",
    "\n",
    "    X_train = (train - mean) / std\n",
    "    X_test = (test - mean) /std\n",
    "    return X_train, X_test\n",
    "\n",
    "x_train, x_test = standardize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 6, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.any(np.isnan(X_test)))\n",
    "# print(np.any(np.isnan(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def CNN_net():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation=\"relu\", input_shape=(X.shape[1],1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Conv1D(128, 3, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.5))    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 726 samples, validate on 182 samples\n",
      "Epoch 1/250\n",
      "726/726 [==============================] - 1s 1ms/step - loss: 9.7370 - val_loss: 2.5732\n",
      "Epoch 2/250\n",
      "726/726 [==============================] - 0s 302us/step - loss: 2.7061 - val_loss: 1.7724\n",
      "Epoch 3/250\n",
      "726/726 [==============================] - 0s 295us/step - loss: 2.0098 - val_loss: 1.6076\n",
      "Epoch 4/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 1.8680 - val_loss: 1.4873\n",
      "Epoch 5/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.5384 - val_loss: 1.5065\n",
      "Epoch 6/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 1.5826 - val_loss: 1.3938\n",
      "Epoch 7/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.5555 - val_loss: 1.3431\n",
      "Epoch 8/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.5394 - val_loss: 1.2836\n",
      "Epoch 9/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.4233 - val_loss: 1.3225\n",
      "Epoch 10/250\n",
      "726/726 [==============================] - 0s 279us/step - loss: 1.4643 - val_loss: 1.2141\n",
      "Epoch 11/250\n",
      "726/726 [==============================] - 0s 275us/step - loss: 1.4548 - val_loss: 1.2925\n",
      "Epoch 12/250\n",
      "726/726 [==============================] - 0s 282us/step - loss: 1.3237 - val_loss: 1.2006\n",
      "Epoch 13/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.3652 - val_loss: 1.1950\n",
      "Epoch 14/250\n",
      "726/726 [==============================] - 0s 275us/step - loss: 1.4510 - val_loss: 1.2551\n",
      "Epoch 15/250\n",
      "726/726 [==============================] - 0s 286us/step - loss: 1.2182 - val_loss: 1.1850\n",
      "Epoch 16/250\n",
      "726/726 [==============================] - 0s 285us/step - loss: 1.3155 - val_loss: 1.1316\n",
      "Epoch 17/250\n",
      "726/726 [==============================] - 0s 290us/step - loss: 1.3397 - val_loss: 1.1761\n",
      "Epoch 18/250\n",
      "726/726 [==============================] - 0s 275us/step - loss: 1.2138 - val_loss: 1.1343\n",
      "Epoch 19/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.3004 - val_loss: 1.1509\n",
      "Epoch 20/250\n",
      "726/726 [==============================] - 0s 315us/step - loss: 1.1912 - val_loss: 1.1566\n",
      "Epoch 21/250\n",
      "726/726 [==============================] - 0s 302us/step - loss: 1.2993 - val_loss: 1.1657\n",
      "Epoch 22/250\n",
      "726/726 [==============================] - 0s 309us/step - loss: 1.2180 - val_loss: 1.1373\n",
      "Epoch 23/250\n",
      "726/726 [==============================] - 0s 290us/step - loss: 1.1954 - val_loss: 1.1494\n",
      "Epoch 24/250\n",
      "726/726 [==============================] - 0s 297us/step - loss: 1.2071 - val_loss: 1.1605\n",
      "Epoch 25/250\n",
      "726/726 [==============================] - 0s 313us/step - loss: 1.2652 - val_loss: 1.1397\n",
      "Epoch 26/250\n",
      "726/726 [==============================] - 0s 307us/step - loss: 1.1709 - val_loss: 1.1636\n",
      "Epoch 27/250\n",
      "726/726 [==============================] - 0s 317us/step - loss: 1.1664 - val_loss: 1.1313\n",
      "Epoch 28/250\n",
      "726/726 [==============================] - 0s 290us/step - loss: 1.2749 - val_loss: 1.0852\n",
      "Epoch 29/250\n",
      "726/726 [==============================] - 0s 299us/step - loss: 1.2066 - val_loss: 1.1470\n",
      "Epoch 30/250\n",
      "726/726 [==============================] - 0s 305us/step - loss: 1.1825 - val_loss: 1.1475\n",
      "Epoch 31/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.1373 - val_loss: 1.1725\n",
      "Epoch 32/250\n",
      "726/726 [==============================] - 0s 316us/step - loss: 1.1384 - val_loss: 1.1370\n",
      "Epoch 33/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 1.1759 - val_loss: 1.1771\n",
      "Epoch 34/250\n",
      "726/726 [==============================] - 0s 294us/step - loss: 1.2446 - val_loss: 1.1200\n",
      "Epoch 35/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 1.2048 - val_loss: 1.1128\n",
      "Epoch 36/250\n",
      "726/726 [==============================] - 0s 304us/step - loss: 1.1398 - val_loss: 1.1386\n",
      "Epoch 37/250\n",
      "726/726 [==============================] - 0s 297us/step - loss: 1.1207 - val_loss: 1.1300\n",
      "Epoch 38/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.2128 - val_loss: 1.1253\n",
      "Epoch 39/250\n",
      "726/726 [==============================] - 0s 292us/step - loss: 1.1147 - val_loss: 1.2219\n",
      "Epoch 40/250\n",
      "726/726 [==============================] - 0s 294us/step - loss: 1.1263 - val_loss: 1.2137\n",
      "Epoch 41/250\n",
      "726/726 [==============================] - 0s 305us/step - loss: 1.0810 - val_loss: 1.1946\n",
      "Epoch 42/250\n",
      "726/726 [==============================] - 0s 341us/step - loss: 1.0943 - val_loss: 1.2974\n",
      "Epoch 43/250\n",
      "726/726 [==============================] - 0s 317us/step - loss: 1.0711 - val_loss: 1.1685\n",
      "Epoch 44/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.1121 - val_loss: 1.1834\n",
      "Epoch 45/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 1.0456 - val_loss: 1.1765\n",
      "Epoch 46/250\n",
      "726/726 [==============================] - 0s 287us/step - loss: 1.0991 - val_loss: 1.1502\n",
      "Epoch 47/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.1436 - val_loss: 1.1683\n",
      "Epoch 48/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.1107 - val_loss: 1.1657\n",
      "Epoch 49/250\n",
      "726/726 [==============================] - 0s 283us/step - loss: 1.1632 - val_loss: 1.1493\n",
      "Epoch 50/250\n",
      "726/726 [==============================] - 0s 273us/step - loss: 1.0679 - val_loss: 1.2054\n",
      "Epoch 51/250\n",
      "726/726 [==============================] - 0s 272us/step - loss: 1.1387 - val_loss: 1.1574\n",
      "Epoch 52/250\n",
      "726/726 [==============================] - 0s 271us/step - loss: 1.0803 - val_loss: 1.1798\n",
      "Epoch 53/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.1516 - val_loss: 1.1932\n",
      "Epoch 54/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.1197 - val_loss: 1.1409\n",
      "Epoch 55/250\n",
      "726/726 [==============================] - 0s 353us/step - loss: 1.0954 - val_loss: 1.1968\n",
      "Epoch 56/250\n",
      "726/726 [==============================] - 0s 372us/step - loss: 1.1207 - val_loss: 1.1629\n",
      "Epoch 57/250\n",
      "726/726 [==============================] - 0s 348us/step - loss: 1.0255 - val_loss: 1.1373\n",
      "Epoch 58/250\n",
      "726/726 [==============================] - 0s 356us/step - loss: 1.0580 - val_loss: 1.2219\n",
      "Epoch 59/250\n",
      "726/726 [==============================] - 0s 350us/step - loss: 1.1174 - val_loss: 1.1675\n",
      "Epoch 60/250\n",
      "726/726 [==============================] - 0s 349us/step - loss: 1.1099 - val_loss: 1.1855\n",
      "Epoch 61/250\n",
      "726/726 [==============================] - 0s 372us/step - loss: 1.0833 - val_loss: 1.1336\n",
      "Epoch 62/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 1.1723 - val_loss: 1.1592\n",
      "Epoch 63/250\n",
      "726/726 [==============================] - 0s 275us/step - loss: 1.1160 - val_loss: 1.1684\n",
      "Epoch 64/250\n",
      "726/726 [==============================] - 0s 319us/step - loss: 1.0852 - val_loss: 1.1757\n",
      "Epoch 65/250\n",
      "726/726 [==============================] - 0s 283us/step - loss: 1.0474 - val_loss: 1.2206\n",
      "Epoch 66/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.0625 - val_loss: 1.1706\n",
      "Epoch 67/250\n",
      "726/726 [==============================] - 0s 286us/step - loss: 1.0779 - val_loss: 1.1995\n",
      "Epoch 68/250\n",
      "726/726 [==============================] - 0s 282us/step - loss: 1.1090 - val_loss: 1.1407\n",
      "Epoch 69/250\n",
      "726/726 [==============================] - 0s 279us/step - loss: 1.0763 - val_loss: 1.1387\n",
      "Epoch 70/250\n",
      "726/726 [==============================] - 0s 308us/step - loss: 1.0257 - val_loss: 1.3041\n",
      "Epoch 71/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/726 [==============================] - 0s 291us/step - loss: 1.1151 - val_loss: 1.1198\n",
      "Epoch 72/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.0332 - val_loss: 1.1392\n",
      "Epoch 73/250\n",
      "726/726 [==============================] - 0s 277us/step - loss: 1.0590 - val_loss: 1.1198\n",
      "Epoch 74/250\n",
      "726/726 [==============================] - 0s 284us/step - loss: 1.0445 - val_loss: 1.2350\n",
      "Epoch 75/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.1660 - val_loss: 1.0935\n",
      "Epoch 76/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 1.0790 - val_loss: 1.1422\n",
      "Epoch 77/250\n",
      "726/726 [==============================] - 0s 302us/step - loss: 1.0460 - val_loss: 1.2245\n",
      "Epoch 78/250\n",
      "726/726 [==============================] - 0s 284us/step - loss: 1.0186 - val_loss: 1.1169\n",
      "Epoch 79/250\n",
      "726/726 [==============================] - 0s 304us/step - loss: 1.0464 - val_loss: 1.1401\n",
      "Epoch 80/250\n",
      "726/726 [==============================] - 0s 305us/step - loss: 1.0759 - val_loss: 1.1389\n",
      "Epoch 81/250\n",
      "726/726 [==============================] - 0s 252us/step - loss: 1.0539 - val_loss: 1.1747\n",
      "Epoch 82/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.0192 - val_loss: 1.1867\n",
      "Epoch 83/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0150 - val_loss: 1.1294\n",
      "Epoch 84/250\n",
      "726/726 [==============================] - 0s 278us/step - loss: 1.0300 - val_loss: 1.1405\n",
      "Epoch 85/250\n",
      "726/726 [==============================] - 0s 325us/step - loss: 1.0576 - val_loss: 1.1254\n",
      "Epoch 86/250\n",
      "726/726 [==============================] - 0s 272us/step - loss: 1.0852 - val_loss: 1.1795\n",
      "Epoch 87/250\n",
      "726/726 [==============================] - 0s 252us/step - loss: 1.0319 - val_loss: 1.2331\n",
      "Epoch 88/250\n",
      "726/726 [==============================] - 0s 301us/step - loss: 1.0189 - val_loss: 1.1170\n",
      "Epoch 89/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0268 - val_loss: 1.1686\n",
      "Epoch 90/250\n",
      "726/726 [==============================] - 0s 308us/step - loss: 1.0319 - val_loss: 1.1501\n",
      "Epoch 91/250\n",
      "726/726 [==============================] - 0s 275us/step - loss: 1.1233 - val_loss: 1.2226\n",
      "Epoch 92/250\n",
      "726/726 [==============================] - 0s 272us/step - loss: 1.0202 - val_loss: 1.1812\n",
      "Epoch 93/250\n",
      "726/726 [==============================] - 0s 287us/step - loss: 1.0298 - val_loss: 1.1083\n",
      "Epoch 94/250\n",
      "726/726 [==============================] - 0s 276us/step - loss: 0.9805 - val_loss: 1.2294\n",
      "Epoch 95/250\n",
      "726/726 [==============================] - 0s 284us/step - loss: 1.0417 - val_loss: 1.1376\n",
      "Epoch 96/250\n",
      "726/726 [==============================] - 0s 255us/step - loss: 1.0353 - val_loss: 1.1480\n",
      "Epoch 97/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0386 - val_loss: 1.1272\n",
      "Epoch 98/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0982 - val_loss: 1.1359\n",
      "Epoch 99/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0431 - val_loss: 1.1034\n",
      "Epoch 100/250\n",
      "726/726 [==============================] - 0s 302us/step - loss: 1.0576 - val_loss: 1.1238\n",
      "Epoch 101/250\n",
      "726/726 [==============================] - 0s 305us/step - loss: 0.9813 - val_loss: 1.1752\n",
      "Epoch 102/250\n",
      "726/726 [==============================] - 0s 279us/step - loss: 1.0283 - val_loss: 1.1454\n",
      "Epoch 103/250\n",
      "726/726 [==============================] - 0s 272us/step - loss: 1.0727 - val_loss: 1.1711\n",
      "Epoch 104/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.1191 - val_loss: 1.1465\n",
      "Epoch 105/250\n",
      "726/726 [==============================] - 0s 282us/step - loss: 1.0785 - val_loss: 1.2261\n",
      "Epoch 106/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.0880 - val_loss: 1.1448\n",
      "Epoch 107/250\n",
      "726/726 [==============================] - 0s 344us/step - loss: 1.1111 - val_loss: 1.1678\n",
      "Epoch 108/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.0421 - val_loss: 1.1664\n",
      "Epoch 109/250\n",
      "726/726 [==============================] - 0s 349us/step - loss: 1.0037 - val_loss: 1.2478\n",
      "Epoch 110/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.0688 - val_loss: 1.1566\n",
      "Epoch 111/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 0.9996 - val_loss: 1.1977\n",
      "Epoch 112/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.0087 - val_loss: 1.1825\n",
      "Epoch 113/250\n",
      "726/726 [==============================] - 0s 351us/step - loss: 0.9787 - val_loss: 1.2214\n",
      "Epoch 114/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 1.0122 - val_loss: 1.1443\n",
      "Epoch 115/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.0400 - val_loss: 1.1528\n",
      "Epoch 116/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9920 - val_loss: 1.1353\n",
      "Epoch 117/250\n",
      "726/726 [==============================] - 0s 313us/step - loss: 1.0411 - val_loss: 1.1445\n",
      "Epoch 118/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.1050 - val_loss: 1.1389\n",
      "Epoch 119/250\n",
      "726/726 [==============================] - 0s 237us/step - loss: 1.0230 - val_loss: 1.1703\n",
      "Epoch 120/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0112 - val_loss: 1.1442\n",
      "Epoch 121/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.0321 - val_loss: 1.2373\n",
      "Epoch 122/250\n",
      "726/726 [==============================] - 0s 262us/step - loss: 1.0328 - val_loss: 1.1774\n",
      "Epoch 123/250\n",
      "726/726 [==============================] - 0s 275us/step - loss: 1.0564 - val_loss: 1.1589\n",
      "Epoch 124/250\n",
      "726/726 [==============================] - 0s 271us/step - loss: 1.0010 - val_loss: 1.2478\n",
      "Epoch 125/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9654 - val_loss: 1.1567\n",
      "Epoch 126/250\n",
      "726/726 [==============================] - 0s 301us/step - loss: 0.9793 - val_loss: 1.2130\n",
      "Epoch 127/250\n",
      "726/726 [==============================] - 0s 272us/step - loss: 1.0005 - val_loss: 1.1664\n",
      "Epoch 128/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.0636 - val_loss: 1.1993\n",
      "Epoch 129/250\n",
      "726/726 [==============================] - 0s 342us/step - loss: 1.0386 - val_loss: 1.1793\n",
      "Epoch 130/250\n",
      "726/726 [==============================] - 0s 344us/step - loss: 0.9786 - val_loss: 1.1654\n",
      "Epoch 131/250\n",
      "726/726 [==============================] - 0s 344us/step - loss: 1.0075 - val_loss: 1.1411\n",
      "Epoch 132/250\n",
      "726/726 [==============================] - 0s 361us/step - loss: 1.0195 - val_loss: 1.1566\n",
      "Epoch 133/250\n",
      "726/726 [==============================] - 0s 366us/step - loss: 1.0500 - val_loss: 1.1442\n",
      "Epoch 134/250\n",
      "726/726 [==============================] - 0s 344us/step - loss: 1.0093 - val_loss: 1.1640\n",
      "Epoch 135/250\n",
      "726/726 [==============================] - 0s 377us/step - loss: 1.0403 - val_loss: 1.1534\n",
      "Epoch 136/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 0.9860 - val_loss: 1.1679\n",
      "Epoch 137/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9603 - val_loss: 1.1203\n",
      "Epoch 138/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0772 - val_loss: 1.1106\n",
      "Epoch 139/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0399 - val_loss: 1.1683\n",
      "Epoch 140/250\n",
      "726/726 [==============================] - 0s 305us/step - loss: 1.0699 - val_loss: 1.1531\n",
      "Epoch 141/250\n",
      "726/726 [==============================] - 0s 257us/step - loss: 1.0106 - val_loss: 1.1470\n",
      "Epoch 142/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0136 - val_loss: 1.1694\n",
      "Epoch 143/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0594 - val_loss: 1.1103\n",
      "Epoch 144/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0576 - val_loss: 1.1551\n",
      "Epoch 145/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9726 - val_loss: 1.1655\n",
      "Epoch 146/250\n",
      "726/726 [==============================] - 0s 283us/step - loss: 0.9893 - val_loss: 1.2125\n",
      "Epoch 147/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9701 - val_loss: 1.1770\n",
      "Epoch 148/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9812 - val_loss: 1.1559\n",
      "Epoch 149/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/726 [==============================] - 0s 258us/step - loss: 1.0183 - val_loss: 1.1300\n",
      "Epoch 150/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9976 - val_loss: 1.1749\n",
      "Epoch 151/250\n",
      "726/726 [==============================] - 0s 284us/step - loss: 0.9928 - val_loss: 1.1428\n",
      "Epoch 152/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0302 - val_loss: 1.1607\n",
      "Epoch 153/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0033 - val_loss: 1.2118\n",
      "Epoch 154/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0756 - val_loss: 1.2304\n",
      "Epoch 155/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0252 - val_loss: 1.1598\n",
      "Epoch 156/250\n",
      "726/726 [==============================] - 0s 268us/step - loss: 0.9985 - val_loss: 1.1199\n",
      "Epoch 157/250\n",
      "726/726 [==============================] - 0s 273us/step - loss: 1.0142 - val_loss: 1.1494\n",
      "Epoch 158/250\n",
      "726/726 [==============================] - 0s 237us/step - loss: 0.9883 - val_loss: 1.2268\n",
      "Epoch 159/250\n",
      "726/726 [==============================] - 0s 237us/step - loss: 0.9830 - val_loss: 1.1684\n",
      "Epoch 160/250\n",
      "726/726 [==============================] - 0s 237us/step - loss: 0.9903 - val_loss: 1.1548\n",
      "Epoch 161/250\n",
      "726/726 [==============================] - 0s 277us/step - loss: 1.0160 - val_loss: 1.1419\n",
      "Epoch 162/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 0.9865 - val_loss: 1.1529\n",
      "Epoch 163/250\n",
      "726/726 [==============================] - 0s 268us/step - loss: 1.0225 - val_loss: 1.1548\n",
      "Epoch 164/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9984 - val_loss: 1.1523\n",
      "Epoch 165/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9880 - val_loss: 1.1583\n",
      "Epoch 166/250\n",
      "726/726 [==============================] - 0s 279us/step - loss: 1.0017 - val_loss: 1.1446\n",
      "Epoch 167/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9670 - val_loss: 1.1443\n",
      "Epoch 168/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9753 - val_loss: 1.2004\n",
      "Epoch 169/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.0451 - val_loss: 1.1347\n",
      "Epoch 170/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9812 - val_loss: 1.1039\n",
      "Epoch 171/250\n",
      "726/726 [==============================] - 0s 260us/step - loss: 1.0290 - val_loss: 1.1221\n",
      "Epoch 172/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9944 - val_loss: 1.1666\n",
      "Epoch 173/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9666 - val_loss: 1.1294\n",
      "Epoch 174/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9734 - val_loss: 1.1761\n",
      "Epoch 175/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9547 - val_loss: 1.1488\n",
      "Epoch 176/250\n",
      "726/726 [==============================] - 0s 266us/step - loss: 0.9913 - val_loss: 1.2156\n",
      "Epoch 177/250\n",
      "726/726 [==============================] - 0s 252us/step - loss: 0.9156 - val_loss: 1.1923\n",
      "Epoch 178/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9987 - val_loss: 1.1200\n",
      "Epoch 179/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9641 - val_loss: 1.1221\n",
      "Epoch 180/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9533 - val_loss: 1.2066\n",
      "Epoch 181/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0487 - val_loss: 1.1527\n",
      "Epoch 182/250\n",
      "726/726 [==============================] - 0s 300us/step - loss: 1.0188 - val_loss: 1.1139\n",
      "Epoch 183/250\n",
      "726/726 [==============================] - 0s 262us/step - loss: 1.0145 - val_loss: 1.1388\n",
      "Epoch 184/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9173 - val_loss: 1.2120\n",
      "Epoch 185/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9513 - val_loss: 1.1840\n",
      "Epoch 186/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9651 - val_loss: 1.1651\n",
      "Epoch 187/250\n",
      "726/726 [==============================] - 0s 291us/step - loss: 1.0033 - val_loss: 1.1455\n",
      "Epoch 188/250\n",
      "726/726 [==============================] - 0s 273us/step - loss: 0.9481 - val_loss: 1.1725\n",
      "Epoch 189/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9901 - val_loss: 1.1762\n",
      "Epoch 190/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0213 - val_loss: 1.1776\n",
      "Epoch 191/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.0258 - val_loss: 1.1299\n",
      "Epoch 192/250\n",
      "726/726 [==============================] - 0s 301us/step - loss: 1.0019 - val_loss: 1.1473\n",
      "Epoch 193/250\n",
      "726/726 [==============================] - 0s 281us/step - loss: 0.9781 - val_loss: 1.1141\n",
      "Epoch 194/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9383 - val_loss: 1.1246\n",
      "Epoch 195/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9616 - val_loss: 1.2624\n",
      "Epoch 196/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 1.0079 - val_loss: 1.1680\n",
      "Epoch 197/250\n",
      "726/726 [==============================] - 0s 297us/step - loss: 1.0488 - val_loss: 1.1836\n",
      "Epoch 198/250\n",
      "726/726 [==============================] - 0s 266us/step - loss: 0.9817 - val_loss: 1.1916\n",
      "Epoch 199/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9871 - val_loss: 1.1576\n",
      "Epoch 200/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9451 - val_loss: 1.1393\n",
      "Epoch 201/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9524 - val_loss: 1.1379\n",
      "Epoch 202/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 0.9854 - val_loss: 1.2073\n",
      "Epoch 203/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9095 - val_loss: 1.2275\n",
      "Epoch 204/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9665 - val_loss: 1.2010\n",
      "Epoch 205/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9693 - val_loss: 1.1469\n",
      "Epoch 206/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9716 - val_loss: 1.1801\n",
      "Epoch 207/250\n",
      "726/726 [==============================] - 0s 297us/step - loss: 0.9731 - val_loss: 1.1713\n",
      "Epoch 208/250\n",
      "726/726 [==============================] - 0s 366us/step - loss: 0.9604 - val_loss: 1.1681\n",
      "Epoch 209/250\n",
      "726/726 [==============================] - 0s 366us/step - loss: 0.9373 - val_loss: 1.1650\n",
      "Epoch 210/250\n",
      "726/726 [==============================] - 0s 344us/step - loss: 0.9695 - val_loss: 1.1991\n",
      "Epoch 211/250\n",
      "726/726 [==============================] - 0s 364us/step - loss: 0.9246 - val_loss: 1.2168\n",
      "Epoch 212/250\n",
      "726/726 [==============================] - 0s 366us/step - loss: 0.9509 - val_loss: 1.1604\n",
      "Epoch 213/250\n",
      "726/726 [==============================] - 0s 344us/step - loss: 0.9974 - val_loss: 1.1672\n",
      "Epoch 214/250\n",
      "726/726 [==============================] - 0s 323us/step - loss: 0.9673 - val_loss: 1.1746\n",
      "Epoch 215/250\n",
      "726/726 [==============================] - 0s 282us/step - loss: 0.9601 - val_loss: 1.1555\n",
      "Epoch 216/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9441 - val_loss: 1.1771\n",
      "Epoch 217/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9956 - val_loss: 1.1693\n",
      "Epoch 218/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9672 - val_loss: 1.2308\n",
      "Epoch 219/250\n",
      "726/726 [==============================] - 0s 289us/step - loss: 1.0050 - val_loss: 1.1642\n",
      "Epoch 220/250\n",
      "726/726 [==============================] - 0s 284us/step - loss: 0.9998 - val_loss: 1.1424\n",
      "Epoch 221/250\n",
      "726/726 [==============================] - 0s 245us/step - loss: 0.9733 - val_loss: 1.1758\n",
      "Epoch 222/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9924 - val_loss: 1.1921\n",
      "Epoch 223/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0093 - val_loss: 1.1351\n",
      "Epoch 224/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9578 - val_loss: 1.1607\n",
      "Epoch 225/250\n",
      "726/726 [==============================] - 0s 293us/step - loss: 0.9682 - val_loss: 1.1503\n",
      "Epoch 226/250\n",
      "726/726 [==============================] - 0s 248us/step - loss: 0.9714 - val_loss: 1.1380\n",
      "Epoch 227/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/726 [==============================] - 0s 258us/step - loss: 0.9305 - val_loss: 1.1398\n",
      "Epoch 228/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9833 - val_loss: 1.1336\n",
      "Epoch 229/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9995 - val_loss: 1.1684\n",
      "Epoch 230/250\n",
      "726/726 [==============================] - 0s 284us/step - loss: 0.9598 - val_loss: 1.1664\n",
      "Epoch 231/250\n",
      "726/726 [==============================] - 0s 277us/step - loss: 0.9546 - val_loss: 1.1508\n",
      "Epoch 232/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9769 - val_loss: 1.1356\n",
      "Epoch 233/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9436 - val_loss: 1.1715\n",
      "Epoch 234/250\n",
      "726/726 [==============================] - 0s 301us/step - loss: 1.0079 - val_loss: 1.1727\n",
      "Epoch 235/250\n",
      "726/726 [==============================] - 0s 254us/step - loss: 0.9391 - val_loss: 1.1399\n",
      "Epoch 236/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9386 - val_loss: 1.1656\n",
      "Epoch 237/250\n",
      "726/726 [==============================] - 0s 301us/step - loss: 0.9763 - val_loss: 1.1458\n",
      "Epoch 238/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0025 - val_loss: 1.1813\n",
      "Epoch 239/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0254 - val_loss: 1.1523\n",
      "Epoch 240/250\n",
      "726/726 [==============================] - 0s 254us/step - loss: 0.9790 - val_loss: 1.2063\n",
      "Epoch 241/250\n",
      "726/726 [==============================] - 0s 280us/step - loss: 0.9412 - val_loss: 1.1461\n",
      "Epoch 242/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 1.0163 - val_loss: 1.1778\n",
      "Epoch 243/250\n",
      "726/726 [==============================] - 0s 306us/step - loss: 0.9686 - val_loss: 1.1269\n",
      "Epoch 244/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9662 - val_loss: 1.1451\n",
      "Epoch 245/250\n",
      "726/726 [==============================] - 0s 268us/step - loss: 0.9925 - val_loss: 1.1274\n",
      "Epoch 246/250\n",
      "726/726 [==============================] - 0s 252us/step - loss: 0.9698 - val_loss: 1.1610\n",
      "Epoch 247/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9642 - val_loss: 1.2043\n",
      "Epoch 248/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9557 - val_loss: 1.1929\n",
      "Epoch 249/250\n",
      "726/726 [==============================] - 0s 258us/step - loss: 0.9480 - val_loss: 1.1563\n",
      "Epoch 250/250\n",
      "726/726 [==============================] - 0s 282us/step - loss: 0.9885 - val_loss: 1.1767\n"
     ]
    }
   ],
   "source": [
    "# Parametres\n",
    "verbose, epochs, batch_size = 1, 250, 5\n",
    "# initialize the model object\n",
    "clf_cnn = CNN_net()\n",
    "# fit network #Train the model using tensorboard instance in the callbacks\n",
    "history = clf_cnn.fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=verbose, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/726 [==============================] - 0s 59us/step\n",
      " Model.evaluate :  0.8235887932711725 \n",
      "\n",
      "Mean Squa Error : 1.1766504147325032\n",
      "Mean Abso Error : 0.7574529346476545\n",
      "Expl. Variance  : 0.38696777154459683\n"
     ]
    }
   ],
   "source": [
    "ypred = clf_cnn.predict(x_test)\n",
    "\n",
    "print(\" Model.evaluate : \",clf_cnn.evaluate(x_train, y_train),'\\n')\n",
    "\n",
    "#evaluate results\n",
    "mse=sklearn.metrics.mean_squared_error(y_test,ypred)\n",
    "mabs=sklearn.metrics.mean_absolute_error(y_test,ypred)\n",
    "exvar=sklearn.metrics.explained_variance_score(y_test,ypred)   \n",
    "print('Mean Squa Error :',mse)\n",
    "print('Mean Abso Error :',mabs)\n",
    "print('Expl. Variance  :',exvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 4, 64)             256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.datatechnotes.com/2019/12/how-to-fit-regression-data-with-cnn.html\n",
    "x_ax = range(len(ypred))\n",
    "plt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss','Validation loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# from the first Fully-Connected layer \n",
    "layer_name = 'dense_1'\n",
    "intermediate_layer_model = Model(inputs=clf_cnn.input,\n",
    "                                 outputs=clf_cnn.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features of the train dataset to use it in future.\n",
    "out_cnn_train = intermediate_layer_model.predict(x_train)\n",
    "# Save the features of the test dataset to use it in future.\n",
    "out_cnn_test = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (from CNN) Shape: (726, 1)\n",
      "Training Labels (from CNN) Shape: (726,) \n",
      "\n",
      "Test Features (from CNN) Shape: (182, 1)\n",
      "Test Labels (from CNN) Shape: (182,) \n",
      "\n",
      "Test Features original Shape: (726, 6, 1)\n",
      "Test Features original Shape: (182, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features (from CNN) Shape:', out_cnn_train.shape)\n",
    "print('Training Labels (from CNN) Shape:', y_train.shape,'\\n')\n",
    "\n",
    "print('Test Features (from CNN) Shape:', out_cnn_test.shape)\n",
    "print('Test Labels (from CNN) Shape:', y_test.shape,'\\n')\n",
    "\n",
    "print('Test Features original Shape:', x_train_.shape)\n",
    "print('Test Features original Shape:', x_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + Random Forest + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djinn example\n",
      "Finding optimal hyper-parameters...\n",
      "Determining learning rate...\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:444: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:477: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:461: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Determining number of epochs needed...\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:528: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:531: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:532: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Optimal learning rate:  0.0027941586783753235\n",
      "Optimal # epochs:  220\n",
      "Optimal batch size:  37\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:262: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:263: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:334: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Epoch: 0001 cost= 0.019487785\n",
      "Epoch: 0002 cost= 0.015071467\n",
      "Epoch: 0003 cost= 0.012858395\n",
      "Epoch: 0004 cost= 0.010692656\n",
      "Epoch: 0005 cost= 0.008691491\n",
      "Epoch: 0006 cost= 0.010615445\n",
      "Epoch: 0007 cost= 0.008955885\n",
      "Epoch: 0008 cost= 0.009618823\n",
      "Epoch: 0009 cost= 0.008448854\n",
      "Epoch: 0010 cost= 0.009579559\n",
      "Epoch: 0011 cost= 0.009561431\n",
      "Epoch: 0012 cost= 0.008086651\n",
      "Epoch: 0013 cost= 0.007969847\n",
      "Epoch: 0014 cost= 0.008943217\n",
      "Epoch: 0015 cost= 0.009315248\n",
      "Epoch: 0016 cost= 0.010126640\n",
      "Epoch: 0017 cost= 0.008739829\n",
      "Epoch: 0018 cost= 0.007917423\n",
      "Epoch: 0019 cost= 0.008637317\n",
      "Epoch: 0020 cost= 0.008877168\n",
      "Epoch: 0021 cost= 0.009357317\n",
      "Epoch: 0022 cost= 0.007781987\n",
      "Epoch: 0023 cost= 0.009265130\n",
      "Epoch: 0024 cost= 0.008886622\n",
      "Epoch: 0025 cost= 0.008751533\n",
      "Epoch: 0026 cost= 0.009221171\n",
      "Epoch: 0027 cost= 0.009445371\n",
      "Epoch: 0028 cost= 0.008479096\n",
      "Epoch: 0029 cost= 0.008396198\n",
      "Epoch: 0030 cost= 0.009112979\n",
      "Epoch: 0031 cost= 0.007942664\n",
      "Epoch: 0032 cost= 0.009141300\n",
      "Epoch: 0033 cost= 0.008981788\n",
      "Epoch: 0034 cost= 0.010620090\n",
      "Epoch: 0035 cost= 0.008347742\n",
      "Epoch: 0036 cost= 0.009556019\n",
      "Epoch: 0037 cost= 0.010318413\n",
      "Epoch: 0038 cost= 0.009464107\n",
      "Epoch: 0039 cost= 0.009641255\n",
      "Epoch: 0040 cost= 0.009577838\n",
      "Epoch: 0041 cost= 0.010285542\n",
      "Epoch: 0042 cost= 0.008265393\n",
      "Epoch: 0043 cost= 0.008900702\n",
      "Epoch: 0044 cost= 0.008041892\n",
      "Epoch: 0045 cost= 0.008469224\n",
      "Epoch: 0046 cost= 0.009836843\n",
      "Epoch: 0047 cost= 0.009307165\n",
      "Epoch: 0048 cost= 0.007441996\n",
      "Epoch: 0049 cost= 0.008268715\n",
      "Epoch: 0050 cost= 0.009041672\n",
      "Epoch: 0051 cost= 0.008215732\n",
      "Epoch: 0052 cost= 0.009869422\n",
      "Epoch: 0053 cost= 0.010112000\n",
      "Epoch: 0054 cost= 0.009947038\n",
      "Epoch: 0055 cost= 0.009767224\n",
      "Epoch: 0056 cost= 0.010155427\n",
      "Epoch: 0057 cost= 0.008288706\n",
      "Epoch: 0058 cost= 0.008149112\n",
      "Epoch: 0059 cost= 0.009937032\n",
      "Epoch: 0060 cost= 0.010048875\n",
      "Epoch: 0061 cost= 0.008756014\n",
      "Epoch: 0062 cost= 0.008505216\n",
      "Epoch: 0063 cost= 0.008091144\n",
      "Epoch: 0064 cost= 0.008801581\n",
      "Epoch: 0065 cost= 0.009447202\n",
      "Epoch: 0066 cost= 0.009427303\n",
      "Epoch: 0067 cost= 0.009320234\n",
      "Epoch: 0068 cost= 0.008660563\n",
      "Epoch: 0069 cost= 0.008987411\n",
      "Epoch: 0070 cost= 0.009092258\n",
      "Epoch: 0071 cost= 0.009020419\n",
      "Epoch: 0072 cost= 0.009112726\n",
      "Epoch: 0073 cost= 0.009205966\n",
      "Epoch: 0074 cost= 0.009856288\n",
      "Epoch: 0075 cost= 0.009113764\n",
      "Epoch: 0076 cost= 0.008748186\n",
      "Epoch: 0077 cost= 0.009503324\n",
      "Epoch: 0078 cost= 0.009798344\n",
      "Epoch: 0079 cost= 0.008959277\n",
      "Epoch: 0080 cost= 0.009162998\n",
      "Epoch: 0081 cost= 0.010225681\n",
      "Epoch: 0082 cost= 0.010196526\n",
      "Epoch: 0083 cost= 0.008574184\n",
      "Epoch: 0084 cost= 0.009076688\n",
      "Epoch: 0085 cost= 0.009104869\n",
      "Epoch: 0086 cost= 0.009422722\n",
      "Epoch: 0087 cost= 0.009436800\n",
      "Epoch: 0088 cost= 0.009444708\n",
      "Epoch: 0089 cost= 0.009408393\n",
      "Epoch: 0090 cost= 0.009395155\n",
      "Epoch: 0091 cost= 0.008310498\n",
      "Epoch: 0092 cost= 0.009355530\n",
      "Epoch: 0093 cost= 0.008485513\n",
      "Epoch: 0094 cost= 0.008966223\n",
      "Epoch: 0095 cost= 0.008344370\n",
      "Epoch: 0096 cost= 0.008792701\n",
      "Epoch: 0097 cost= 0.008942284\n",
      "Epoch: 0098 cost= 0.009775796\n",
      "Epoch: 0099 cost= 0.009335246\n",
      "Epoch: 0100 cost= 0.008978549\n",
      "Epoch: 0101 cost= 0.008811163\n",
      "Epoch: 0102 cost= 0.010331322\n",
      "Epoch: 0103 cost= 0.010253844\n",
      "Epoch: 0104 cost= 0.008714905\n",
      "Epoch: 0105 cost= 0.008999671\n",
      "Epoch: 0106 cost= 0.009183340\n",
      "Epoch: 0107 cost= 0.009076714\n",
      "Epoch: 0108 cost= 0.009816181\n",
      "Epoch: 0109 cost= 0.008964687\n",
      "Epoch: 0110 cost= 0.009997833\n",
      "Epoch: 0111 cost= 0.007729250\n",
      "Epoch: 0112 cost= 0.008219826\n",
      "Epoch: 0113 cost= 0.008165724\n",
      "Epoch: 0114 cost= 0.008287374\n",
      "Epoch: 0115 cost= 0.008682495\n",
      "Epoch: 0116 cost= 0.009980509\n",
      "Epoch: 0117 cost= 0.009408962\n",
      "Epoch: 0118 cost= 0.009995759\n",
      "Epoch: 0119 cost= 0.009812514\n",
      "Epoch: 0120 cost= 0.007887056\n",
      "Epoch: 0121 cost= 0.008298370\n",
      "Epoch: 0122 cost= 0.008710546\n",
      "Epoch: 0123 cost= 0.009403415\n",
      "Epoch: 0124 cost= 0.008225924\n",
      "Epoch: 0125 cost= 0.009844516\n",
      "Epoch: 0126 cost= 0.009052242\n",
      "Epoch: 0127 cost= 0.009074857\n",
      "Epoch: 0128 cost= 0.009846023\n",
      "Epoch: 0129 cost= 0.008824307\n",
      "Epoch: 0130 cost= 0.008631909\n",
      "Epoch: 0131 cost= 0.009713501\n",
      "Epoch: 0132 cost= 0.009091973\n",
      "Epoch: 0133 cost= 0.008461088\n",
      "Epoch: 0134 cost= 0.008862995\n",
      "Epoch: 0135 cost= 0.008152280\n",
      "Epoch: 0136 cost= 0.008730676\n",
      "Epoch: 0137 cost= 0.009081579\n",
      "Epoch: 0138 cost= 0.008199044\n",
      "Epoch: 0139 cost= 0.009073741\n",
      "Epoch: 0140 cost= 0.008944025\n",
      "Epoch: 0141 cost= 0.009533876\n",
      "Epoch: 0142 cost= 0.007881417\n",
      "Epoch: 0143 cost= 0.008899093\n",
      "Epoch: 0144 cost= 0.008374035\n",
      "Epoch: 0145 cost= 0.008912724\n",
      "Epoch: 0146 cost= 0.008684117\n",
      "Epoch: 0147 cost= 0.009323321\n",
      "Epoch: 0148 cost= 0.009356499\n",
      "Epoch: 0149 cost= 0.009514712\n",
      "Epoch: 0150 cost= 0.009220024\n",
      "Epoch: 0151 cost= 0.008995359\n",
      "Epoch: 0152 cost= 0.009682337\n",
      "Epoch: 0153 cost= 0.007888973\n",
      "Epoch: 0154 cost= 0.007812564\n",
      "Epoch: 0155 cost= 0.008200387\n",
      "Epoch: 0156 cost= 0.008785965\n",
      "Epoch: 0157 cost= 0.009961251\n",
      "Epoch: 0158 cost= 0.009002723\n",
      "Epoch: 0159 cost= 0.008987286\n",
      "Epoch: 0160 cost= 0.008412734\n",
      "Epoch: 0161 cost= 0.008868655\n",
      "Epoch: 0162 cost= 0.009102317\n",
      "Epoch: 0163 cost= 0.009232206\n",
      "Epoch: 0164 cost= 0.010395094\n",
      "Epoch: 0165 cost= 0.007754911\n",
      "Epoch: 0166 cost= 0.008872555\n",
      "Epoch: 0167 cost= 0.008362710\n",
      "Epoch: 0168 cost= 0.008485152\n",
      "Epoch: 0169 cost= 0.009845114\n",
      "Epoch: 0170 cost= 0.008316780\n",
      "Epoch: 0171 cost= 0.009891129\n",
      "Epoch: 0172 cost= 0.012057677\n",
      "Epoch: 0173 cost= 0.008474479\n",
      "Epoch: 0174 cost= 0.010615179\n",
      "Epoch: 0175 cost= 0.009430321\n",
      "Epoch: 0176 cost= 0.009811309\n",
      "Epoch: 0177 cost= 0.009370604\n",
      "Epoch: 0178 cost= 0.010405728\n",
      "Epoch: 0179 cost= 0.008938627\n",
      "Epoch: 0180 cost= 0.008994478\n",
      "Epoch: 0181 cost= 0.008055537\n",
      "Epoch: 0182 cost= 0.008881601\n",
      "Epoch: 0183 cost= 0.010165983\n",
      "Epoch: 0184 cost= 0.009365939\n",
      "Epoch: 0185 cost= 0.009374938\n",
      "Epoch: 0186 cost= 0.008908425\n",
      "Epoch: 0187 cost= 0.009207434\n",
      "Epoch: 0188 cost= 0.009168742\n",
      "Epoch: 0189 cost= 0.007786240\n",
      "Epoch: 0190 cost= 0.009835709\n",
      "Epoch: 0191 cost= 0.009641161\n",
      "Epoch: 0192 cost= 0.009316310\n",
      "Epoch: 0193 cost= 0.008191755\n",
      "Epoch: 0194 cost= 0.008227645\n",
      "Epoch: 0195 cost= 0.009033488\n",
      "Epoch: 0196 cost= 0.009527472\n",
      "Epoch: 0197 cost= 0.009092270\n",
      "Epoch: 0198 cost= 0.008858527\n",
      "Epoch: 0199 cost= 0.008989421\n",
      "Epoch: 0200 cost= 0.010930347\n",
      "Epoch: 0201 cost= 0.008923431\n",
      "Epoch: 0202 cost= 0.009301664\n",
      "Epoch: 0203 cost= 0.008716658\n",
      "Epoch: 0204 cost= 0.009029729\n",
      "Epoch: 0205 cost= 0.008887857\n",
      "Epoch: 0206 cost= 0.009370451\n",
      "Epoch: 0207 cost= 0.010180312\n",
      "Epoch: 0208 cost= 0.008650479\n",
      "Epoch: 0209 cost= 0.008753462\n",
      "Epoch: 0210 cost= 0.007791172\n",
      "Epoch: 0211 cost= 0.009968859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0212 cost= 0.009156884\n",
      "Epoch: 0213 cost= 0.008481788\n",
      "Epoch: 0214 cost= 0.008412007\n",
      "Epoch: 0215 cost= 0.008445273\n",
      "Epoch: 0216 cost= 0.007921851\n",
      "Epoch: 0217 cost= 0.009185321\n",
      "Epoch: 0218 cost= 0.010076925\n",
      "Epoch: 0219 cost= 0.009267260\n",
      "Epoch: 0220 cost= 0.008651471\n",
      "Optimization Finished!\n",
      "Model saved in: ./reg_djinn_test_tree0.ckpt\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn.py:276: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./reg_djinn_test_tree0.ckpt\n",
      "Model 0 restored\n",
      "Mean Squa Error : 1.1396745026589397\n",
      "Mean Abso Error : 0.7362111682734646\n",
      "Expl. Variance  : 0.4097634735537927 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from djinn import djinn\n",
    "print(\"djinn example\")    \n",
    "modelname=\"reg_djinn_test\"   # name the model\n",
    "ntrees=1                 # number of trees = number of neural nets in ensemble\n",
    "maxdepth=5               # max depth of tree -- optimize this for each data set\n",
    "dropout_keep=1.0         # dropout typically set to 1 for non-Bayesian models\n",
    "\n",
    "#initialize the model\n",
    "model=djinn.DJINN_Regressor(ntrees,maxdepth,dropout_keep)\n",
    "x_train, y_train, x_test, y_test = out_cnn_train, y_train, out_cnn_test, y_test\n",
    "\n",
    "# find optimal settings: this function returns dict with hyper-parameters\n",
    "# each djinn function accepts random seeds for reproducible behavior\n",
    "optimal=model.get_hyperparameters(x_train, y_train, random_state=42)\n",
    "batchsize=optimal['batch_size']\n",
    "learnrate=optimal['learn_rate']\n",
    "epochs=optimal['epochs']\n",
    "\n",
    "# batchsize=304\n",
    "# learnrate=0.002474296684203603\n",
    "# epochs=210\n",
    " \n",
    "# train the model with hyperparameters determined above\n",
    "model.train(x_train,y_train,epochs=epochs,learn_rate=learnrate, batch_size=batchsize, \n",
    "              display_step=1, save_files=True, file_name=modelname, \n",
    "              save_model=True,model_name=modelname, random_state=1)\n",
    "\n",
    "# *note there is a function model.fit(x_train,y_train, ... ) that wraps \n",
    "# get_hyperparameters() and train(), so that you do not have to manually\n",
    "# pass hyperparameters to train(). However, get_hyperparameters() can\n",
    "# be expensive, so I recommend running it once per dataset and using those\n",
    "# hyperparameter values in train() to save computational time\n",
    "\n",
    "# make predictions\n",
    "m=model.predict(x_test) #returns the median prediction if more than one tree\n",
    "\n",
    "#evaluate results\n",
    "evaluate(y_test,m)\n",
    "\n",
    "#close model \n",
    "model.close_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + ( SVM, XGB, DTree, ExtraTrees, RandomFores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) \n",
      "\n",
      "Mean Squa Error : 1.0028292510416181\n",
      "Mean Abso Error : 0.6969977880939956\n",
      "Expl. Variance  : 0.48070495299962723 \n",
      "\n",
      "================================================================================\n",
      "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
      "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
      "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
      "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
      "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "             warm_start=False) \n",
      "\n",
      "Mean Squa Error : 1.2528304325070787\n",
      "Mean Abso Error : 0.7764602646401114\n",
      "Expl. Variance  : 0.37361885085804547 \n",
      "\n",
      "================================================================================\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "              normalize=False, tol=0.001, verbose=False) \n",
      "\n",
      "Mean Squa Error : 1.2641019188165907\n",
      "Mean Abso Error : 0.7656338007725634\n",
      "Expl. Variance  : 0.3468904099030883 \n",
      "\n",
      "================================================================================\n",
      "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,\n",
      "          fit_path=True, max_iter=500, normalize=True, positive=False,\n",
      "          precompute='auto', verbose=False) \n",
      "\n",
      "Mean Squa Error : 1.9458485744325988\n",
      "Mean Abso Error : 1.09667832837786\n",
      "Expl. Variance  : 0.0 \n",
      "\n",
      "================================================================================\n",
      "ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "              normalize=False, threshold_lambda=10000.0, tol=0.001,\n",
      "              verbose=False) \n",
      "\n",
      "Mean Squa Error : 1.2641019187370748\n",
      "Mean Abso Error : 0.7656338007630836\n",
      "Expl. Variance  : 0.34689040994530074 \n",
      "\n",
      "================================================================================\n",
      "PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,\n",
      "                           epsilon=0.1, fit_intercept=True,\n",
      "                           loss='epsilon_insensitive', max_iter=1000,\n",
      "                           n_iter_no_change=5, random_state=None, shuffle=True,\n",
      "                           tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False) \n",
      "\n",
      "Mean Squa Error : 1.2733608299914971\n",
      "Mean Abso Error : 0.7674178398170783\n",
      "Expl. Variance  : 0.3447580782827022 \n",
      "\n",
      "================================================================================\n",
      "TheilSenRegressor(copy_X=True, fit_intercept=True, max_iter=300,\n",
      "                  max_subpopulation=10000, n_jobs=None, n_subsamples=None,\n",
      "                  random_state=None, tol=0.001, verbose=False) \n",
      "\n",
      "Mean Squa Error : 1.2897818657215818\n",
      "Mean Abso Error : 0.7697490959098748\n",
      "Expl. Variance  : 0.3319635706795835 \n",
      "\n",
      "================================================================================\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) \n",
      "\n",
      "Mean Squa Error : 1.2648994859787717\n",
      "Mean Abso Error : 0.7657287491730281\n",
      "Expl. Variance  : 0.3464670012658644 \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/41925157/logisticregression-unknown-label-type-continuous-using-sklearn-in-python\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(gamma='scale'),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "    linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()\n",
    "              ]\n",
    "\n",
    "for item in classifiers:\n",
    "    print(item,'\\n')\n",
    "    clf = item\n",
    "    clf.fit(out_cnn_train, y_train)\n",
    "    #print(clf.predict(predictionData),'\\n')\n",
    "    #Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "    m = clf.predict(out_cnn_test)\n",
    "    #evaluate results\n",
    "    mse=sklearn.metrics.mean_squared_error(y_test,m)\n",
    "    mabs=sklearn.metrics.mean_absolute_error(y_test,m)\n",
    "    exvar=sklearn.metrics.explained_variance_score(y_test,m)   \n",
    "    print('Mean Squa Error :',mse)\n",
    "    print('Mean Abso Error :',mabs)\n",
    "    print('Expl. Variance  :',exvar,'\\n')\n",
    "    print(\"================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN XGBRegressor      \n",
      "Mean Squa Error : 1.3196314151543835\n",
      "Mean Abso Error : 0.8455447478634971\n",
      "Expl. Variance  : 0.3270072213574784 \n",
      "\n",
      "CNN ExtraTreesRegressor      \n",
      "Mean Squa Error : 1.5830243598620142\n",
      "Mean Abso Error : 0.9273054160125589\n",
      "Expl. Variance  : 0.19493267298354566 \n",
      "\n",
      "CNN DecisionTreeRegressor      \n",
      "Mean Squa Error : 1.740058462416056\n",
      "Mean Abso Error : 0.9819784798534797\n",
      "Expl. Variance  : 0.10933088086725506 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from xgboost import XGBRegressor\n",
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(out_cnn_train, y_train , verbose=False)\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(out_cnn_test)\n",
    "print('CNN XGBRegressor      ')\n",
    "evaluate(y_test,XGBpredictions)\n",
    "\n",
    "    \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "Ext = ExtraTreesRegressor(n_estimators=10)\n",
    "Ext.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictionsCNN_Ext = Ext.predict(out_cnn_test)\n",
    "print('CNN ExtraTreesRegressor      ')\n",
    "evaluate(y_test,predictionsCNN_Ext)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "clf_dt = tree.DecisionTreeRegressor()\n",
    "clf_dt.fit(out_cnn_train, y_train)\n",
    "# Get the mean absolute error on the validation data :\n",
    "clf_dtpredictions = clf_dt.predict(out_cnn_test)\n",
    "print('CNN DecisionTreeRegressor      ')\n",
    "evaluate(y_test,clf_dtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(726, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cnn_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "726/726 [==============================] - 0s 541us/step - loss: 136163.1854\n",
      "Epoch 2/50\n",
      "726/726 [==============================] - 0s 213us/step - loss: 40139.0670\n",
      "Epoch 3/50\n",
      "726/726 [==============================] - 0s 235us/step - loss: 19993.1677\n",
      "Epoch 4/50\n",
      "726/726 [==============================] - 0s 214us/step - loss: 19794.8293\n",
      "Epoch 5/50\n",
      "726/726 [==============================] - 0s 234us/step - loss: 19674.0427\n",
      "Epoch 6/50\n",
      "726/726 [==============================] - 0s 221us/step - loss: 19412.5128\n",
      "Epoch 7/50\n",
      "726/726 [==============================] - 0s 237us/step - loss: 19247.8487\n",
      "Epoch 8/50\n",
      "726/726 [==============================] - 0s 250us/step - loss: 19064.6344\n",
      "Epoch 9/50\n",
      "726/726 [==============================] - 0s 211us/step - loss: 18791.1487\n",
      "Epoch 10/50\n",
      "726/726 [==============================] - 0s 219us/step - loss: 18651.9327\n",
      "Epoch 11/50\n",
      "726/726 [==============================] - 0s 231us/step - loss: 18388.3673\n",
      "Epoch 12/50\n",
      "726/726 [==============================] - 0s 196us/step - loss: 18191.6202\n",
      "Epoch 13/50\n",
      "726/726 [==============================] - 0s 240us/step - loss: 18015.0829\n",
      "Epoch 14/50\n",
      "726/726 [==============================] - 0s 217us/step - loss: 17832.0024\n",
      "Epoch 15/50\n",
      "726/726 [==============================] - 0s 230us/step - loss: 17711.4857\n",
      "Epoch 16/50\n",
      "726/726 [==============================] - 0s 185us/step - loss: 17463.6299\n",
      "Epoch 17/50\n",
      "726/726 [==============================] - 0s 245us/step - loss: 17328.0154\n",
      "Epoch 18/50\n",
      "726/726 [==============================] - 0s 249us/step - loss: 17141.7185\n",
      "Epoch 19/50\n",
      "726/726 [==============================] - 0s 243us/step - loss: 16957.7099\n",
      "Epoch 20/50\n",
      "726/726 [==============================] - 0s 203us/step - loss: 16828.4254\n",
      "Epoch 21/50\n",
      "726/726 [==============================] - 0s 236us/step - loss: 16692.9624\n",
      "Epoch 22/50\n",
      "726/726 [==============================] - 0s 193us/step - loss: 16448.6144\n",
      "Epoch 23/50\n",
      "726/726 [==============================] - 0s 255us/step - loss: 16293.6642\n",
      "Epoch 24/50\n",
      "726/726 [==============================] - 0s 240us/step - loss: 16157.8780\n",
      "Epoch 25/50\n",
      "726/726 [==============================] - 0s 199us/step - loss: 16074.1000\n",
      "Epoch 26/50\n",
      "726/726 [==============================] - 0s 204us/step - loss: 15889.8400\n",
      "Epoch 27/50\n",
      "726/726 [==============================] - 0s 213us/step - loss: 15772.5948\n",
      "Epoch 28/50\n",
      "726/726 [==============================] - 0s 218us/step - loss: 15657.9919\n",
      "Epoch 29/50\n",
      "726/726 [==============================] - 0s 221us/step - loss: 15547.4291\n",
      "Epoch 30/50\n",
      "726/726 [==============================] - 0s 197us/step - loss: 15472.0190\n",
      "Epoch 31/50\n",
      "726/726 [==============================] - 0s 222us/step - loss: 15400.8296\n",
      "Epoch 32/50\n",
      "726/726 [==============================] - 0s 188us/step - loss: 15373.4327\n",
      "Epoch 33/50\n",
      "726/726 [==============================] - 0s 225us/step - loss: 15255.2723\n",
      "Epoch 34/50\n",
      "726/726 [==============================] - 0s 223us/step - loss: 15211.3677\n",
      "Epoch 35/50\n",
      "726/726 [==============================] - 0s 207us/step - loss: 15131.3126\n",
      "Epoch 36/50\n",
      "726/726 [==============================] - 0s 230us/step - loss: 15089.4677\n",
      "Epoch 37/50\n",
      "726/726 [==============================] - 0s 227us/step - loss: 15088.6044\n",
      "Epoch 38/50\n",
      "726/726 [==============================] - 0s 188us/step - loss: 14985.3723\n",
      "Epoch 39/50\n",
      "726/726 [==============================] - 0s 241us/step - loss: 14986.6294\n",
      "Epoch 40/50\n",
      "726/726 [==============================] - 0s 464us/step - loss: 14963.1473\n",
      "Epoch 41/50\n",
      "  5/726 [..............................] - ETA: 0s - loss: 6346.6079"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.120246). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/726 [==============================] - 0s 228us/step - loss: 14889.5764\n",
      "Epoch 42/50\n",
      "726/726 [==============================] - 0s 237us/step - loss: 14972.0761\n",
      "Epoch 43/50\n",
      "726/726 [==============================] - 0s 219us/step - loss: 14911.3568\n",
      "Epoch 44/50\n",
      "726/726 [==============================] - 0s 253us/step - loss: 14871.1269\n",
      "Epoch 45/50\n",
      "726/726 [==============================] - 0s 205us/step - loss: 14890.5891\n",
      "Epoch 46/50\n",
      "726/726 [==============================] - 0s 256us/step - loss: 14843.8831\n",
      "Epoch 47/50\n",
      "726/726 [==============================] - 0s 201us/step - loss: 14824.5605\n",
      "Epoch 48/50\n",
      "726/726 [==============================] - 0s 210us/step - loss: 14915.8649\n",
      "Epoch 49/50\n",
      "726/726 [==============================] - 0s 342us/step - loss: 14819.5869\n",
      "Epoch 50/50\n",
      "726/726 [==============================] - 0s 260us/step - loss: 14889.7923\n",
      "182/182 [==============================] - 0s 416us/step\n",
      "CNN MLP Mean Squa Error : 16.117302065934062\n",
      "CNN MLP Mean Abso Error : 3.7693296703296704\n",
      "CNN MLP Expl. Variance  : 0.001554840457588802\n"
     ]
    }
   ],
   "source": [
    "# Regression Example With Boston Dataset: Baseline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(128, input_dim=out_cnn_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(64, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "clf_MLP = KerasClassifier(build_fn = baseline_model, epochs = 50, batch_size=5, verbose=1)\n",
    "clf_MLP.fit(out_cnn_train, y_train)\n",
    "y_predmlp = clf_MLP.predict(out_cnn_test)\n",
    "\n",
    "#print(\"CNN MLP Model.evaluate : \",clf_MLP.evaluate(out_cnn_test, y_train),'\\n')\n",
    "#evaluate results\n",
    "mse=sklearn.metrics.mean_squared_error(y_test,y_predmlp)\n",
    "mabs=sklearn.metrics.mean_absolute_error(y_test,y_predmlp)\n",
    "exvar=sklearn.metrics.explained_variance_score(y_test,y_predmlp)   \n",
    "print('CNN MLP Mean Squa Error :',mse)\n",
    "print('CNN MLP Mean Abso Error :',mabs)\n",
    "print('CNN MLP Expl. Variance  :',exvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by RandomForest, ExtraTrees, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ = x_train_.reshape(x_train_.shape[0], x_train_.shape[1])\n",
    "x_test_  = x_test_.reshape(x_test_.shape[0], x_test_.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN RandomForestRegressor      \n",
      "Mean Squa Error : 0.9271511900891092\n",
      "Mean Abso Error : 0.672556690526964\n",
      "Expl. Variance  : 0.519473311562582 \n",
      "\n",
      "CNN XGBRegressor      \n",
      "Mean Squa Error : 1.0821057183292784\n",
      "Mean Abso Error : 0.7319394397499797\n",
      "Expl. Variance  : 0.4364850648225346 \n",
      "\n",
      "CNN ExtraTreesRegressor      \n",
      "Mean Squa Error : 0.9297100506818841\n",
      "Mean Abso Error : 0.6841619636316065\n",
      "Expl. Variance  : 0.5207827375966139 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier : from dataset originl\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(x_train_, y_train_)\n",
    "predictions = rf.predict(x_test_)\n",
    "print('CNN RandomForestRegressor      ')\n",
    "evaluate(y_test_,predictions)\n",
    "\n",
    "\n",
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from xgboost import XGBRegressor\n",
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(x_train_, y_train_ , verbose=False)\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(x_test_)\n",
    "print('CNN XGBRegressor      ')\n",
    "evaluate(y_test_,XGBpredictions)\n",
    "\n",
    "    \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "Ext = ExtraTreesRegressor(n_estimators=10)\n",
    "Ext.fit(x_train_, y_train_)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictionsCNN_Ext = Ext.predict(x_test_)\n",
    "print('CNN ExtraTreesRegressor      ')\n",
    "evaluate(y_test_,predictionsCNN_Ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
