{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import sklearn\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "try: from sklearn.model_selection import train_test_split\n",
    "except: from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test,m):\n",
    "    #evaluate results\n",
    "    mse=sklearn.metrics.mean_squared_error(y_test,m)\n",
    "    mabs=sklearn.metrics.mean_absolute_error(y_test,m)\n",
    "    exvar=sklearn.metrics.explained_variance_score(y_test,m)   \n",
    "    print('Mean Squa Error :',mse)\n",
    "    print('Mean Abso Error :',mabs)\n",
    "    print('Expl. Variance  :',exvar,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the train & test and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127.9439</td>\n",
       "      <td>264.9656</td>\n",
       "      <td>68.3481</td>\n",
       "      <td>521.7570</td>\n",
       "      <td>443.6997</td>\n",
       "      <td>195.9648</td>\n",
       "      <td>166.7701</td>\n",
       "      <td>504.2850</td>\n",
       "      <td>104.0226</td>\n",
       "      <td>205.2257</td>\n",
       "      <td>...</td>\n",
       "      <td>92861.7375</td>\n",
       "      <td>84584.2549</td>\n",
       "      <td>89041.0312</td>\n",
       "      <td>87204.4296</td>\n",
       "      <td>97380.7385</td>\n",
       "      <td>94753.7880</td>\n",
       "      <td>81677.9764</td>\n",
       "      <td>103129.6938</td>\n",
       "      <td>97196.8128</td>\n",
       "      <td>1463622.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.4677</td>\n",
       "      <td>278.6497</td>\n",
       "      <td>47.4062</td>\n",
       "      <td>417.3653</td>\n",
       "      <td>551.7083</td>\n",
       "      <td>401.1797</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>161.7773</td>\n",
       "      <td>368.1543</td>\n",
       "      <td>45.9850</td>\n",
       "      <td>...</td>\n",
       "      <td>83029.8436</td>\n",
       "      <td>91483.7433</td>\n",
       "      <td>93748.1139</td>\n",
       "      <td>99467.6415</td>\n",
       "      <td>95935.6597</td>\n",
       "      <td>84254.7876</td>\n",
       "      <td>93786.3095</td>\n",
       "      <td>94499.4790</td>\n",
       "      <td>88941.7152</td>\n",
       "      <td>1489888.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>511.7510</td>\n",
       "      <td>104.4383</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>380.4079</td>\n",
       "      <td>345.8587</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>90.5588</td>\n",
       "      <td>7.2899</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>91309.1752</td>\n",
       "      <td>107598.5159</td>\n",
       "      <td>101897.2685</td>\n",
       "      <td>100876.0477</td>\n",
       "      <td>79350.0981</td>\n",
       "      <td>100060.2964</td>\n",
       "      <td>92733.9494</td>\n",
       "      <td>100646.9126</td>\n",
       "      <td>83394.8729</td>\n",
       "      <td>1495035.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.6990</td>\n",
       "      <td>216.4378</td>\n",
       "      <td>355.2960</td>\n",
       "      <td>67.8151</td>\n",
       "      <td>518.7256</td>\n",
       "      <td>72.1572</td>\n",
       "      <td>222.7933</td>\n",
       "      <td>223.9242</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>312.4474</td>\n",
       "      <td>...</td>\n",
       "      <td>92083.6042</td>\n",
       "      <td>103182.5412</td>\n",
       "      <td>80688.8463</td>\n",
       "      <td>92306.4190</td>\n",
       "      <td>106440.6778</td>\n",
       "      <td>102118.7041</td>\n",
       "      <td>99295.1266</td>\n",
       "      <td>96503.3818</td>\n",
       "      <td>77942.2947</td>\n",
       "      <td>1459841.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>243.3420</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>198.4878</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>566.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>85602.1738</td>\n",
       "      <td>98370.5585</td>\n",
       "      <td>97148.3728</td>\n",
       "      <td>95775.0777</td>\n",
       "      <td>80723.5930</td>\n",
       "      <td>95865.7812</td>\n",
       "      <td>88525.7698</td>\n",
       "      <td>94546.5417</td>\n",
       "      <td>85021.1027</td>\n",
       "      <td>1475765.923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  127.9439  264.9656   68.3481  521.7570  443.6997  195.9648  166.7701   \n",
       "1  500.4677  278.6497   47.4062  417.3653  551.7083  401.1797    0.0000   \n",
       "2  511.7510  104.4383  566.0000  380.4079  345.8587    0.0000   90.5588   \n",
       "3   19.6990  216.4378  355.2960   67.8151  518.7256   72.1572  222.7933   \n",
       "4    0.0000    0.0000  243.3420    0.0000  566.0000    0.0000  198.4878   \n",
       "\n",
       "         7         8         9   ...          39           40           41  \\\n",
       "0  504.2850  104.0226  205.2257  ...  92861.7375   84584.2549   89041.0312   \n",
       "1  161.7773  368.1543   45.9850  ...  83029.8436   91483.7433   93748.1139   \n",
       "2    7.2899  566.0000  566.0000  ...  91309.1752  107598.5159  101897.2685   \n",
       "3  223.9242  566.0000  312.4474  ...  92083.6042  103182.5412   80688.8463   \n",
       "4    0.0000  566.0000  566.0000  ...  85602.1738   98370.5585   97148.3728   \n",
       "\n",
       "            42           43           44          45           46          47  \\\n",
       "0   87204.4296   97380.7385   94753.7880  81677.9764  103129.6938  97196.8128   \n",
       "1   99467.6415   95935.6597   84254.7876  93786.3095   94499.4790  88941.7152   \n",
       "2  100876.0477   79350.0981  100060.2964  92733.9494  100646.9126  83394.8729   \n",
       "3   92306.4190  106440.6778  102118.7041  99295.1266   96503.3818  77942.2947   \n",
       "4   95775.0777   80723.5930   95865.7812  88525.7698   94546.5417  85021.1027   \n",
       "\n",
       "            48  \n",
       "0  1463622.174  \n",
       "1  1489888.502  \n",
       "2  1495035.934  \n",
       "3  1459841.498  \n",
       "4  1475765.923  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "# load data\n",
    "df=pd.read_csv('WECs_DataSet/Sydney_Data.csv', header = None)\n",
    "\n",
    "# drop nan \n",
    "df = df.dropna()\n",
    "# the head of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "# df to values\n",
    "df = df.values\n",
    "Y = df[:,48]\n",
    "X = df[:,0:48]\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(X.shape)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.2, random_state=1) \n",
    "x_train_, x_test_, y_train_, y_test_ = x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "\n",
    "    mean = np.mean(train, axis=0)\n",
    "    std = np.std(train, axis=0)+0.000001\n",
    "\n",
    "    X_train = (train - mean) / std\n",
    "    X_test = (test - mean) /std\n",
    "    return X_train, X_test\n",
    "\n",
    "x_train, x_test = standardize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14400, 48, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.any(np.isnan(X_test)))\n",
    "# print(np.any(np.isnan(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def CNN_net():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation=\"relu\", input_shape=(X.shape[1],1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a.berrouachedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 57600 samples, validate on 14400 samples\n",
      "Epoch 1/250\n",
      "57600/57600 [==============================] - 49s 856us/step - loss: 108989544185.0653 - val_loss: 2474257267.8861\n",
      "Epoch 2/250\n",
      "57600/57600 [==============================] - 42s 737us/step - loss: 4948433499.8229 - val_loss: 1149785189.8611\n",
      "Epoch 3/250\n",
      "57600/57600 [==============================] - 42s 727us/step - loss: 4218300673.6170 - val_loss: 870509601.1986\n",
      "Epoch 4/250\n",
      "57600/57600 [==============================] - 44s 760us/step - loss: 4024232746.8646 - val_loss: 746154916.2403\n",
      "Epoch 5/250\n",
      "57600/57600 [==============================] - 44s 767us/step - loss: 3933769824.8319 - val_loss: 1001846648.6083\n",
      "Epoch 6/250\n",
      "57600/57600 [==============================] - 46s 791us/step - loss: 3911256230.3223 - val_loss: 1185487968.7278\n",
      "Epoch 7/250\n",
      "57600/57600 [==============================] - 44s 768us/step - loss: 3881887282.2208 - val_loss: 780551959.1028\n",
      "Epoch 8/250\n",
      "57600/57600 [==============================] - 55s 949us/step - loss: 3839890034.0590 - val_loss: 844316912.4375\n",
      "Epoch 9/250\n",
      "57600/57600 [==============================] - 45s 775us/step - loss: 3825581943.5056 - val_loss: 901191706.9208\n",
      "Epoch 10/250\n",
      "57600/57600 [==============================] - 47s 816us/step - loss: 3859769439.7205 - val_loss: 1048333049.7389\n",
      "Epoch 11/250\n",
      "57600/57600 [==============================] - 46s 791us/step - loss: 3873367999.0538 - val_loss: 1862559825.5667\n",
      "Epoch 12/250\n",
      "57600/57600 [==============================] - 46s 807us/step - loss: 3899974549.9576 - val_loss: 953127770.7056\n",
      "Epoch 13/250\n",
      "57600/57600 [==============================] - 41s 707us/step - loss: 3875678741.8389 - val_loss: 1473477811.9278\n",
      "Epoch 14/250\n",
      "57600/57600 [==============================] - 44s 758us/step - loss: 3817732912.6451 - val_loss: 1015396365.1972\n",
      "Epoch 15/250\n",
      "57600/57600 [==============================] - 44s 759us/step - loss: 3842182889.9111 - val_loss: 1790713185.4111\n",
      "Epoch 16/250\n",
      "57600/57600 [==============================] - 45s 785us/step - loss: 3805319368.5431 - val_loss: 1045396557.3722\n",
      "Epoch 17/250\n",
      "57600/57600 [==============================] - 45s 790us/step - loss: 3789120170.5962 - val_loss: 1355879557.9056\n",
      "Epoch 18/250\n",
      "57600/57600 [==============================] - 45s 789us/step - loss: 3785269048.9215 - val_loss: 1972879717.9444\n",
      "Epoch 19/250\n",
      "57600/57600 [==============================] - 44s 756us/step - loss: 3831646985.7417 - val_loss: 943450954.7889\n",
      "Epoch 20/250\n",
      "57600/57600 [==============================] - 44s 756us/step - loss: 3822256888.7448 - val_loss: 1176799089.5056\n",
      "Epoch 21/250\n",
      "57600/57600 [==============================] - 44s 769us/step - loss: 3808562948.5740 - val_loss: 1164886050.7139\n",
      "Epoch 22/250\n",
      "57600/57600 [==============================] - 45s 789us/step - loss: 3837548352.2097 - val_loss: 1488249431.3778\n",
      "Epoch 23/250\n",
      "57600/57600 [==============================] - 43s 752us/step - loss: 3811606622.4427 - val_loss: 1387842496.4667\n",
      "Epoch 24/250\n",
      "57600/57600 [==============================] - 45s 779us/step - loss: 3786424909.5361 - val_loss: 2419982088.0444\n",
      "Epoch 25/250\n",
      "57600/57600 [==============================] - 44s 757us/step - loss: 3794883024.5139 - val_loss: 1604968102.2167\n",
      "Epoch 26/250\n",
      "57600/57600 [==============================] - 41s 706us/step - loss: 3753421808.0903 - val_loss: 1992114836.4556\n",
      "Epoch 27/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 3750819448.4931 - val_loss: 1432205110.6944\n",
      "Epoch 28/250\n",
      "57600/57600 [==============================] - 39s 682us/step - loss: 3778641705.3899 - val_loss: 1656288324.5556\n",
      "Epoch 29/250\n",
      "57600/57600 [==============================] - 39s 681us/step - loss: 3759183132.5674 - val_loss: 3044467329.5556\n",
      "Epoch 30/250\n",
      "57600/57600 [==============================] - 42s 728us/step - loss: 3721430814.7674 - val_loss: 2879413148.3333\n",
      "Epoch 31/250\n",
      "57600/57600 [==============================] - 44s 772us/step - loss: 3741862364.4972 - val_loss: 1403321482.8056\n",
      "Epoch 32/250\n",
      "57600/57600 [==============================] - 44s 757us/step - loss: 3716286571.0236 - val_loss: 2304374045.7333\n",
      "Epoch 33/250\n",
      "57600/57600 [==============================] - 41s 717us/step - loss: 3770399069.1743 - val_loss: 1779547766.7000\n",
      "Epoch 34/250\n",
      "57600/57600 [==============================] - 44s 770us/step - loss: 3727476930.4972 - val_loss: 2463949390.0667\n",
      "Epoch 35/250\n",
      "57600/57600 [==============================] - 45s 773us/step - loss: 3702076585.2083 - val_loss: 2484993159.6000\n",
      "Epoch 36/250\n",
      "57600/57600 [==============================] - 44s 771us/step - loss: 3746215817.0267 - val_loss: 1813477176.9000\n",
      "Epoch 37/250\n",
      "57600/57600 [==============================] - 42s 727us/step - loss: 3703248567.0792 - val_loss: 3125596514.4667\n",
      "Epoch 38/250\n",
      "57600/57600 [==============================] - 47s 815us/step - loss: 3741605028.7424 - val_loss: 2856643601.6667\n",
      "Epoch 39/250\n",
      "57600/57600 [==============================] - 52s 905us/step - loss: 3656607297.8528 - val_loss: 2319412289.3333\n",
      "Epoch 40/250\n",
      "57600/57600 [==============================] - 50s 859us/step - loss: 3701237714.6726 - val_loss: 2776921186.4667\n",
      "Epoch 41/250\n",
      "57600/57600 [==============================] - 44s 763us/step - loss: 3666616902.9785 - val_loss: 3741184693.3333\n",
      "Epoch 42/250\n",
      "57600/57600 [==============================] - 45s 775us/step - loss: 3687786321.7007 - val_loss: 2847904494.9778\n",
      "Epoch 43/250\n",
      "57600/57600 [==============================] - 44s 769us/step - loss: 3650397586.3274 - val_loss: 4440405155.7778\n",
      "Epoch 44/250\n",
      "57600/57600 [==============================] - 41s 720us/step - loss: 3689738937.8819 - val_loss: 3556389592.0889\n",
      "Epoch 45/250\n",
      "57600/57600 [==============================] - 41s 715us/step - loss: 3659423505.0295 - val_loss: 3176435354.4000\n",
      "Epoch 46/250\n",
      "57600/57600 [==============================] - 41s 710us/step - loss: 3650844832.8556 - val_loss: 3486565477.2000\n",
      "Epoch 47/250\n",
      "57600/57600 [==============================] - 41s 717us/step - loss: 3659931335.7094 - val_loss: 4597071699.9556\n",
      "Epoch 48/250\n",
      "57600/57600 [==============================] - 48s 838us/step - loss: 3641211981.8642 - val_loss: 4146100017.3778\n",
      "Epoch 49/250\n",
      "57600/57600 [==============================] - 49s 852us/step - loss: 3657510138.5170 - val_loss: 5313874966.7556\n",
      "Epoch 50/250\n",
      "57600/57600 [==============================] - 46s 801us/step - loss: 3648857300.8056 - val_loss: 4540538380.7556\n",
      "Epoch 51/250\n",
      "57600/57600 [==============================] - 53s 919us/step - loss: 3614065241.0406 - val_loss: 4724310393.7778\n",
      "Epoch 52/250\n",
      "57600/57600 [==============================] - 46s 792us/step - loss: 3620421155.5569 - val_loss: 5284510954.1778\n",
      "Epoch 53/250\n",
      "57600/57600 [==============================] - 44s 758us/step - loss: 3588443735.2545 - val_loss: 6069567224.6222\n",
      "Epoch 54/250\n",
      "57600/57600 [==============================] - 41s 716us/step - loss: 3613349530.8217 - val_loss: 5091158784.8000\n",
      "Epoch 55/250\n",
      "57600/57600 [==============================] - 49s 843us/step - loss: 3582724104.2722 - val_loss: 5314740592.5333\n",
      "Epoch 56/250\n",
      "57600/57600 [==============================] - 45s 773us/step - loss: 3596906380.3788 - val_loss: 7139443533.2444\n",
      "Epoch 57/250\n",
      "57600/57600 [==============================] - 43s 744us/step - loss: 3543030858.2021 - val_loss: 6209728623.1111\n",
      "Epoch 58/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57600/57600 [==============================] - 42s 730us/step - loss: 3572490716.3347 - val_loss: 6478122009.6889\n",
      "Epoch 59/250\n",
      "57600/57600 [==============================] - 41s 713us/step - loss: 3565887598.8295 - val_loss: 7363864336.7111\n",
      "Epoch 60/250\n",
      "57600/57600 [==============================] - 42s 732us/step - loss: 3563605074.6885 - val_loss: 6663888948.8889\n",
      "Epoch 61/250\n",
      "57600/57600 [==============================] - 41s 710us/step - loss: 3562193277.9118 - val_loss: 8610391181.6000\n",
      "Epoch 62/250\n",
      "57600/57600 [==============================] - 41s 720us/step - loss: 3555881191.9828 - val_loss: 9283743049.7778\n",
      "Epoch 63/250\n",
      "57600/57600 [==============================] - 42s 736us/step - loss: 3542525380.6493 - val_loss: 7799219014.4889\n",
      "Epoch 64/250\n",
      "57600/57600 [==============================] - 42s 729us/step - loss: 3523370727.1885 - val_loss: 10415989612.2667\n",
      "Epoch 65/250\n",
      "57600/57600 [==============================] - 42s 728us/step - loss: 3534573921.2875 - val_loss: 10577918583.1111\n",
      "Epoch 66/250\n",
      "57600/57600 [==============================] - 42s 724us/step - loss: 3482945156.8267 - val_loss: 10511298546.3111\n",
      "Epoch 67/250\n",
      "57600/57600 [==============================] - 43s 744us/step - loss: 3520640373.7684 - val_loss: 8930156256.5333\n",
      "Epoch 68/250\n",
      "57600/57600 [==============================] - 42s 724us/step - loss: 3516417614.8319 - val_loss: 11004734368.7111\n",
      "Epoch 69/250\n",
      "57600/57600 [==============================] - 42s 736us/step - loss: 3493068799.2743 - val_loss: 10780490818.4889\n",
      "Epoch 70/250\n",
      "57600/57600 [==============================] - 43s 741us/step - loss: 3465626591.8361 - val_loss: 12713442178.8444\n",
      "Epoch 71/250\n",
      "57600/57600 [==============================] - 42s 729us/step - loss: 3458634866.1896 - val_loss: 15489948467.5556\n",
      "Epoch 72/250\n",
      "57600/57600 [==============================] - 41s 715us/step - loss: 3463145683.8073 - val_loss: 12766859804.8000\n",
      "Epoch 73/250\n",
      "57600/57600 [==============================] - 41s 706us/step - loss: 3475567620.1816 - val_loss: 13083897652.9778\n",
      "Epoch 74/250\n",
      "57600/57600 [==============================] - 41s 717us/step - loss: 3433075006.4556 - val_loss: 14209750062.7556\n",
      "Epoch 75/250\n",
      "57600/57600 [==============================] - 41s 716us/step - loss: 3461409117.4257 - val_loss: 16080577906.8444\n",
      "Epoch 76/250\n",
      "57600/57600 [==============================] - 40s 698us/step - loss: 3473122961.6293 - val_loss: 16041925515.7333\n",
      "Epoch 77/250\n",
      "57600/57600 [==============================] - 40s 701us/step - loss: 3442948002.8885 - val_loss: 17648250546.8444\n",
      "Epoch 78/250\n",
      "57600/57600 [==============================] - 40s 703us/step - loss: 3450987407.2910 - val_loss: 18107175438.9333\n",
      "Epoch 79/250\n",
      "57600/57600 [==============================] - 42s 725us/step - loss: 3406274989.4066 - val_loss: 20068681849.2444\n",
      "Epoch 80/250\n",
      "57600/57600 [==============================] - 41s 719us/step - loss: 3398248807.3729 - val_loss: 18941158134.0444\n",
      "Epoch 81/250\n",
      "57600/57600 [==============================] - 41s 713us/step - loss: 3418262188.4896 - val_loss: 23263931088.3556\n",
      "Epoch 82/250\n",
      "57600/57600 [==============================] - 41s 710us/step - loss: 3382155257.1382 - val_loss: 21534680814.9333\n",
      "Epoch 83/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 3402862845.1458 - val_loss: 23844136599.1111\n",
      "Epoch 84/250\n",
      "57600/57600 [==============================] - 43s 744us/step - loss: 3329295285.2000 - val_loss: 24494553187.5556\n",
      "Epoch 85/250\n",
      "57600/57600 [==============================] - 42s 730us/step - loss: 3371634725.9528 - val_loss: 26176367670.0444\n",
      "Epoch 86/250\n",
      "57600/57600 [==============================] - 41s 710us/step - loss: 3323785210.7052 - val_loss: 27186600852.6222\n",
      "Epoch 87/250\n",
      "57600/57600 [==============================] - 40s 700us/step - loss: 3337398194.5965 - val_loss: 26645846498.1333\n",
      "Epoch 88/250\n",
      "57600/57600 [==============================] - 42s 726us/step - loss: 3334262350.1264 - val_loss: 32469802022.4000\n",
      "Epoch 89/250\n",
      "57600/57600 [==============================] - 41s 709us/step - loss: 3306017506.7365 - val_loss: 31043172462.2222\n",
      "Epoch 90/250\n",
      "57600/57600 [==============================] - 43s 744us/step - loss: 3312566060.9010 - val_loss: 34553208143.6444\n",
      "Epoch 91/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 3330563776.6778 - val_loss: 31245313604.9778\n",
      "Epoch 92/250\n",
      "57600/57600 [==============================] - 41s 713us/step - loss: 3307372608.3854 - val_loss: 32736097949.8667\n",
      "Epoch 93/250\n",
      "57600/57600 [==============================] - 44s 770us/step - loss: 3334788267.7241 - val_loss: 38269648572.4444\n",
      "Epoch 94/250\n",
      "57600/57600 [==============================] - 47s 809us/step - loss: 3275581419.8674 - val_loss: 38429461645.5111\n",
      "Epoch 95/250\n",
      "57600/57600 [==============================] - 46s 791us/step - loss: 3252405921.7285 - val_loss: 36865922493.8667\n",
      "Epoch 96/250\n",
      "57600/57600 [==============================] - 45s 788us/step - loss: 3264310297.2142 - val_loss: 42857893970.4889\n",
      "Epoch 97/250\n",
      "57600/57600 [==============================] - 50s 872us/step - loss: 3217155025.5604 - val_loss: 46335611282.4889\n",
      "Epoch 98/250\n",
      "57600/57600 [==============================] - 42s 723us/step - loss: 3242464767.9017 - val_loss: 49572831583.2889\n",
      "Epoch 99/250\n",
      "57600/57600 [==============================] - 43s 747us/step - loss: 3220146643.9302 - val_loss: 52505926712.8889\n",
      "Epoch 100/250\n",
      "57600/57600 [==============================] - 43s 752us/step - loss: 3234607034.2274 - val_loss: 49519858373.6889\n",
      "Epoch 101/250\n",
      "57600/57600 [==============================] - 42s 737us/step - loss: 3181961570.1365 - val_loss: 48268849009.7778\n",
      "Epoch 102/250\n",
      "57600/57600 [==============================] - 49s 850us/step - loss: 3207558767.3950 - val_loss: 59058456321.4222\n",
      "Epoch 103/250\n",
      "57600/57600 [==============================] - 44s 760us/step - loss: 3173662062.3267 - val_loss: 57666062166.7556\n",
      "Epoch 104/250\n",
      "57600/57600 [==============================] - 42s 722us/step - loss: 3177041535.8403 - val_loss: 53771500299.3778\n",
      "Epoch 105/250\n",
      "57600/57600 [==============================] - 42s 729us/step - loss: 3151972303.9365 - val_loss: 64280889049.6000\n",
      "Epoch 106/250\n",
      "57600/57600 [==============================] - 41s 716us/step - loss: 3164688501.9139 - val_loss: 53750048803.5556\n",
      "Epoch 107/250\n",
      "57600/57600 [==============================] - 42s 737us/step - loss: 3146457308.3087 - val_loss: 65153036605.1556\n",
      "Epoch 108/250\n",
      "57600/57600 [==============================] - 43s 741us/step - loss: 3158741424.9701 - val_loss: 65698679948.8000\n",
      "Epoch 109/250\n",
      "57600/57600 [==============================] - 43s 741us/step - loss: 3189710092.4609 - val_loss: 73752256971.3778\n",
      "Epoch 110/250\n",
      "57600/57600 [==============================] - 43s 749us/step - loss: 3135494142.4007 - val_loss: 75581466571.3778\n",
      "Epoch 111/250\n",
      "57600/57600 [==============================] - 45s 788us/step - loss: 3140552743.7490 - val_loss: 77934927746.8445\n",
      "Epoch 112/250\n",
      "57600/57600 [==============================] - 48s 826us/step - loss: 3148170774.8226 - val_loss: 76760821528.1778\n",
      "Epoch 113/250\n",
      "57600/57600 [==============================] - 46s 797us/step - loss: 3086404735.1861 - val_loss: 80736165893.6889\n",
      "Epoch 114/250\n",
      "57600/57600 [==============================] - 48s 834us/step - loss: 3148951309.6771 - val_loss: 84205765737.2444\n",
      "Epoch 115/250\n",
      "57600/57600 [==============================] - 50s 876us/step - loss: 3113520539.9583 - val_loss: 83644327351.4667\n",
      "Epoch 116/250\n",
      "57600/57600 [==============================] - 47s 820us/step - loss: 3089275593.2032 - val_loss: 93079658276.9778\n",
      "Epoch 117/250\n",
      "57600/57600 [==============================] - 47s 815us/step - loss: 3082235834.0281 - val_loss: 85408058589.8667\n",
      "Epoch 118/250\n",
      "57600/57600 [==============================] - 48s 830us/step - loss: 3083293318.3490 - val_loss: 91643700920.8889\n",
      "Epoch 119/250\n",
      "57600/57600 [==============================] - 46s 796us/step - loss: 3066730709.9431 - val_loss: 97541733353.2444\n",
      "Epoch 120/250\n",
      "57600/57600 [==============================] - 43s 750us/step - loss: 3066116906.1188 - val_loss: 97639319466.6667\n",
      "Epoch 121/250\n",
      "57600/57600 [==============================] - 44s 763us/step - loss: 3077546293.0208 - val_loss: 100255350442.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/250\n",
      "57600/57600 [==============================] - 43s 750us/step - loss: 3074020232.2946 - val_loss: 100827391726.9333\n",
      "Epoch 123/250\n",
      "57600/57600 [==============================] - 43s 755us/step - loss: 3057266874.8410 - val_loss: 93229216520.5333\n",
      "Epoch 124/250\n",
      "57600/57600 [==============================] - 44s 755us/step - loss: 3066590080.9806 - val_loss: 109972124273.7778\n",
      "Epoch 125/250\n",
      "57600/57600 [==============================] - 46s 791us/step - loss: 3033203525.7221 - val_loss: 102141984150.7556\n",
      "Epoch 126/250\n",
      "57600/57600 [==============================] - 45s 776us/step - loss: 3040286740.0431 - val_loss: 102200776686.9333\n",
      "Epoch 127/250\n",
      "57600/57600 [==============================] - 43s 749us/step - loss: 3002257248.7125 - val_loss: 111502553318.4000\n",
      "Epoch 128/250\n",
      "57600/57600 [==============================] - 44s 771us/step - loss: 3052385090.4990 - val_loss: 108154736725.3333\n",
      "Epoch 129/250\n",
      "57600/57600 [==============================] - 50s 876us/step - loss: 3013222149.2625 - val_loss: 117291849221.6889\n",
      "Epoch 130/250\n",
      "57600/57600 [==============================] - 50s 867us/step - loss: 2988474984.2503 - val_loss: 113435140699.0222\n",
      "Epoch 131/250\n",
      "57600/57600 [==============================] - 50s 867us/step - loss: 3019141794.5523 - val_loss: 116201849048.1778\n",
      "Epoch 132/250\n",
      "57600/57600 [==============================] - 46s 799us/step - loss: 2997196645.5340 - val_loss: 124897231997.1555\n",
      "Epoch 133/250\n",
      "57600/57600 [==============================] - 51s 883us/step - loss: 3004021056.3175 - val_loss: 125016081026.8445\n",
      "Epoch 134/250\n",
      "57600/57600 [==============================] - 47s 812us/step - loss: 2998421755.3606 - val_loss: 124905101428.6222\n",
      "Epoch 135/250\n",
      "57600/57600 [==============================] - 48s 825us/step - loss: 2983568596.8722 - val_loss: 124682981651.9111\n",
      "Epoch 136/250\n",
      "57600/57600 [==============================] - 47s 814us/step - loss: 2991120207.7814 - val_loss: 131086776519.1111\n",
      "Epoch 137/250\n",
      "57600/57600 [==============================] - 46s 806us/step - loss: 2964490499.0125 - val_loss: 125890193382.4000\n",
      "Epoch 138/250\n",
      "57600/57600 [==============================] - 45s 779us/step - loss: 2972481627.5125 - val_loss: 122586503873.4222\n",
      "Epoch 139/250\n",
      "57600/57600 [==============================] - 42s 721us/step - loss: 2964194827.4514 - val_loss: 134305990863.6444\n",
      "Epoch 140/250\n",
      "57600/57600 [==============================] - 42s 722us/step - loss: 2931011990.4087 - val_loss: 133280260352.0000\n",
      "Epoch 141/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2953992272.6562 - val_loss: 134758252529.7778\n",
      "Epoch 142/250\n",
      "57600/57600 [==============================] - 41s 716us/step - loss: 2929940744.5634 - val_loss: 133300280786.4889\n",
      "Epoch 143/250\n",
      "57600/57600 [==============================] - 41s 716us/step - loss: 2943245976.1250 - val_loss: 133360832904.5334\n",
      "Epoch 144/250\n",
      "57600/57600 [==============================] - 41s 720us/step - loss: 2951175156.5889 - val_loss: 132259587009.4222\n",
      "Epoch 145/250\n",
      "57600/57600 [==============================] - 43s 741us/step - loss: 2957355401.4148 - val_loss: 138990930201.6000\n",
      "Epoch 146/250\n",
      "57600/57600 [==============================] - 42s 722us/step - loss: 2932009079.9087 - val_loss: 131585857883.0222\n",
      "Epoch 147/250\n",
      "57600/57600 [==============================] - 41s 711us/step - loss: 2908988516.6391 - val_loss: 137777960655.6444\n",
      "Epoch 148/250\n",
      "57600/57600 [==============================] - 41s 720us/step - loss: 2879011157.1429 - val_loss: 145864682916.9778\n",
      "Epoch 149/250\n",
      "57600/57600 [==============================] - 41s 720us/step - loss: 2911155904.2472 - val_loss: 136516432938.6667\n",
      "Epoch 150/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2930952490.1017 - val_loss: 135996212758.7556\n",
      "Epoch 151/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2894300500.3847 - val_loss: 136186133785.6000\n",
      "Epoch 152/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2896492133.0932 - val_loss: 135828854178.1333\n",
      "Epoch 153/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2893025185.3396 - val_loss: 139783255438.2222\n",
      "Epoch 154/250\n",
      "57600/57600 [==============================] - 43s 745us/step - loss: 2893142552.6417 - val_loss: 131159936452.2666\n",
      "Epoch 155/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2867896926.3310 - val_loss: 136938026934.0445\n",
      "Epoch 156/250\n",
      "57600/57600 [==============================] - 40s 694us/step - loss: 2901009569.7401 - val_loss: 145775320314.3111\n",
      "Epoch 157/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2865884471.1135 - val_loss: 141767708455.8222\n",
      "Epoch 158/250\n",
      "57600/57600 [==============================] - 40s 702us/step - loss: 2879265001.1684 - val_loss: 144053120614.4000\n",
      "Epoch 159/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2863908691.4266 - val_loss: 136469974809.6000\n",
      "Epoch 160/250\n",
      "57600/57600 [==============================] - 40s 702us/step - loss: 2861967502.2918 - val_loss: 130792835148.8000\n",
      "Epoch 161/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2864232454.7323 - val_loss: 138358743768.1778\n",
      "Epoch 162/250\n",
      "57600/57600 [==============================] - 41s 704us/step - loss: 2861332423.8132 - val_loss: 134481161810.4889\n",
      "Epoch 163/250\n",
      "57600/57600 [==============================] - 40s 689us/step - loss: 2820244482.4972 - val_loss: 136670790391.4666\n",
      "Epoch 164/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2836273520.1726 - val_loss: 140281120901.6889\n",
      "Epoch 165/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2864469711.6478 - val_loss: 149164583549.1555\n",
      "Epoch 166/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2847732226.3507 - val_loss: 143029453309.1555\n",
      "Epoch 167/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2840170993.9802 - val_loss: 124718525516.8000\n",
      "Epoch 168/250\n",
      "57600/57600 [==============================] - 40s 700us/step - loss: 2820854781.4727 - val_loss: 138024990802.4889\n",
      "Epoch 169/250\n",
      "57600/57600 [==============================] - 40s 690us/step - loss: 2800337565.3128 - val_loss: 134951310247.8222\n",
      "Epoch 170/250\n",
      "57600/57600 [==============================] - 40s 702us/step - loss: 2829976092.2134 - val_loss: 142223563306.6667\n",
      "Epoch 171/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2830348049.2094 - val_loss: 134150249961.2444\n",
      "Epoch 172/250\n",
      "57600/57600 [==============================] - 40s 702us/step - loss: 2776673510.9226 - val_loss: 138518440337.0667\n",
      "Epoch 173/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2824154126.9632 - val_loss: 137464360337.0667\n",
      "Epoch 174/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2797639447.7229 - val_loss: 133155522170.3111\n",
      "Epoch 175/250\n",
      "57600/57600 [==============================] - 40s 688us/step - loss: 2802072493.0328 - val_loss: 139203341326.2222\n",
      "Epoch 176/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2774385507.0827 - val_loss: 131561604067.5556\n",
      "Epoch 177/250\n",
      "57600/57600 [==============================] - 40s 688us/step - loss: 2776138976.7226 - val_loss: 130739346449.0667\n",
      "Epoch 178/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2780115802.7637 - val_loss: 132204830273.4222\n",
      "Epoch 179/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2769383387.3005 - val_loss: 134385798687.2889\n",
      "Epoch 180/250\n",
      "57600/57600 [==============================] - 40s 701us/step - loss: 2771804008.2524 - val_loss: 135335197388.8000\n",
      "Epoch 181/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2749101994.4707 - val_loss: 130210432514.8445\n",
      "Epoch 182/250\n",
      "57600/57600 [==============================] - 40s 698us/step - loss: 2763493761.9313 - val_loss: 125393583257.6000\n",
      "Epoch 183/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2750610117.5613 - val_loss: 122155670175.2889\n",
      "Epoch 184/250\n",
      "57600/57600 [==============================] - 40s 702us/step - loss: 2752096845.7476 - val_loss: 127762826387.9111\n",
      "Epoch 185/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57600/57600 [==============================] - 40s 697us/step - loss: 2746117477.4285 - val_loss: 122405459840.0000\n",
      "Epoch 186/250\n",
      "57600/57600 [==============================] - 40s 700us/step - loss: 2739152428.8865 - val_loss: 121933069232.3556\n",
      "Epoch 187/250\n",
      "57600/57600 [==============================] - 40s 690us/step - loss: 2719749094.7012 - val_loss: 129729366357.3333\n",
      "Epoch 188/250\n",
      "57600/57600 [==============================] - 40s 694us/step - loss: 2752837025.1743 - val_loss: 127771010540.0889\n",
      "Epoch 189/250\n",
      "57600/57600 [==============================] - 39s 686us/step - loss: 2738936492.0965 - val_loss: 119066683690.6667\n",
      "Epoch 190/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2744954127.0330 - val_loss: 117733444499.9111\n",
      "Epoch 191/250\n",
      "57600/57600 [==============================] - 40s 688us/step - loss: 2690776225.4127 - val_loss: 122005482925.5111\n",
      "Epoch 192/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2712896895.8250 - val_loss: 126034689063.8222\n",
      "Epoch 193/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2704933471.6976 - val_loss: 121782566334.5778\n",
      "Epoch 194/250\n",
      "57600/57600 [==============================] - 40s 700us/step - loss: 2686752757.8175 - val_loss: 109782355123.2000\n",
      "Epoch 195/250\n",
      "57600/57600 [==============================] - 40s 696us/step - loss: 2708769648.0149 - val_loss: 117004069356.0889\n",
      "Epoch 196/250\n",
      "57600/57600 [==============================] - 40s 701us/step - loss: 2683520544.2844 - val_loss: 114267765523.9111\n",
      "Epoch 197/250\n",
      "57600/57600 [==============================] - 40s 694us/step - loss: 2683354368.8354 - val_loss: 118936053222.4000\n",
      "Epoch 198/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2670399115.1660 - val_loss: 115579064834.8445\n",
      "Epoch 199/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2676772707.0247 - val_loss: 118782701388.8000\n",
      "Epoch 200/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2675158880.5962 - val_loss: 108415064871.8222\n",
      "Epoch 201/250\n",
      "57600/57600 [==============================] - 42s 721us/step - loss: 2667449061.3118 - val_loss: 114973756856.8889\n",
      "Epoch 202/250\n",
      "57600/57600 [==============================] - 44s 762us/step - loss: 2670249046.2069 - val_loss: 108665631195.0222\n",
      "Epoch 203/250\n",
      "57600/57600 [==============================] - 44s 759us/step - loss: 2626518490.6604 - val_loss: 112920762934.0445\n",
      "Epoch 204/250\n",
      "57600/57600 [==============================] - 42s 724us/step - loss: 2657056231.5885 - val_loss: 113354193675.3778\n",
      "Epoch 205/250\n",
      "57600/57600 [==============================] - 41s 714us/step - loss: 2629880236.2872 - val_loss: 100179732795.7333\n",
      "Epoch 206/250\n",
      "57600/57600 [==============================] - 41s 705us/step - loss: 2648945179.2243 - val_loss: 106006081467.7333\n",
      "Epoch 207/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2656500808.5993 - val_loss: 110814456866.1333\n",
      "Epoch 208/250\n",
      "57600/57600 [==============================] - 41s 710us/step - loss: 2613031364.2316 - val_loss: 101191518037.3333\n",
      "Epoch 209/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2606127197.6826 - val_loss: 100693768231.8222\n",
      "Epoch 210/250\n",
      "57600/57600 [==============================] - 41s 705us/step - loss: 2619488371.4944 - val_loss: 101649425709.5111\n",
      "Epoch 211/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2618482365.8028 - val_loss: 98503264611.5556\n",
      "Epoch 212/250\n",
      "57600/57600 [==============================] - 40s 703us/step - loss: 2608215431.8340 - val_loss: 97024435549.8667\n",
      "Epoch 213/250\n",
      "57600/57600 [==============================] - 40s 688us/step - loss: 2583762747.6340 - val_loss: 98378652717.5111\n",
      "Epoch 214/250\n",
      "57600/57600 [==============================] - 40s 698us/step - loss: 2588720625.0276 - val_loss: 92580733459.9111\n",
      "Epoch 215/250\n",
      "57600/57600 [==============================] - 40s 689us/step - loss: 2578197369.0137 - val_loss: 94047228711.8222\n",
      "Epoch 216/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2580986851.1271 - val_loss: 91505473575.8222\n",
      "Epoch 217/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2569066034.1694 - val_loss: 90515619490.1333\n",
      "Epoch 218/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2561717622.7014 - val_loss: 88849865048.1778\n",
      "Epoch 219/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2550692253.7458 - val_loss: 91453985200.3556\n",
      "Epoch 220/250\n",
      "57600/57600 [==============================] - 40s 701us/step - loss: 2540730759.3920 - val_loss: 85864122999.4667\n",
      "Epoch 221/250\n",
      "57600/57600 [==============================] - 40s 703us/step - loss: 2551995169.8028 - val_loss: 90114419194.3111\n",
      "Epoch 222/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2529966553.9753 - val_loss: 85889481809.0667\n",
      "Epoch 223/250\n",
      "57600/57600 [==============================] - 40s 694us/step - loss: 2533269652.7467 - val_loss: 81777963000.8889\n",
      "Epoch 224/250\n",
      "57600/57600 [==============================] - 40s 698us/step - loss: 2519257012.6368 - val_loss: 82292906331.0222\n",
      "Epoch 225/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2491648932.6905 - val_loss: 80008226463.2889\n",
      "Epoch 226/250\n",
      "57600/57600 [==============================] - 40s 699us/step - loss: 2507182105.7698 - val_loss: 84148149998.9333\n",
      "Epoch 227/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2502455661.7318 - val_loss: 78785109539.5556\n",
      "Epoch 228/250\n",
      "57600/57600 [==============================] - 42s 734us/step - loss: 2457517615.9917 - val_loss: 82170603140.2667\n",
      "Epoch 229/250\n",
      "57600/57600 [==============================] - 43s 753us/step - loss: 2496044225.6493 - val_loss: 78548445226.6667\n",
      "Epoch 230/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2484531589.4740 - val_loss: 75594425315.5556\n",
      "Epoch 231/250\n",
      "57600/57600 [==============================] - 40s 697us/step - loss: 2482652452.0281 - val_loss: 73324921261.5111\n",
      "Epoch 232/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2450564071.0231 - val_loss: 75159563625.2444\n",
      "Epoch 233/250\n",
      "57600/57600 [==============================] - 40s 698us/step - loss: 2452030034.2229 - val_loss: 75168031625.9556\n",
      "Epoch 234/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2456741905.8635 - val_loss: 70884457251.5556\n",
      "Epoch 235/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2426991295.0476 - val_loss: 71599209346.8445\n",
      "Epoch 236/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2418357067.6680 - val_loss: 68094498282.6667\n",
      "Epoch 237/250\n",
      "57600/57600 [==============================] - 41s 703us/step - loss: 2423061110.2415 - val_loss: 75938532612.2667\n",
      "Epoch 238/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2432238392.0451 - val_loss: 63520789317.6889\n",
      "Epoch 239/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2404671499.6936 - val_loss: 64780449433.6000\n",
      "Epoch 240/250\n",
      "57600/57600 [==============================] - 40s 686us/step - loss: 2391582494.6491 - val_loss: 67136022515.2000\n",
      "Epoch 241/250\n",
      "57600/57600 [==============================] - 40s 695us/step - loss: 2415866070.5681 - val_loss: 63287987300.9778\n",
      "Epoch 242/250\n",
      "57600/57600 [==============================] - 40s 686us/step - loss: 2377067762.0403 - val_loss: 57896672429.5111\n",
      "Epoch 243/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2392008356.8681 - val_loss: 61082522402.1333\n",
      "Epoch 244/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2394675664.7658 - val_loss: 59133168476.4444\n",
      "Epoch 245/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2366663789.8531 - val_loss: 59885800577.4222\n",
      "Epoch 246/250\n",
      "57600/57600 [==============================] - 40s 694us/step - loss: 2362910296.1738 - val_loss: 57127700723.2000\n",
      "Epoch 247/250\n",
      "57600/57600 [==============================] - 40s 693us/step - loss: 2365363604.3431 - val_loss: 59233210712.1778\n",
      "Epoch 248/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57600/57600 [==============================] - 40s 691us/step - loss: 2350173212.7931 - val_loss: 56035631634.4889\n",
      "Epoch 249/250\n",
      "57600/57600 [==============================] - 40s 692us/step - loss: 2327110223.6450 - val_loss: 53533979972.2667\n",
      "Epoch 250/250\n",
      "57600/57600 [==============================] - 40s 691us/step - loss: 2326519294.5408 - val_loss: 56803023634.4889\n"
     ]
    }
   ],
   "source": [
    "# Parametres\n",
    "verbose, epochs, batch_size = 1, 250, 5\n",
    "# initialize the model object\n",
    "clf_cnn = CNN_net()\n",
    "# fit network #Train the model using tensorboard instance in the callbacks\n",
    "history = clf_cnn.fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=verbose, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57600/57600 [==============================] - 3s 54us/step\n",
      " Model.evaluate :  56859808927.28889 \n",
      "\n",
      "Mean Squa Error : 56803022780.5706\n",
      "Mean Abso Error : 237826.56009111114\n",
      "Expl. Variance  : 0.5472144174215151\n"
     ]
    }
   ],
   "source": [
    "ypred = clf_cnn.predict(x_test)\n",
    "\n",
    "print(\" Model.evaluate : \",clf_cnn.evaluate(x_train, y_train),'\\n')\n",
    "\n",
    "#evaluate results\n",
    "mse=sklearn.metrics.mean_squared_error(y_test,ypred)\n",
    "mabs=sklearn.metrics.mean_absolute_error(y_test,ypred)\n",
    "exvar=sklearn.metrics.explained_variance_score(y_test,ypred)   \n",
    "print('Mean Squa Error :',mse)\n",
    "print('Mean Abso Error :',mabs)\n",
    "print('Expl. Variance  :',exvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 46, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 21, 128)           24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 26,241\n",
      "Trainable params: 26,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.datatechnotes.com/2019/12/how-to-fit-regression-data-with-cnn.html\n",
    "x_ax = range(len(ypred))\n",
    "plt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss','Validation loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# from the first Fully-Connected layer \n",
    "layer_name = 'dense_1'\n",
    "intermediate_layer_model = Model(inputs=clf_cnn.input,\n",
    "                                 outputs=clf_cnn.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features of the train dataset to use it in future.\n",
    "out_cnn_train = intermediate_layer_model.predict(x_train)\n",
    "# Save the features of the test dataset to use it in future.\n",
    "out_cnn_test = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (from CNN) Shape: (57600, 1)\n",
      "Training Labels (from CNN) Shape: (57600,) \n",
      "\n",
      "Test Features (from CNN) Shape: (14400, 1)\n",
      "Test Labels (from CNN) Shape: (14400,) \n",
      "\n",
      "Test Features original Shape: (57600, 48, 1)\n",
      "Test Features original Shape: (14400, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features (from CNN) Shape:', out_cnn_train.shape)\n",
    "print('Training Labels (from CNN) Shape:', y_train.shape,'\\n')\n",
    "\n",
    "print('Test Features (from CNN) Shape:', out_cnn_test.shape)\n",
    "print('Test Labels (from CNN) Shape:', y_test.shape,'\\n')\n",
    "\n",
    "print('Test Features original Shape:', x_train_.shape)\n",
    "print('Test Features original Shape:', x_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + Random Forest + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djinn example\n",
      "Finding optimal hyper-parameters...\n",
      "Determining learning rate...\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:444: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:477: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:461: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Determining number of epochs needed...\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:528: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:531: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:532: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Optimal learning rate:  0.00046415888336127773\n",
      "Optimal # epochs:  210\n",
      "Optimal batch size:  2880\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:262: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:263: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn_fns.py:334: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Epoch: 0001 cost= 1.191405535\n",
      "Epoch: 0002 cost= 0.987820082\n",
      "Epoch: 0003 cost= 0.807466537\n",
      "Epoch: 0004 cost= 0.647176574\n",
      "Epoch: 0005 cost= 0.509929601\n",
      "Epoch: 0006 cost= 0.391754713\n",
      "Epoch: 0007 cost= 0.295806838\n",
      "Epoch: 0008 cost= 0.217287538\n",
      "Epoch: 0009 cost= 0.150463518\n",
      "Epoch: 0010 cost= 0.098054850\n",
      "Epoch: 0011 cost= 0.060773157\n",
      "Epoch: 0012 cost= 0.037425777\n",
      "Epoch: 0013 cost= 0.023732079\n",
      "Epoch: 0014 cost= 0.017353843\n",
      "Epoch: 0015 cost= 0.014555793\n",
      "Epoch: 0016 cost= 0.013401652\n",
      "Epoch: 0017 cost= 0.012591417\n",
      "Epoch: 0018 cost= 0.012238479\n",
      "Epoch: 0019 cost= 0.011819382\n",
      "Epoch: 0020 cost= 0.011129133\n",
      "Epoch: 0021 cost= 0.010727504\n",
      "Epoch: 0022 cost= 0.010341413\n",
      "Epoch: 0023 cost= 0.009935331\n",
      "Epoch: 0024 cost= 0.009501087\n",
      "Epoch: 0025 cost= 0.009069487\n",
      "Epoch: 0026 cost= 0.008801820\n",
      "Epoch: 0027 cost= 0.008509414\n",
      "Epoch: 0028 cost= 0.008168334\n",
      "Epoch: 0029 cost= 0.007970386\n",
      "Epoch: 0030 cost= 0.007794071\n",
      "Epoch: 0031 cost= 0.007587475\n",
      "Epoch: 0032 cost= 0.007450403\n",
      "Epoch: 0033 cost= 0.007314066\n",
      "Epoch: 0034 cost= 0.007180626\n",
      "Epoch: 0035 cost= 0.007144621\n",
      "Epoch: 0036 cost= 0.006988383\n",
      "Epoch: 0037 cost= 0.006845602\n",
      "Epoch: 0038 cost= 0.006888090\n",
      "Epoch: 0039 cost= 0.006804972\n",
      "Epoch: 0040 cost= 0.006796971\n",
      "Epoch: 0041 cost= 0.006793017\n",
      "Epoch: 0042 cost= 0.006760031\n",
      "Epoch: 0043 cost= 0.006653241\n",
      "Epoch: 0044 cost= 0.006660116\n",
      "Epoch: 0045 cost= 0.006724911\n",
      "Epoch: 0046 cost= 0.006735001\n",
      "Epoch: 0047 cost= 0.006726293\n",
      "Epoch: 0048 cost= 0.006653373\n",
      "Epoch: 0049 cost= 0.006729705\n",
      "Epoch: 0050 cost= 0.006618827\n",
      "Epoch: 0051 cost= 0.006656406\n",
      "Epoch: 0052 cost= 0.006641535\n",
      "Epoch: 0053 cost= 0.006642070\n",
      "Epoch: 0054 cost= 0.006728627\n",
      "Epoch: 0055 cost= 0.006668461\n",
      "Epoch: 0056 cost= 0.006680854\n",
      "Epoch: 0057 cost= 0.006655557\n",
      "Epoch: 0058 cost= 0.006697738\n",
      "Epoch: 0059 cost= 0.006678147\n",
      "Epoch: 0060 cost= 0.006689617\n",
      "Epoch: 0061 cost= 0.006689692\n",
      "Epoch: 0062 cost= 0.006717932\n",
      "Epoch: 0063 cost= 0.006694210\n",
      "Epoch: 0064 cost= 0.006805658\n",
      "Epoch: 0065 cost= 0.006702051\n",
      "Epoch: 0066 cost= 0.006748922\n",
      "Epoch: 0067 cost= 0.006655122\n",
      "Epoch: 0068 cost= 0.006679863\n",
      "Epoch: 0069 cost= 0.006692661\n",
      "Epoch: 0070 cost= 0.006694866\n",
      "Epoch: 0071 cost= 0.006754966\n",
      "Epoch: 0072 cost= 0.006727834\n",
      "Epoch: 0073 cost= 0.006750516\n",
      "Epoch: 0074 cost= 0.006692590\n",
      "Epoch: 0075 cost= 0.006756169\n",
      "Epoch: 0076 cost= 0.006751189\n",
      "Epoch: 0077 cost= 0.006687756\n",
      "Epoch: 0078 cost= 0.006787700\n",
      "Epoch: 0079 cost= 0.006748603\n",
      "Epoch: 0080 cost= 0.006804214\n",
      "Epoch: 0081 cost= 0.006695893\n",
      "Epoch: 0082 cost= 0.006653946\n",
      "Epoch: 0083 cost= 0.006749910\n",
      "Epoch: 0084 cost= 0.006722664\n",
      "Epoch: 0085 cost= 0.006678248\n",
      "Epoch: 0086 cost= 0.006701753\n",
      "Epoch: 0087 cost= 0.006874098\n",
      "Epoch: 0088 cost= 0.006631955\n",
      "Epoch: 0089 cost= 0.006665596\n",
      "Epoch: 0090 cost= 0.006723705\n",
      "Epoch: 0091 cost= 0.006627272\n",
      "Epoch: 0092 cost= 0.006709749\n",
      "Epoch: 0093 cost= 0.006703559\n",
      "Epoch: 0094 cost= 0.006698098\n",
      "Epoch: 0095 cost= 0.006677537\n",
      "Epoch: 0096 cost= 0.006769917\n",
      "Epoch: 0097 cost= 0.006717393\n",
      "Epoch: 0098 cost= 0.006660318\n",
      "Epoch: 0099 cost= 0.006747350\n",
      "Epoch: 0100 cost= 0.006716184\n",
      "Epoch: 0101 cost= 0.006693336\n",
      "Epoch: 0102 cost= 0.006759567\n",
      "Epoch: 0103 cost= 0.006670962\n",
      "Epoch: 0104 cost= 0.006644067\n",
      "Epoch: 0105 cost= 0.006674613\n",
      "Epoch: 0106 cost= 0.006630081\n",
      "Epoch: 0107 cost= 0.006686434\n",
      "Epoch: 0108 cost= 0.006740622\n",
      "Epoch: 0109 cost= 0.006661958\n",
      "Epoch: 0110 cost= 0.006715736\n",
      "Epoch: 0111 cost= 0.006756540\n",
      "Epoch: 0112 cost= 0.006658433\n",
      "Epoch: 0113 cost= 0.006667780\n",
      "Epoch: 0114 cost= 0.006745117\n",
      "Epoch: 0115 cost= 0.006765364\n",
      "Epoch: 0116 cost= 0.006734269\n",
      "Epoch: 0117 cost= 0.006692263\n",
      "Epoch: 0118 cost= 0.006774739\n",
      "Epoch: 0119 cost= 0.006796825\n",
      "Epoch: 0120 cost= 0.006707880\n",
      "Epoch: 0121 cost= 0.006754209\n",
      "Epoch: 0122 cost= 0.006696153\n",
      "Epoch: 0123 cost= 0.006811866\n",
      "Epoch: 0124 cost= 0.006693603\n",
      "Epoch: 0125 cost= 0.006646209\n",
      "Epoch: 0126 cost= 0.006655699\n",
      "Epoch: 0127 cost= 0.006683635\n",
      "Epoch: 0128 cost= 0.006740036\n",
      "Epoch: 0129 cost= 0.006676369\n",
      "Epoch: 0130 cost= 0.006750108\n",
      "Epoch: 0131 cost= 0.006695702\n",
      "Epoch: 0132 cost= 0.006749876\n",
      "Epoch: 0133 cost= 0.006736369\n",
      "Epoch: 0134 cost= 0.006697222\n",
      "Epoch: 0135 cost= 0.006814812\n",
      "Epoch: 0136 cost= 0.006730696\n",
      "Epoch: 0137 cost= 0.006728285\n",
      "Epoch: 0138 cost= 0.006668831\n",
      "Epoch: 0139 cost= 0.006660092\n",
      "Epoch: 0140 cost= 0.006694763\n",
      "Epoch: 0141 cost= 0.006661797\n",
      "Epoch: 0142 cost= 0.006716605\n",
      "Epoch: 0143 cost= 0.006683214\n",
      "Epoch: 0144 cost= 0.006684334\n",
      "Epoch: 0145 cost= 0.006695902\n",
      "Epoch: 0146 cost= 0.006795007\n",
      "Epoch: 0147 cost= 0.006634741\n",
      "Epoch: 0148 cost= 0.006641400\n",
      "Epoch: 0149 cost= 0.006695862\n",
      "Epoch: 0150 cost= 0.006652912\n",
      "Epoch: 0151 cost= 0.006672583\n",
      "Epoch: 0152 cost= 0.006774226\n",
      "Epoch: 0153 cost= 0.006709645\n",
      "Epoch: 0154 cost= 0.006685031\n",
      "Epoch: 0155 cost= 0.006683608\n",
      "Epoch: 0156 cost= 0.006716358\n",
      "Epoch: 0157 cost= 0.006718031\n",
      "Epoch: 0158 cost= 0.006766751\n",
      "Epoch: 0159 cost= 0.006720304\n",
      "Epoch: 0160 cost= 0.006693915\n",
      "Epoch: 0161 cost= 0.006678784\n",
      "Epoch: 0162 cost= 0.006739576\n",
      "Epoch: 0163 cost= 0.006651207\n",
      "Epoch: 0164 cost= 0.006737033\n",
      "Epoch: 0165 cost= 0.006628128\n",
      "Epoch: 0166 cost= 0.006664750\n",
      "Epoch: 0167 cost= 0.006764897\n",
      "Epoch: 0168 cost= 0.006729702\n",
      "Epoch: 0169 cost= 0.006782637\n",
      "Epoch: 0170 cost= 0.006761268\n",
      "Epoch: 0171 cost= 0.006776585\n",
      "Epoch: 0172 cost= 0.006785150\n",
      "Epoch: 0173 cost= 0.006699981\n",
      "Epoch: 0174 cost= 0.006738607\n",
      "Epoch: 0175 cost= 0.006706194\n",
      "Epoch: 0176 cost= 0.006637380\n",
      "Epoch: 0177 cost= 0.006720803\n",
      "Epoch: 0178 cost= 0.006811873\n",
      "Epoch: 0179 cost= 0.006688549\n",
      "Epoch: 0180 cost= 0.006656283\n",
      "Epoch: 0181 cost= 0.006748156\n",
      "Epoch: 0182 cost= 0.006712851\n",
      "Epoch: 0183 cost= 0.006664777\n",
      "Epoch: 0184 cost= 0.006692489\n",
      "Epoch: 0185 cost= 0.006678488\n",
      "Epoch: 0186 cost= 0.006732453\n",
      "Epoch: 0187 cost= 0.006737228\n",
      "Epoch: 0188 cost= 0.006679002\n",
      "Epoch: 0189 cost= 0.006618724\n",
      "Epoch: 0190 cost= 0.006769123\n",
      "Epoch: 0191 cost= 0.006708980\n",
      "Epoch: 0192 cost= 0.006699697\n",
      "Epoch: 0193 cost= 0.006716805\n",
      "Epoch: 0194 cost= 0.006672317\n",
      "Epoch: 0195 cost= 0.006651251\n",
      "Epoch: 0196 cost= 0.006727358\n",
      "Epoch: 0197 cost= 0.006688743\n",
      "Epoch: 0198 cost= 0.006706023\n",
      "Epoch: 0199 cost= 0.006673478\n",
      "Epoch: 0200 cost= 0.006655202\n",
      "Epoch: 0201 cost= 0.006684921\n",
      "Epoch: 0202 cost= 0.006685082\n",
      "Epoch: 0203 cost= 0.006754844\n",
      "Epoch: 0204 cost= 0.006649567\n",
      "Epoch: 0205 cost= 0.006783354\n",
      "Epoch: 0206 cost= 0.006675379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0207 cost= 0.006686316\n",
      "Epoch: 0208 cost= 0.006687758\n",
      "Epoch: 0209 cost= 0.006733761\n",
      "Epoch: 0210 cost= 0.006711907\n",
      "Optimization Finished!\n",
      "Model saved in: ./reg_djinn_test_tree0.ckpt\n",
      "WARNING:tensorflow:From ..\\djinn\\djinn.py:276: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./reg_djinn_test_tree0.ckpt\n",
      "Model 0 restored\n",
      "Mean Squa Error : 206324235.06406847\n",
      "Mean Abso Error : 11412.49481340278\n",
      "Expl. Variance  : 0.6132459090713013 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from djinn import djinn\n",
    "print(\"djinn example\")    \n",
    "modelname=\"reg_djinn_test\"   # name the model\n",
    "ntrees=1                 # number of trees = number of neural nets in ensemble\n",
    "maxdepth=5               # max depth of tree -- optimize this for each data set\n",
    "dropout_keep=1.0         # dropout typically set to 1 for non-Bayesian models\n",
    "\n",
    "#initialize the model\n",
    "model=djinn.DJINN_Regressor(ntrees,maxdepth,dropout_keep)\n",
    "x_train, y_train, x_test, y_test = out_cnn_train, y_train, out_cnn_test, y_test\n",
    "\n",
    "# find optimal settings: this function returns dict with hyper-parameters\n",
    "# each djinn function accepts random seeds for reproducible behavior\n",
    "optimal=model.get_hyperparameters(x_train, y_train, random_state=42)\n",
    "batchsize=optimal['batch_size']\n",
    "learnrate=optimal['learn_rate']\n",
    "epochs=optimal['epochs']\n",
    "\n",
    "\n",
    " \n",
    "# train the model with hyperparameters determined above\n",
    "model.train(x_train,y_train,epochs=epochs,learn_rate=learnrate, batch_size=batchsize, \n",
    "              display_step=1, save_files=True, file_name=modelname, \n",
    "              save_model=True,model_name=modelname, random_state=1)\n",
    "\n",
    "# *note there is a function model.fit(x_train,y_train, ... ) that wraps \n",
    "# get_hyperparameters() and train(), so that you do not have to manually\n",
    "# pass hyperparameters to train(). However, get_hyperparameters() can\n",
    "# be expensive, so I recommend running it once per dataset and using those\n",
    "# hyperparameter values in train() to save computational time\n",
    "\n",
    "# make predictions\n",
    "m=model.predict(x_test) #returns the median prediction if more than one tree\n",
    "\n",
    "#evaluate results\n",
    "evaluate(y_test,m)\n",
    "\n",
    "#close model \n",
    "model.close_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by CNN + ( SVM, XGB, DTree, ExtraTrees, RandomFores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) \n",
      "\n",
      "Mean Squa Error : 366348716.7815958\n",
      "Mean Abso Error : 15024.211446207539\n",
      "Expl. Variance  : 0.3178842263599818 \n",
      "\n",
      "================================================================================\n",
      "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
      "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
      "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
      "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
      "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "             warm_start=False) \n",
      "\n",
      "Mean Squa Error : 7.457534215591784e+40\n",
      "Mean Abso Error : 2.7307124192661907e+20\n",
      "Expl. Variance  : -1.3944381860388618e+28 \n",
      "\n",
      "================================================================================\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "              normalize=False, tol=0.001, verbose=False) \n",
      "\n",
      "Mean Squa Error : 211651789.5534434\n",
      "Mean Abso Error : 11519.263537443658\n",
      "Expl. Variance  : 0.6033132991615313 \n",
      "\n",
      "================================================================================\n",
      "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,\n",
      "          fit_path=True, max_iter=500, normalize=True, positive=False,\n",
      "          precompute='auto', verbose=False) \n",
      "\n",
      "Mean Squa Error : 211659147.7452617\n",
      "Mean Abso Error : 11527.815088302525\n",
      "Expl. Variance  : 0.6032980609546736 \n",
      "\n",
      "================================================================================\n",
      "ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "              normalize=False, threshold_lambda=10000.0, tol=0.001,\n",
      "              verbose=False) \n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-26cdef7caa56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_cnn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m#print(clf.predict(predictionData),'\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;31m# Iterative procedure of ARDRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miter_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[0msigma_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_sigma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m             \u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_coeff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\bayes.py\u001b[0m in \u001b[0;36mupdate_sigma\u001b[1;34m(X, alpha_, lambda_, keep_lambda, n_samples)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[1;31m# Compute sigma and mu (using Woodbury matrix identity)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_sigma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m             sigma_ = pinvh(np.eye(n_samples) / alpha_ +\n\u001b[0m\u001b[0;32m    545\u001b[0m                            np.dot(X[:, keep_lambda] *\n\u001b[0;32m    546\u001b[0m                            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_lambda\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\twodim_base.py\u001b[0m in \u001b[0;36meye\u001b[1;34m(N, M, k, dtype, order)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/41925157/logisticregression-unknown-label-type-continuous-using-sklearn-in-python\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(gamma='scale'),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "    linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()\n",
    "              ]\n",
    "\n",
    "for item in classifiers:\n",
    "    print(item,'\\n')\n",
    "    clf = item\n",
    "    clf.fit(out_cnn_train, y_train)\n",
    "    #print(clf.predict(predictionData),'\\n')\n",
    "    #Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "    m = clf.predict(out_cnn_test)\n",
    "    #evaluate results\n",
    "    mse=sklearn.metrics.mean_squared_error(y_test,m)\n",
    "    mabs=sklearn.metrics.mean_absolute_error(y_test,m)\n",
    "    exvar=sklearn.metrics.explained_variance_score(y_test,m)   \n",
    "    print('Mean Squa Error :',mse)\n",
    "    print('Mean Abso Error :',mabs)\n",
    "    print('Expl. Variance  :',exvar,'\\n')\n",
    "    print(\"================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from xgboost import XGBRegressor\n",
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(out_cnn_train, y_train , verbose=False)\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(out_cnn_test)\n",
    "print('CNN XGBRegressor      ')\n",
    "evaluate(y_test,XGBpredictions)\n",
    "\n",
    "    \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "Ext = ExtraTreesRegressor(n_estimators=10)\n",
    "Ext.fit(out_cnn_train, y_train)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictionsCNN_Ext = Ext.predict(out_cnn_test)\n",
    "print('CNN ExtraTreesRegressor      ')\n",
    "evaluate(y_test,predictionsCNN_Ext)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "clf_dt = tree.DecisionTreeRegressor()\n",
    "clf_dt.fit(out_cnn_train, y_train)\n",
    "# Get the mean absolute error on the validation data :\n",
    "clf_dtpredictions = clf_dt.predict(out_cnn_test)\n",
    "print('CNN DecisionTreeRegressor      ')\n",
    "evaluate(y_test,clf_dtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cnn_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Example With Boston Dataset: Baseline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(128, input_dim=out_cnn_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(64, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "clf_MLP = KerasClassifier(build_fn = baseline_model, epochs = 50, batch_size=5, verbose=1)\n",
    "clf_MLP.fit(out_cnn_train, y_train)\n",
    "y_predmlp = clf_MLP.predict(out_cnn_test)\n",
    "\n",
    "#print(\"CNN MLP Model.evaluate : \",clf_MLP.evaluate(out_cnn_test, y_train),'\\n')\n",
    "#evaluate results\n",
    "mse=sklearn.metrics.mean_squared_error(y_test,y_predmlp)\n",
    "mabs=sklearn.metrics.mean_absolute_error(y_test,y_predmlp)\n",
    "exvar=sklearn.metrics.explained_variance_score(y_test,y_predmlp)   \n",
    "print('CNN MLP Mean Squa Error :',mse)\n",
    "print('CNN MLP Mean Abso Error :',mabs)\n",
    "print('CNN MLP Expl. Variance  :',exvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification by RandomForest, ExtraTrees, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ = x_train_.reshape(x_train_.shape[0], x_train_.shape[1])\n",
    "x_test_  = x_test_.reshape(x_test_.shape[0], x_test_.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier : from dataset originl\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(x_train_, y_train_)\n",
    "predictions = rf.predict(x_test_)\n",
    "print('CNN RandomForestRegressor      ')\n",
    "evaluate(y_test_,predictions)\n",
    "\n",
    "\n",
    "#Feed the extracted features with the labels to RANDOM FOREST \n",
    "from xgboost import XGBRegressor\n",
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(x_train_, y_train_ , verbose=False)\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(x_test_)\n",
    "print('CNN XGBRegressor      ')\n",
    "evaluate(y_test_,XGBpredictions)\n",
    "\n",
    "    \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "Ext = ExtraTreesRegressor(n_estimators=10)\n",
    "Ext.fit(x_train_, y_train_)\n",
    "#Feed the features of the test to ExtraTreesClassifier Classifier to predict its class\n",
    "predictionsCNN_Ext = Ext.predict(x_test_)\n",
    "print('CNN ExtraTreesRegressor      ')\n",
    "evaluate(y_test_,predictionsCNN_Ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
